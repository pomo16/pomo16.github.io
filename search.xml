<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python Garbage Collection]]></title>
    <url>%2F2021%2F03%2F04%2FPython-Garbage-Collection%2F</url>
    <content type="text"><![CDATA[概述Python 的垃圾回收以引用计数机制（Reference Counting）为主，并配合使用标记 - 清除（Mark and Sweep）和分代回收（Generational Collection）等 GC 策略。 一、引用计数 Reference CountingPython 中主要依赖引用计数来完成垃圾回收 原理Python 的每个对象内部都会维护一个引用计数器，程序运行过程中会实时更新引用计数器的值，以此来反映引用当前对象的数量。当有新引用指向该对象时，引用计数器加1；当指向对象的引用失效时，引用计数器减1。当引用计数器数值为0时，该对象的内存就会被释放掉。可以通过 sys 包的 getrefcount() 来获取一个名称所引用的对象当前的引用计数（ps：这里 getrefcount() 本身会使得引用计数加一）。 1sys.getrefcount(a) 引用计数加1的情况： 对象被创建，e.g. a=1 对象被引用，e.g. b=a 对象作为参数，被传入函数 对象作为一个元素，被放入容器中 引用计数减1的情况： 对象别名被显示销毁 del 对象别名被赋予新的对象 一个对象离开他的作用域 对象所在的容器被销毁或者是从容器中删除对象 优点 逻辑简单、高效执行、实时性高，一旦某个对象的引用计数归零，内存就会被释放，无需像其他 GC 方法需要等到某个时机执行 GC 时间被打平，对运行中的程序影响较小 缺点 逻辑简单但有一定的实现成本，每个对象都需要独立的空间维护引用计数器，加大了对空间的负担 尽管 GC 时间被打平，但在一些场景下也会比较慢。在释放一个大对象时，如字典，需要对引用的所有对象循环嵌套调用，时间花销较大 引用计数无法避免循环引用的问题，必须通过其他 GC 策略来辅助解决，这也导致一些高级语言如 Java 的 GC 没有采用引用计数机制 循环引用当两个对象互相引用、且不存在任何外部引用时，两个对象的引用计数会始终为1，永远不会被垃圾回收。举个例子： 123456789# list1和list2引用的对象引用计数加1list1 = []list2 = []# list1和list2的对象互相引用，对象引用计数分别加1，即为2list1.append(list2)list2.append(list1)# list1和list2引用的对象引用计数减1，但对象的引用计数仍然为1，且永远无法归0del list1del list2 为了解决循环引用问题，Python 又引入了以下两种 GC 策略。 二、标记 - 清除 Mark and SweepPython 采用标记 - 清除算法来解决容器对象可能产生的循环引用问题。（ps：只有容器对象才有可能产生循环引用，如列表、字典、自定义的类、元组等等。像整型、字符串等基础数据类型并不会出现循环引用。） 基本原理标记 - 清除是一种基于 Tracing 的垃圾回收算法，主要分为两个阶段：第一阶段是标记，把所有存活对象打上标记；第二阶段是清除，对没有标记的失活对象进行回收。 标记阶段：遍历所有对象，如果是可达的，说明还有对象引用它，则该对象被标记（ps：GC root 一般是一些全局变量、调用栈、寄存器等不可被删除的对象） 清除阶段：再次遍历对象，如果发现某个对象没有被标记为可达，则将其回收 详细说明 参考：CPython 垃圾收集器设计 在标记 - 清除算法中，为了追踪容器对象，需要每个容器对象维护两个额外的指针，用来将容器对象组成一个双向链表（double-linked lists），指针分别指向前后两个容器对象，方便插入和删除操作。Python 解释器（CPython）维护了两个这样的双向链表，一个链表存放着需要被扫描的容器对象（Object to Scan），另一个链表存放着临时不可达对象（Unreachable）。下面举例说明标记-清除的详细过程： 1234567891011121314151617181920import gcclass Link: def __init__(self, next_link=None): self.next_link = next_linklink_3 = Link()link_2 = Link(link_3)link_1 = Link(link_2)link_3.next_link = link_1A = link_1del link_1, link_2, link_3link_4 = Link()link_4.next_link = link_4del link_4# Collect the unreachable Link object (and its .__dict__ dict).gc.collect()# Output: 2 该例子中，link1、link2、link3 形成引用环，同时 link1 还被名称 A 引用。link4 自引用，也构成一个引用环。在垃圾收集器中，每一个节点除了有一个记录当前引用计数的变量 ref_count 还有一个 gc_ref 变量，这个 gc_ref 是 ref_count 的一个副本，所以初始值为 ref_count 的大小。 GC 启动时，遍历 Object to Scan 链表中的容器对象，并且将当前对象所引用的所有对象的 gc_ref 减 1。这一步相当于解除了循环引用的影响，因为只有真正被外部引用的对象才满足 gc_ref &gt; 1。 GC 再次扫描所有的容器对象，如果对象的 gc_ref 为0，则该对象被标记为 GC_TENTATIVELY_UNREACHABLE ，并且被移至 Unreachable 链表中（link3、link4）。（下图为处理了 link3 和 link4 的时刻，link1 和 link2 还没被处理） 紧接着2，如果对象的 gc_ref 不为0，那么这个对象就会被标记为 GC_REACHABLE（link1）。同时当 GC 发现一个节点是可达的，则会从该节点出发可以到达的所有节点标记为 GC_REACHABLE。如果被标记为 GC_REACHABLE 的节点在 Unreachable 链表中，则需要将其移回 Object to Scan 链表中（link3）。 第二次遍历结束后，存在于 Unreachable 链表中的对象就是真正需要被释放的对象，GC 随即释放。（link4） Notice标记 - 清除 执行时会 STW （stop the world），整个应用程序会被暂停直到标记 - 清除结束后才会恢复。 三、分代回收 Generational Collection由于 标记 - 清除 会 STW，为了限制垃圾回收的时间，Python 通过引入分代回收来优化自身的垃圾回收机制，以空间换时间的方式来提高垃圾回收效率。 先验知识 分代回收思想基于一个统计事实：对于程序，存在一定比例的内存块的生存周期比较短；而剩下的内存块，生存周期会比较长，甚至会从程序开始一直持续到程序结束。生存期较短对象的比例通常在 80%～90% 之间。 简而言之，对象存在时间越长，越可能不是垃圾，应该越少去收集。这样在执行 标记-清除 算法时可以有效减小遍历的对象数，从而提高垃圾回收的速度。 原理 Python GC 给对象定义了三种世代(0,1,2)，每一个新生对象在 generation 0 中；如果它在一轮 GC 扫描中活了下来，那么它将被移至 generation 2，在那里他将较少的被扫描；如果它又活过了一轮 GC，它又将被移至 generation 2，在那里它被扫描的次数将会更少。 GC = 垃圾检查 + 垃圾回收。垃圾检查触发时机：世代中的对象达到一定的数目。 每个世代都有自己的阈值，且越老年代的阈值会越小（阈值越小表示垃圾检查的频率越小）。 阈值解释（以 (700, 10, 10) 为例）： 第一个参数 700 表示距离上一次 generation 0 垃圾检查，Python 分配内存数目减去释放内存的数目，如果超过 700 就会触发 generation 0 的垃圾检查 第二个参数 10 表示距离上一次 generation 1 垃圾检查，generation 0 垃圾检查的次数 第三个参数 10 表示距离上一次 generation 2 垃圾检查，generation 1 垃圾检查的次数 通过以下两个函数可以查询和调整阈值： 12gc.get_threshold() # (threshold0, threshold1, threshold2). default (700, 10, 10)gc.set_threshold(threshold0[, threshold1[, threshold2]]) 垃圾回收触发时机 调用 gc.collect(),需要先导入 gc 模块。 自动垃圾回收：当 gc 模块的计数器达到阀值的时候（上述情况）。 程序退出的时候。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive]]></title>
    <url>%2F2020%2F07%2F15%2FHive%2F</url>
    <content type="text"><![CDATA[Hive 简介Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。有以下特点： 简单、易上手（sql） 灵活性高，可自定义用户函数（UDF）和存储格式 为超大的数据集设计的计算和存储能力，集群扩展容易 统一的元数据管理，可与 presto／impala／sparksql 等共享数据 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理 Hive 体系架构 command-line shell &amp; thrift/jdbc：数据操作方式 command-line shell：通过 hive 命令行的方式来操作数据 thrift/jdbc：通过 thrift 协议按照标准的 JDBC 的方式操作数据 Metastore：一个管理库表元数据的服务 客户端连接 Metastore 服务，Metastore 再去连接 MySQL 数据库来存取元数据。有了 Metastore 服务，就可以有多个客户端同时连接，而且这些客户端不需要知道 MySQL 数据库的用户名和密码，只需要连接 Metastore 服务即可。 HQL 执行流程 语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法、语法解析，将 SQL 转化为抽象语法树 AST Tree 语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock 生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree 优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量 生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务 优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划 数据类型基本数据类型 大类 类型 Integers（整型） TINYINT（1 字节的有符号整数）、SMALLINT（2 字节的有符号整数）、INT（4 字节的有符号整数）、BIGINT（8 字节的有符号整数） Boolean（布尔型） BOOLEAN（TRUE/FALSE） Floating point numbers（浮点型） FLOAT（单精度浮点型）、DOUBLE（双精度浮点型） Fixed point numbers（定点数） DECIMAL（用户自定义精度定点数，比如 DECIMAL(7,2)） String types（字符串） STRING（指定字符集的字符序列）、VARCHAR（具有最大长度限制的字符序列）、 CHAR（固定长度的字符序列） Date and time types（日期时间类型） TIMESTAMP（时间戳）、TIMESTAMP WITH LOCAL TIME ZONE（时间戳，纳秒精度）、DATE（日期类型） Binary types（二进制类型） BINARY（字节序列） TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别： TIMESTAMP WITH LOCAL TIME ZONE：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间 TIMESTAMP ：提交什么时间就保存什么时间，查询时也不做任何转换 隐式转换Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。 复杂类型 类型 描述 示例 STRUCT 类似于对象，是字段的集合，字段的类型可以不同，可以使用 名称.字段名 方式进行访问 STRUCT (‘xiaoming’, 12 , ‘2018-12-12’) MAP 键值对的集合，可以使用 名称[key] 的方式访问对应的值 map(‘a’, 1, ‘b’, 2) ARRAY 数组是一组具有相同类型和名称的变量的集合，可以使用 名称[index] 访问对应的值 ARRAY(‘a’, ‘b’, ‘c’, ‘d’) 示例1234567CREATE TABLE students( name STRING, -- 姓名 age INT, -- 年龄 subject ARRAY&lt;STRING&gt;, --学科 score MAP&lt;STRING,FLOAT&gt;, --各个学科考试成绩 address STRUCT&lt;houseNumber:int, street:STRING, city:STRING, province：STRING&gt; --家庭居住地址) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; 内容格式当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。 所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示： 分隔符 描述 \n 对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录 ^A (Ctrl+A) 分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 \001 来表示 ^B 用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割， 在 CREATE TABLE 语句中也可以使用八进制编码 \002 表示 ^C 用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 \003 表示 使用示例： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 存储格式支持的存储格式Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式： 格式 说明 TextFile 存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。 SequenceFile SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。 RCFile RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。 ORC Files ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。 Avro Files Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。 Parquet Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。 以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。 指定存储格式通常在创建表的时候使用 STORED AS 参数指定： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; STORED AS TEXTFILE STORED AS SEQUENCEFILE STORED AS ORC STORED AS PARQUET STORED AS AVRO STORED AS RCFILE 内部表和外部表内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下： 内部表 外部表 数据存储位置 内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 /user/hive/warehouse/数据库名.db/表名/ 目录下 外部表数据的存储位置创建表时由 Location 参数指定； 导入数据 在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理 外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置 删除表 删除元数据（metadata）和文件 只删除元数据（metadata）]]></content>
      <categories>
        <category>data</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop核心组件]]></title>
    <url>%2F2020%2F07%2F15%2FHadoop%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[目前的大数据生态基本都依赖于 Hadoop 生态，其可靠、高效、可伸缩，可以处理 PB 级别的数据。Hadoop 核心组件包括 HDFS、YARN 和 MapReduce。 HDFS——分布式文件存储系统HDFS（Hadoop Distributed File System） 是 Hadoop 下的分布式文件存储系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。 HDFS 架构 HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成： NameNode：负责执行有关文件系统命名空间的操作，例如打开、关闭、重命名等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。 DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建、删除等操作。 HDFS 的文件系统命名空间的层次结构与大多数文件系统类似，支持目录和文件的创建、移动、删除等操作，支持配置用户和访问权限，但是不支持软硬链接。NameNode 负责维护文件系统命名空间，并记录变更。 数据复制备份Hadoop 的设计初衷是运行在廉价的机器上，这意味着硬件是不可靠的。HDFS 提供数据复制机制来保证容错性。HDFS 将每个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认块大小=128M，复制因子=3）。 复制原理：HDFS 一般都会部署在不同机房的多台容器上，通常同一机房容器之间的网络带宽大于跨机房容器之间的网络带宽。因此 HDFS 采用机架感知副本放置策略（可自定义策略）。 当复制因子为 3 ：在写入程序位于 DataNode 上时，就优先将写入文件的一个副本放置在该 DataNode 上，否则放在随机 DataNode 上。之后在另一个远程机房上的任意一个节点上放置另一个副本，并在该机房上的另一个节点上放置最后一个副本。此策略可以减少机房间的写入流量，从而提高写入性能。 当复制因子大于 3：随机确定第 4 个和之后副本的放置位置，同时保持每个机房的副本数量低于上限，上限值通常为 （复制系数 - 1）/ 机架数量 + 2，需要注意的是不允许同一个 DataNode 上具有同一个块的多个副本。 副本选择：为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机房上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。 架构稳定性 心跳机制和重新复制：每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。 数据的完整性：由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性。（当客户端创建 HDFS 文件时，它会计算文件的每个块的校验和，并将校验和存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的校验和匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。） 元数据的磁盘故障：FsImage(元数据检查点，包含整个 HDFS 的所有目录文件信息) 和 EditLog(写操作记录) 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。 支持快照：快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。 HDFS 特点 高容错：多副本方案 高吞吐：支持高吞吐数据访问而非低延迟 大文件支持：文档大小可达 GB 到 TB 级别 简单一致性模型：更适合一次写入多次读取 (write-once-read-many) 的访问模型，支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据 跨平台移植性：具有良好的跨平台移植性，大部分大数据计算框架都将其作为数据持久化首选方案 HDFS 写数据原理 角色主要职能： client：文件切块 NameNode：为每个数据块分配 DataNode DataNode：通过复制管道存储数据 以客户端需要写入200M数据为例： 用户向 client 提供 BLOCKSIZE（大文件会被切分成块，通常是 64M 或者 128M） 和 REPLICATION FACTOR（每个数据块会被存储在不同的地方，通常是3个）参数 client 根据参数将大文件切分成两个块（向上取整 ：200 / 128 = 2） client 告知 NameNode 要求写入一个三备份的 128M 数据块 NameNode 分配 DataNode 并按照升序排列告知 client（DataNode1 DataNode2 DataNode3） client 只将数据（和列表）发送给 DataNode1 存储，然后 DataNode1 将相同的数据传输给 DataNode2，以此类推直到最后一个节点（DataNode3）完成存储 当前块完成所有节点的存储后，会发送完成信号给 NameNode NameNode 告知 client 当前数据块被成功存储和备份在 HDFS 中 client 重复 3~7 直到所有的数据块完成传输后，告知 NameNode 所有数据块已完成写入并请求关闭文件 HDFS 读数据原理 client 向 NameNode 通过文件名请求文件 NameNode 向 client 回复这个文件的所有数据块的列表和每个数据块所对应的 DataNode 的列表（按距离客户端远近排列） client 按顺序下载数据块，向每个块对应的最近 DataNode（列表中的第一个）下载数据 HDFS 故障类型和检测方法由于 NameNode 挂掉基本全军覆没，所以一般主要关注 DataNode 的故障检测。 故障1：节点故障 故障检测：心跳机制（每3s发一次给 NameNode，NameNode 10min没收到就认为死亡）会排除死亡的 DataNode 故障2：通讯故障（无法收发） 故障检测：每次 client 发送数据给 DataNode 时，接收者都会回复一个应答信号。有重试机制，多次失败后 client 就会认为 DataNode 挂掉或者网络故障 故障3：数据损坏 故障检测：client 在发送数据时头部会包含校验和，且数据和校验和会被同时存储。所有的 DataNode 定时发送数据块报告给 NameNode。在发送报告之前 DataNode 会检测校验和是否正常，不会发送损坏的数据块信息，因此 NameNode 根据数量 diff 就可得知有多少数据损坏 故障处理 写入故障：client 在向 DataNode 写入数据的时候会接收应答信号，如果收不到某个 DataNode 的信号，client 就会调整通道跳过此节点。此时这个数据块没有被充分备份，NameNode 会稍后处理（详见 DataNode 故障） 读取故障：如果最近的 DataNode 挂掉，client 就会从次近的节点上读取数据 DataNode 故障 NameNode 维护了两张表，一个是数据块列表（记录每个数据块存储在哪些 DataNode 上），一个是 DataNode 列表（记录每个 DataNode 存储了哪些数据块）。如果 NameNode 发现一个 DataNode 上的数据块已经损坏，则会更新数据块表（将此 DN 移除出该表）；如果发现是某个 DataNode 挂掉，则会同时更新两张表。 未充分备份的数据块处理：NameNode 通过定时扫描数据块列表就可以得知未充分备份的数据块，其可以要求未有备份的新 DataNode 去已有备份的 DataNode 拷贝数据块，使数据块充分备份。不过 HDFS 不保证至少存活一份数据，只负责尽可能地选择合理的备份位置。 YARN——集群资源管理器Apache YARN（Yet Another Resource Negotiator）是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。 YARN 架构 ResourceManager：ResourceManager 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。ResourceManager 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。 NodeManager：NodeManager 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下： 启动时向 ResourceManager 注册并定时发送心跳消息，等待 ResourceManager 的指令 维护 Container 的生命周期，监控 Container 的资源使用情况 管理任务运行时的相关依赖，根据 ApplicationMaster 的需要，在启动 Container 之前将需要的程序及其依赖拷贝到本地 ApplicationMaster：在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 ApplicationMaster。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下： 根据应用的运行状态来决定动态计算资源需求 向 ResourceManager 申请资源，监控申请的资源的使用情况 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息 负责任务的容错 Container：Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 Container 表示的。YARN 会为每个任务分配一个 Container，该任务只能使用该 Container 中描述的资源。ApplicationMaster 可在 Container 内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求一个容器来启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求一个容器来运行 Giraph 任务。 YARN 工作原理简述 Client 提交作业到 YARN 上 Resource Manager 选择一个 Node Manager，启动一个 Container 并运行 Application Master 实例 Application Master 根据实际需要向 Resource Manager 请求更多的 Container 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务） Application Master 通过获取到的 Container 资源执行分布式计算 MapReduceHadoop MapReduce 是一个并行分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。 MapReduce 主要有三个阶段： Map 阶段：map 或者 mapper 将输入的数据集拆分成独立块。通常这些输入数据会存放在 HDFS，输入文件会被逐行输入到 mapper 函数中，mapper 函数会并行处理并创建一些数据块。 Shuffle 阶段：Map 转换到 Reduce 的中间过程，一般有 partitions、sort、combine 等操作 Reduce 阶段：对各个数据块上的数据并行运算，输出最终结果 1(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; shuffle -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output) MapReduce 具体流程以基本的词频统计为例： input：读取文件 splitting：文件按行拆分，K1 为行数，V1 为对应行的文本内容 mapping：每行按空格拆分，拆分得到 List(K2, V2)，K2 代表每个单词，V2 代表出现次数 shuffling：由于 Mapping 操作可能分发到不同的节点上并行运算，因此 shuffling 需要把相同 key 的数据转发到相同节点上合并。K2 为每个单词，List(V2) 为可迭代集合，V2 为 Mapping 中的 V2 reducing：对 List(V2) 进行规约求和，最终输出词频统计结果 MapReduce 编程模型中 splitting 和 shuffing 操作都是由框架实现的，需要我们自己编程实现的只有 mapping 和 reducing，这也就是 MapReduce 这个称呼的来源。 Combiner, Partitioner and SortCombiner、 Partioner 和 Sort 是 map 运算之后的可选操作，他们属于 shuffle 阶段的实现方式。 Combiner：执行本地化的 reduce 操作，在 map 计算之后先简单地在每个节点本地进行重复 key 的合并 Partitioner：分类器，将每个 map 的输出结果分发到新的节点上，具有相同 key 值的数据会分发到同一节点 Sort：key 有序排列，一般结合了 shuffle 操作]]></content>
      <categories>
        <category>data</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker安装ES相关]]></title>
    <url>%2F2020%2F06%2F05%2FDocker%E5%AE%89%E8%A3%85ES%E5%8F%8AIK%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[安装ElasticSearch镜像拉取1docker pull docker.elastic.co/elasticsearch/elasticsearch:7.1.1 启动容器1docker run -d --name es -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" -e ES_JAVA_OPTS="-Xms256m -Xmx256m" docker.elastic.co/elasticsearch/elasticsearch:7.1.1 相关配置1docker exec -it es /bin/bash 123456789101112131415161718192021222324# 显示文件ls结果如下：LICENSE.txt README.textile config lib modulesNOTICE.txt bin data logs plugins# 进入配置文件夹cd config# 显示文件ls结果如下：elasticsearch.keystore ingest-geoip log4j2.properties roles.yml users_roleselasticsearch.yml jvm.options role_mapping.yml users# 修改配置文件vi elasticsearch.yml# 加入跨域配置http.cors.enabled: truehttp.cors.allow-origin: "*"# 退出容器exit 1docker restart es 验证1curl localhost:9200 安装ik分词器准备分词器压缩包1docker exec -it es /bin/bash 1mkdir /usr/share/elasticsearch/plugins/ik 1exit 1docker cp elasticsearch-analysis-ik-7.1.1.zip es:/usr/share/elasticsearch/plugins/ik/ 附：https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.1.1/elasticsearch-analysis-ik-7.1.1.zip 1scp -i ~/.ssh/xxx.pem -C elasticsearch-analysis-ik-7.1.1.zip &#123;user&#125;@&#123;ip&#125;:/root 解压安装1docker exec -it es /bin/bash 1cd /usr/share/elasticsearch/plugins/ik 1unzip elasticsearch-analysis-ik-7.1.1.zip 1rm -rf elasticsearch-analysis-ik-7.1.0.zip 1exit 1docker restart es 基于Docker使用elasticsearch-dump镜像拉取1docker pull taskrabbit/elasticsearch-dump 创建文件存放路径1mkdir -p /data/ 用例 运用 docker run --rm -ti taskrabbit/elasticsearch-dump 调用能力 -v &lt;your dumps dir&gt;:&lt;your mount point&gt; 指定外部挂载到 docker 中的目录 三种导出方式：（实际使用方式三选一，三个文件都要导出） 12345678# 将analyzer分词从es复制到另一个esdocker run --rm -ti elasticdump/elasticsearch-dump --input=http://ip1:9200/index1 --output=http://ip2:9200/index2 --type=analyzer# 将mapping从index复制到另一个indexdocker run --rm -ti elasticdump/elasticsearch-dump --input=http://ip1:9200/index1 --output=http://ip1:9200/my_index --type=mapping # 将data复制到文件docker run --rm -ti -v /data:/tmp elasticdump/elasticsearch-dump --input=http://ip:9200/my_index --output=/tmp/my_index_data.json --type=data 从文件恢复数据到es： 1234# 从文件中恢复datadocker run --rm -ti -v /data:/tmp elasticdump/elasticsearch-dump --input=/tmp/my_index_data.json --output=http://ip:9200/my_index --type=data# mapping和analyzer略 常见错误 1Error: connect ECONNREFUSED 127.0.0.1:9200 访问 localhost 时 docker 要添加 --net=host，表示宿主和容器同网络，如不添加访问的是容器内的网络。 1docker run --net=host --rm -ti -v /data:/tmp elasticdump/elasticsearch-dump --input=/tmp/my_index_data.json --output=http://localhost:9200/my_index --type=data]]></content>
      <categories>
        <category>web</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 数组、字符串和切片]]></title>
    <url>%2F2020%2F06%2F03%2FGo-%E6%95%B0%E7%BB%84%E5%92%8C%E5%88%87%E7%89%87%2F</url>
    <content type="text"><![CDATA[数组定义方式1234var a [3]int // 定义长度为3的int型数组, 元素全部为0var b = [...]int&#123;1, 2, 3&#125; // 定义长度为3的int型数组, 元素为 1, 2, 3var c = [...]int&#123;2: 3, 1: 2&#125; // 定义长度为3的int型数组, 元素为 0, 2, 3var d = [...]int&#123;1, 2, 4: 5, 6&#125; // 定义长度为6的int型数组, 元素为 1, 2, 0, 0, 5, 6 方式a：长度确定，数组中每个元素都以零值初始化 方式b：顺序指定全部元素的初始化值，数组的长度根据元素个数自动计算 方式c：以索引的方式来指定数组初始化元素，数组长度为最大指定索引，没有明确值的元素用零值初始化 方式d：混合了方式b和方式c 访问数组Go 中数组是值语义，一个数组变量就是整个数组，并不是隐式的指向第一个元素的指针（C语言）。所以当数组变量被赋值或者被传递的时候，实际上会复制整个数组。大数组会造成大的开销，为了避免复制数组带来的开销，可以传递一个指向数组的指针，但是数组指针并不是数组。 123456789var a = [...]int&#123;1, 2, 3&#125; // a 是一个数组var b = &amp;a // b 是指向数组的指针fmt.Println(a[0], a[1]) // 打印数组的前2个元素fmt.Println(b[0], b[1]) // 通过数组指针访问数组元素的方式和数组类似for i, v := range b &#123; // 通过数组指针迭代数组的元素 fmt.Println(i, v)&#125; 但是数组指针类型依然不够灵活，因为数组的长度是数组类型的组成部分，指向不同长度数组的数组指针类型也是完全不同的。可以将数组看作一个特殊的结构体，结构的字段名对应数组的索引，同时结构体成员的数目是固定的。内置函数 len 返回数组长度，cap 返回数组容量。不过对于数组来说，这两个的返回结果是一样的。 我们还可以用 for 循环来遍历数组： 123456789for i := range a &#123; fmt.Printf("a[%d]: %d\n", i, a[i])&#125;for i, v := range b &#123; fmt.Printf("b[%d]: %d\n", i, v)&#125;for i := 0; i &lt; len(c); i++ &#123; fmt.Printf("c[%d]: %d\n", i, c[i])&#125; for range 的性能会好一些，因为这种迭代保证不会出现数组越界的情况，每轮迭代对数组元素的访问可以省去对下标越界的判断。另外 for range 可以忽略迭代时的下标： 1234var times [5][0]intfor range times &#123; fmt.Println("hello")&#125; 在上述使用中，尽管数组第一维有长度，但是 [0]int 大小是0，因此整个数组占用内存大小依然是0。没有额外的内存代价，我们就通过 for range 实现了 times 次快速迭代。 数组类型除了数值型数组，还可以定义字符串数组、结构体数组、函数数组、接口数组、管道数组等等。另外还有不常用的空数组，因为一般我们更倾向于去使用空结构体来用于管道同步。 字符串字符串是一个不可改变的字节序列，和数组不同，字符串的元素不可修改，是一个只读的字节数组，且长度固定。由于 Go 的源代码要求 UTF8 编码，导致 Go 源代码中出现的字符串字面量常量一般也是 UTF8 编码（对于转义字符，则没有这个限制）。源代码中的文本字符串通常被解释为采用 UTF8 编码的 Unicode 码点（rune）序列。字符串是只读的字节序列，可以包含包括byte值0的任意数据。我们也可以用字符串表示 GBK 等非 UTF8 编码的数据，但此时字符串可以看做是一个只读的二进制数组，因为 for range 等语法并不能支持非 UTF8 编码的字符串的遍历。 Go 语言字符串的底层结构在 reflect.StringHeader 中定义： 1234type StringHeader struct &#123; Data uintptr Len int&#125; 字符串结构包括了字符串指向的底层数组和字符串的字节长度。字符串其实就是一个结构体，因此字符串的赋值操作也就是 reflect.StringHeader 结构体的复制过程，并不会涉及底层字节数组的复制。[2]string 字符串数组对应的底层结构和 [2]reflect.StringHeader 对应的底层结构是一样的，可以将字符串数组看作一个结构体数组。举例来说 “Hello, world” 字符串底层数据和以下数组是完全一致的： 123var data = [...]byte&#123; 'h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd',&#125; 字符串虽然不是切片，但是支持切片操作，不同位置的切片底层也访问的同一块内存数据（因为字符串是只读的，相同的字符串面值常量通常是对应同一个字符串常量）: 123456s := "hello, world"hello := s[:5]world := s[7:]s1 := "hello, world"[:5]s2 := "hello, world"[7:] 字符串和数组类似，内置的len函数返回字符串的长度。或通过 reflect.StringHeader 结构访问字符串的长度（不推荐）。 12len(s)fmt.Println("len(s):", (*reflect.StringHeader)(unsafe.Pointer(&amp;s)).Len) 提到 Go 字符串时，我们一般都会假设字符串对应的是一个合法的 UTF8 编码的字符序列。可以用内置的 print 调试函数或 fmt.Print 函数直接打印，也可以用 for range 循环直接遍历 UTF8 解码后的 Unicode 码点值。我们也可以在字符串面值中直指定UTF8编码后的值（源文件中全部是ASCII码，可以避免出现多字节的字符）： 12fmt.Println("\xe4\xb8\x96") // 打印: 世fmt.Println("\xe7\x95\x8c") // 打印: 界 Go 语言的字符串中可以存放任意的二进制字节序列，而且即使是 UTF8 字符序列也可能会遇到坏的编码。如果遇到一个错误的 UTF8 编码输入，将生成一个特别的 Unicode 字符 ‘\uFFFD’，通常显示为 ‘�’ 。错误编码不会向后扩散是 UTF8 编码的优秀特性之一： 1fmt.Println("\xe4\x00\x00\xe7\x95\x8cabc") // �界abc 不过在 for range 迭代这个含有损坏的UTF8字符串时，第一字符的第二和第三字节依然会被单独迭代到，此时迭代的值是损坏后的0： 12345678910for i, c := range "\xe4\x00\x00\xe7\x95\x8cabc" &#123; fmt.Println(i, c)&#125;// 0 65533 // \uFFFD, 对应 �// 1 0 // 空字符// 2 0 // 空字符// 3 30028 // 界// 6 97 // a// 7 98 // b// 8 99 // c Go 语言除了 for range 语法对 UTF8 字符串提供了特殊支持外，还对字符串和 []rune 类型的相互转换提供了特殊的支持。 12fmt.Printf("%#v\n", []rune("世界")) // []int32&#123;19990, 30028&#125;fmt.Printf("%#v\n", string([]rune&#123;'世', '界'&#125;)) // 世界 上面可以看出 rune 只是 int32 的别名，用于表示每个 Unicode 码点，目前只使用了21个 bit 位。 字符串的强制类型转换涉及 []byte 和 []rune 两种类型。在将字符串转为 []byte 时，如果转换后的变量并没有被修改的情形，编译器可能会直接返回原始的字符串对应的底层数据。而在将字符串转为 []rune 时，由于字符串底层的 []byte 和 []int32 类型的内部布局完全不同，所以这种转换可能隐含重新分配内存的操作（重构字符串），最差的情况是时间复杂度达到 O(n)。 切片切片简单来说就是一种简化版的动态数组，实际使用中，切片比数组的使用范围广泛很多。下面是切片的结构定义，reflect.SliceHeader： 12345type SliceHeader struct &#123; Data uintptr Len int Cap int&#125; 切片的开头部分和 Go 字符串是一样的，但是切片多了一个 Cap 成员表示切片指向的内存空间的最大容量（对应元素的个数，而不是字节数）。和数组一样，内置的 len 函数返回切片中有效元素的长度，内置的 cap 函数返回切片容量大小，容量必须大于或等于切片的长度。切片可以和 nil 进行比较，只有当切片底层数据指针为空时切片本身为 nil，这时候切片的长度和容量信息将是无效的。如果有切片的底层数据指针为空，但是长度和容量不为0的情况，那么说明切片本身已经被损坏了（比如直接通过 reflect.SliceHeader 或 unsafe 包对切片作了不正确的修改）。 切片的定义方式如下： 1234567891011var ( a []int // nil切片, 和 nil 相等, 一般用来表示一个不存在的切片 b = []int&#123;&#125; // 空切片, 和 nil 不相等, 一般用来表示一个空的集合 c = []int&#123;1, 2, 3&#125; // 有3个元素的切片, len和cap都为3 d = c[:2] // 有2个元素的切片, len为2, cap为3 e = c[0:2:cap(c)] // 有2个元素的切片, len为2, cap为3 f = c[:0] // 有0个元素的切片, len为0, cap为3 g = make([]int, 3) // 有3个元素的切片, len和cap都为3 h = make([]int, 2, 3) // 有2个元素的切片, len为2, cap为3 i = make([]int, 0, 3) // 有0个元素的切片, len为0, cap为3) 切片的遍历、读取和修改都和数组一致。在对切片本身赋值或参数传递时，和数组指针的操作方式类似，只是复制切片头信息（reflect.SliceHeader），并不会复制底层的数据。对于类型，和数组的最大不同是，切片的类型和长度信息无关，只要是相同类型元素构成的切片均对应相同的切片类型。 如前所说，切片是一种简化版的动态数组，这是切片类型的灵魂。除了构造切片和遍历切片之外，添加切片元素、删除切片元素都是切片处理中经常遇到的问题。 添加切片元素内置的泛型函数 append 可以在切片的尾部追加 N 个元素： 1234var a []inta = append(a, 1) // 追加1个元素a = append(a, 1, 2, 3) // 追加多个元素, 手写解包方式a = append(a, []int&#123;1,2,3&#125;...) // 追加一个切片, 切片需要解包 不过要注意的是，在容量不足的情况下，append 的操作会导致重新分配内存，可能导致巨大的内存分配和复制数据代价。即使容量足够，依然需要用 append 函数的返回值来更新切片本身，因为新切片的长度已经发生了变化。 除了在切片的尾部追加，我们还可以在切片的开头添加元素： 123var a = []int&#123;1,2,3&#125;a = append([]int&#123;0&#125;, a...) // 在开头添加1个元素a = append([]int&#123;-3,-2,-1&#125;, a...) // 在开头添加1个切片 在开头一般都会导致内存的重新分配，而且会导致已有的元素全部复制1次。因此，从切片的开头添加元素的性能一般要比从尾部追加元素的性能差很多。 由于 append 函数返回新的切片，也就是它支持链式操作： 123var a []inta = append(a[:i], append([]int&#123;x&#125;, a[i:]...)...) // 在第i个位置插入xa = append(a[:i], append([]int&#123;1,2,3&#125;, a[i:]...)...) // 在第i个位置插入切片 或者用 copy 和 append 组合可以避免创建中间的临时切片： 123456789//在第i个位置添加一个元素a = append(a, 0) // 切片扩展1个空间copy(a[i+1:], a[i:]) // a[i:]向后移动1个位置a[i] = x // 设置新添加的元素//在第i个位置添加多个元素(切片)a = append(a, x...) // 为x切片扩展足够的空间copy(a[i+len(x):], a[i:]) // a[i:]向后移动len(x)个位置copy(a[i:], x) // 复制新添加的切片 人为扩容属于副作用，有违切片本身的设计思想。 删除切片元素删除尾部元素： 123a = []int&#123;1, 2, 3&#125;a = a[:len(a)-1] // 删除尾部1个元素a = a[:len(a)-N] // 删除尾部N个元素 删除头部元素： 1234567891011121314//移动指针a = []int&#123;1, 2, 3&#125;a = a[1:] // 删除开头1个元素a = a[N:] // 删除开头N个元素//不移动指针原地完成a = []int&#123;1, 2, 3&#125;a = append(a[:0], a[1:]...) // 删除开头1个元素a = append(a[:0], a[N:]...) // 删除开头N个元素//使用copy删除开头a = []int&#123;1, 2, 3&#125;a = a[:copy(a, a[1:])] // 删除开头1个元素a = a[:copy(a, a[N:])] // 删除开头N个元素 删除中间元素： 123456//使用copy和append组合原地完成a = []int&#123;1, 2, 3, ...&#125;a = append(a[:i], a[i+1:]...) // 删除中间1个元素a = append(a[:i], a[i+N:]...) // 删除中间N个元素a = a[:i+copy(a[i:], a[i+1:])] // 删除中间1个元素a = a[:i+copy(a[i:], a[i+N:])] // 删除中间N个元素 删除开头的元素和删除尾部的元素都可以认为是删除中间元素操作的特殊情况。 切片内存技巧对于切片来说， len 为0但是 cap 容量不为0的切片则是非常有用的特性。当然，如果 len 和 cap 都为 0 的话，则变成一个真正的空切片，虽然它并不是一个 nil 值的切片。在判断一个切片是否为空时，一般通过 len 获取切片的长度来判断，一般很少将切片和 nil 值做直接的比较。 0长切片特性使用案例： 12345678910111213141516171819202122//删除[]byte中的空格func TrimSpace(s []byte) []byte &#123; //len为0但是cap为s的长度，删除操作中append肯定不会超出cap b := s[:0] for _, x := range s &#123; if x != ' ' &#123; b = append(b, x) &#125; &#125; return b&#125;//普适到所有过滤删除需求func Filter(s []byte, fn func(x byte) bool) []byte &#123; b := s[:0] for _, x := range s &#123; if !fn(x) &#123; b = append(b, x) &#125; &#125; return b&#125; 切片高效操作的要点是要降低内存分配的次数，尽量保证 append 操作不会超出 cap 的容量，降低触发内存分配的次数和每次分配内存大小。 避免切片内存泄露如前面所说，切片操作并不会复制底层的数据。底层的数组会被保存在内存中，直到它不再被引用。但是有时候可能会因为一个小的内存引用而导致底层整个数组处于被使用的状态，这会延迟自动内存回收器对底层数组的回收。 例如，FindPhoneNumber 函数加载整个文件到内存，然后搜索第一个出现的电话号码，最后结果以切片方式返回。 1234func FindPhoneNumber(filename string) []byte &#123; b, _ := ioutil.ReadFile(filename) return regexp.MustCompile("[0-9]+").Find(b)&#125; 这段代码返回的 []byte 指向保存整个文件的数组。因为切片引用了整个原始数组，导致自动垃圾回收器不能及时释放底层数组的空间。一个小的需求可能导致需要长时间保存整个文件数据。这虽然这并不是传统意义上的内存泄漏，但是可能会拖慢系统的整体性能。 要修复这个问题，可以将感兴趣的数据复制到一个新的切片中（数据的传值是Go语言编程的一个哲学，虽然传值有一定的代价，但是换取的好处是切断了对原始数据的依赖）： 12345func FindPhoneNumber(filename string) []byte &#123; b, _ := ioutil.ReadFile(filename) b = regexp.MustCompile("[0-9]+").Find(b) return append([]byte&#123;&#125;, b...)&#125; 类似的问题，在删除切片元素时可能会遇到。假设切片里存放的是指针对象，那么下面删除末尾的元素后，被删除的元素依然被切片底层数组引用，从而导致不能及时被自动垃圾回收器回收（这要依赖回收器的实现方式）： 12var a []*int&#123; ... &#125;a = a[:len(a)-1] // 被删除的最后一个元素依然被引用, 可能导致GC操作被阻碍 保险的方式是先将需要自动内存回收的元素设置为 nil，保证自动回收器可以发现需要回收的对象，然后再进行切片的删除操作： 123var a []*int&#123; ... &#125;a[len(a)-1] = nil // GC回收最后一个元素内存a = a[:len(a)-1] // 从切片删除最后一个元素 当然，如果切片本身生命周期短的话完全不需要这样，等待整个切片被 GC 回收即可。 切片强制类型转换为了安全，当两个切片类型 []T 和 []Y 的底层原始切片类型不同时，Go语言是无法直接转换类型的。但是有时候转换有简化编码或者提升性能的价值，比如在64位系统上，需要对一个 []float64 切片进行高速排序，我们可以将它强制转为 []int 整数切片，然后以整数的方式进行排序（因为 float64 遵循 IEEE754 浮点数标准特性，当浮点数有序时对应的整数也必然是有序的）。下面的代码通过两种方法将 []float64 类型的切片转换为 []int 类型的切片： 123456789101112131415161718192021222324// +build amd64 arm64import "sort"var a = []float64&#123;4, 2, 5, 7, 2, 1, 88, 1&#125;func SortFloat64FastV1(a []float64) &#123; // 强制类型转换 var b []int = ((*[1 &lt;&lt; 20]int)(unsafe.Pointer(&amp;a[0])))[:len(a):cap(a)] // 以int方式给float64排序 sort.Ints(b)&#125;func SortFloat64FastV2(a []float64) &#123; // 通过 reflect.SliceHeader 更新切片头部信息实现转换 var c []int aHdr := (*reflect.SliceHeader)(unsafe.Pointer(&amp;a)) cHdr := (*reflect.SliceHeader)(unsafe.Pointer(&amp;c)) *cHdr = *aHdr // 以int方式给float64排序 sort.Ints(c)&#125; 第一种强制转换是先将切片数据的开始地址转换为一个较大的数组的指针，然后对数组指针对应的数组重新做切片操作。中间需要 unsafe.Pointer 来连接两个不同类型的指针传递。需要注意的是，Go 语言实现中非0大小数组的长度不得超过2GB，因此需要针对数组元素的类型大小计算数组的最大长度范围（[]uint8 最大2GB，[]uint16 最大1GB，以此类推，但是 []struct{} 数组的长度可以超过2GB）。 第二种转换操作是分别取到两个不同类型的切片头信息指针，任何类型的切片头部信息底层都是对应 reflect.SliceHeader 结构，然后通过更新结构体方式来更新切片信息，从而实现 a 对应的 []float64 切片到 c 对应的 []int 类型切片的转换。 不过需要注意的是，这个方法可行的前提是要保证 []float64 中没有 NaN 和 Inf 等非规范的浮点数（因为浮点数中 NaN 不可排序，正0和负0相等，但是整数中没有这类情形）。 Other数组和切片在传参上的差异直接看例子： 123456789101112131415161718192021func test() &#123; arr := [...]int&#123;0, 1, 2&#125; fmt.Printf("%v\n", arr) slice := []int&#123;3, 4, 5&#125; fmt.Printf("%v\n", slice) changeArrayItem(arr) changeSliceItem(slice) fmt.Printf("%v\n", arr) fmt.Printf("%v\n", slice)&#125;func changeArrayItem(base [3]int) &#123; base[0] = 10&#125;func changeSliceItem(base []int) &#123; base[0] = 10&#125; 12345//output[0 1 2][3 4 5][0 1 2][10 4 5] 可以看出数组是值传递，切片是引用传递，而且数组形参在定义时必须准确定义长度。 切片在传参上还有一个坑，就是 append 操作造成的引用丢失： 123456789101112func appendTest() &#123; slice := []int&#123;3, 4, 5&#125; fmt.Printf("before: %v\n", slice) appendSliceItem(slice) fmt.Printf("after: %v\n", slice)&#125;func appendSliceItem(slice []int) &#123; slice = append(slice, 100) fmt.Printf("func: %v\n", slice)&#125; 1234//outputbefore: [3 4 5]func: [3 4 5 100]after: [3 4 5] 由于 append 返回的是一个新的切片引用，因此在函数内的添加变更只体现在新切片引用上。除非函数返回新的切片引用，否则原上下文无法让旧引用重新赋值为新切片的引用。要解决这个问题除了直接用函数返回值更新外，还可以用引用的引用来解决： 123456789101112func appendTest() &#123; slice := []int&#123;3, 4, 5&#125; fmt.Printf("before: %v\n", slice) appendSliceItem(&amp;slice) fmt.Printf("after: %v\n", slice)&#125;func appendSliceItem(slice *[]int) &#123; *slice = append(*slice, 100) fmt.Printf("func: %v\n", *slice)&#125; 1234//outputbefore: [3 4 5]func: [3 4 5 100]after: [3 4 5 100] 下面用一个递归例子在展示两种方式： 123456789101112131415161718192021222324252627func recursiveTest() &#123; arr1 := []int&#123;&#125; insertTo10(&amp;arr1) fmt.Println(arr1) arr2 := []int&#123;&#125; arr2 = insertTo10V2(arr2) fmt.Println(arr2)&#125;func insertTo10(arr *[]int) &#123; length := len(*arr) if length == 10 &#123; return &#125; *arr = append(*arr, length) insertTo10(arr)&#125;func insertTo10V2(arr []int) []int &#123; length := len(arr) if length == 10 &#123; return arr &#125; arr = append(arr, length) return insertTo10V2(arr)&#125; 123//output[0 1 2 3 4 5 6 7 8 9][0 1 2 3 4 5 6 7 8 9]]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go struct{}类型Channel]]></title>
    <url>%2F2020%2F06%2F02%2FGo-struct-%E7%B1%BB%E5%9E%8BChannel%2F</url>
    <content type="text"><![CDATA[IntroductionGo 中的 Channel 除了通信功能，往往还会扮演信号量或阻塞控制的角色。后者往往只需要 Channel 的语言特性，并不关注 Channel 传输的内容。在此背景下就有了 struct{} 类型 Channel 的使用场景，该类型 Channel 不占用任何内存。该 Channel 只可以写入 struct{}{} ，即空结构体。 应用1. 等待任务结束12345678done := make(chan struct&#123;&#125;)go func() &#123; doLongRunningThing() close(done)&#125;()// do some other things// wait for that long running thing to finish&lt;-done 2. 多任务同时开始实现类似并行而非并发的效果 12345678910start := make(chan struct&#123;&#125;)for i := 0; i &lt; 10000; i++ &#123; go func() &#123; &lt;-start // wait for the start channel to be closed doWork(i) // do something &#125;()&#125;// at this point, all goroutines are ready to go// we just need to tell them to start by closing the start channelclose(start) 3. 事件中断12345678for &#123; select &#123; case m := &lt;-email: sendEmail(m) case &lt;-stop: // triggered when the stop channel is closed break // (or return) exit &#125;&#125;]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Concurrency in Go]]></title>
    <url>%2F2020%2F05%2F16%2FConcurrency-in-Go%2F</url>
    <content type="text"><![CDATA[1. Why Use Concurrency？1.1 Parallel Execution Two programs execute in parallel if they execute at exactly the same time At time t, an instruction is being performed for both P1 and P2 Need replicated hardware Why Use Parallel Execution？ Tasks may complete more quickly Some tasks must be performed sequentially But some tasks are parallelizable and some are not 1.2 Von Neumann BottleneckSpeedup without Parallelism Can we achieve speedup without Parallelism？ Design faster processors Get speedup without changing software Design processor with more memory Reduces the Von Neumann Bottleneck Cache access time = 1 clock cycle Main memory access time = ~100 clock cycles Increasing on-chip cache improves performance Moore’s Law Predicted that transistor density would double every two years Smaller transistors switch faster Not a physical law, just an observation Exponential increase in density would lead to exponential increase in speed 1.3 Power wallPower/Temperature Problem Transistors consume power when they switch Increasing transistor density leads to increased power consumption Smaller transistors use less power, but density scaling is much faster High power leads to high temperature Air cooling(fans) can only remove so much heat Dynamic Power P = α * CFV2 α is percent of time switching C is capacitance(related to size) F is the clock frequency V is voltage swing（from low to high） Voltage is important 0 to 5V uses much more power than 0 to 1.3V Dennard Scaling Voltage should scale with transistor size Keeps power consumption and temperature low Problem: Voltage can’t go too low Must stay above threshold voltage Noise problems occur Problem: Doesn’t consider leakage power Dennard scaling must stop Multi-Core Systems P = α * CFV2 Cannot increase frequency Can still add processor cores, without increasing frequency Trend is apparent today Parallel execution is needed to exploit multi-core systems Code made to execute on multiple cores Different programs on different cores 1.4 Concurrent vs ParallelConcurrent execution Concurrent execution is not necessarily the same as parallel execution Concurrent: start and end times overlap Parallel: execute at exactly the same time Parallel tasks must be executed on different hardware Concurrent tasks may be executed on the same hardware Only one task actually executed at a time Mapping from tasks to hardware is not directly controlled by the programmer At least not in go 1.5 Concurrent Programming Programmer determines which tasks can be executed in parallel Mapping tasks to hardware Operating system Go runtime schedule Benefit 1: Hiding Latency Concurrency improves performance, even without parallelism Tasks must periodically wait for something i.e. wait for memory X = Y + Z read Y, Z from memory May wait 100+ clock cycles Other concurrent tasks can operate while one task is waiting Benefit 2: Hardware Mapping Hardware Mapping in Go: Programmer does not determine the hardware mapping Programmer makes parallelism possible Hardware mapping depends on many factors Where is the data? What are the communication costs? 2. Concurrency Basics2.1 Processes An instance of a running program Things unique to a process Memory Virtual address space Code, stack, heap, shared libraries Registers Program counter, data regs, stack ptr, … Operating Systems Allows many processes to execute concurrently Processes are switched quickly (20ms) User has the impression of parallelism Operating system must give processes fair access to resources 2.2 SchedulingScheduling Processes Operating system schedules processes for execution Gives the illusion of parallel execution OS gives fair access to CPU, memory, etc. Context Switch Control flow changes from one process to another Process “context” must be swapped 2.3 Threads and GoroutinesThreads vs Processes Threads share some context Many threads can exist in one process OS schedules threads rather than proccesses Goroutines Like a thread in Go Many Goroutines execute within a single OS thread Go Runtime Scheduler Schedules goroutines inside an OS thread Like a little OS inside a single OS thread Logical processor is mapped to a thread 2.4 Interleavings Order of executive within a task is known Order of executive between concurrent tasks is unknown Interleaving of instructions between tasks is unknown Many interleavings are possible Must consider all possibilities Ordering is non-deterministic 2.5 Race Conditions Outcome depends on non-deterministic ordering Races occur due to communication 2.6 Communication Between Tasks Threads are largely independent but not completely independent Web server, one thread per client Image processing, 1 thread per pixel block 3. Threads in Go3.1 GoroutinesCreating a Goroutine One goroutine is created automatically to execute the main() Other goroutines are created using the go keyword Exiting a Goroutine A goroutine exits when its code is complete When the main goroutine is complete, all other goroutines exit A goroutine may not complete its execution because main completes early Adding a delay to wait for a goroutine is bad, because timing assumptions may be wrong and timing is nondeterministic Need formal synchronization constructs 3.2 Basic Synchronization Using global events whose execution is viewed by all threads, simultaneously Example: Global event is viewed by all tasks at the same time Print must occur after update of x Synchronization is used to restrict bad interleavings In this case, synchronization reduces performance and efficiency. It’s bad, but it’s necessary here. 3.3 Wait GroupsSync WaitGroup Sync package contains functions to synchronize between goroutines sync.WaitGroup forces a goroutine to wait for other goroutines Contains an internal counter Increment counter for each goroutine to wait for Decrement counter when each goroutine completes Waiting goroutine cannot continue until counter is 0 Using WaitGroup Add() increments the counter Done() decrements the counter Wait() blocks until counter == 0 WaitGroup Example123456789101112func foo(wg *sync.WaitGroup) &#123; fmt.Printf("New routine") wg.Done()&#125;func main() &#123; var wg sync.WaitGroup wg.Add(1) go foo(&amp;wg) wg.Wait() fmt.Printf("Main routine")&#125; 3.4 CommunicationGoroutine Communication Goroutines usually work together to perform a bigger task Often need to send data to collaborate Example: Find the product of 4 integers Make 2 goroutines, each multiplies a pair Main goroutine multiplies the 2 results Need to send ints from main routine to the two sub-routines Need to send results from sub-routines back to main routine Channels Transfer data between goroutines Channels are typed Use make() to create a channel 1c := make(chan int) Send and receive data using the &lt;- operator Send data on a channel 1c &lt;- 3 Receive data from a channel 1x := &lt;- c Channel example: 123456789101112func prod(v1 int, v2 int, c chan int) &#123; c &lt;- v1 * v2&#125;func main() &#123; c := make(chan int) go prod(1, 2, c) go prod(3, 4, c) a := &lt;- c b := &lt;- c fmt.Println(a * b)&#125; 3.5 Blocking in ChannelsUnbuffered Channel Unbuffered channels cannot hold data in transit Default is unbuffered Sending blocks until data is received Receiving blocks until data is sent Blocking and Synchronization Channel communication is synchronous Blocking is the same as waiting for communication Receiving and ignoring the result is the same as a Wait() 3.6 Buffered ChannelsChannel Capacity Channels can contain a limited number of objects Default size 0(unbuffered) Capacity is the number of objects it can hold in transit Optional argument to make() defines channel capacity 1c := make(chan int, 3) Sending only blocks if buffer is full Receiving only blocks if buffer is empty Channel Blocking, Receive Channel with capacity 1 First receive blocks until send occurs Second receive blocks forever Channel Blocking, Send Second send blocks until receive is done Receive can block until first send is done Use of Buffering Sender and receiver do not need to operate at exactly the same speed Speed mismatch is acceptable 4. Synchronized Communication4.1 Blocking on ChannelsIterating Through a Channel Common to iteratively read from a channel 123for i := range c &#123; fmt.Println(i)&#125; Continues to read from channel c One iteration each time a new value is received i is assigned to the read value Iterates when sender calls close(c) Receiving from Multiple Goroutines Multiple channels may be used to receive from multiple sources Data from both sources may be needed Read sequentially 123a := &lt;- c1b := &lt;- c2fmt.Println(a*b) Select Statement May have a choice of which data to use i.e. First-come first-served Use the select statement to wait on the first data from a set of channels 123456select &#123; case a = &lt;- c1: fmt.Println(a) case b = &lt;- c2: fmt.Println(b)&#125; 4.2 SelectSelect Send or Receive May select either send or receive operations 123456select &#123; case a = &lt;- inchan: fmt.Println("Received a") case outchan &lt;- b: fmt.Println("Sent b")&#125; Select with an Abort channel Use select with a separate abort channel May want to receive data until an abort signal is received 12345678for &#123; select &#123; case a = &lt;- c: fmt.Println(a) case &lt;-abort: return &#125;&#125; Default Select May want a default operation to avoid blocking 12345678select &#123; case a = &lt;- c1: fmt.Println(a) case b = &lt;- c2: fmt.Println(b) default: fmt.Println("nop")&#125; 4.3 Mutual ExclutionGoroutines Sharing Variables Sharing variables concurrently can cause problems Two goroutines writing to a shared variable can interfere with each other Function can be invoked concurrently without interfering with other goroutines(Concurrency-Safe) Variable Sharing Example12345678910111213var i int = 0var wg sync.WaitGroupfunc inc() &#123; i = i + 1 wg.Done()&#125;func main() &#123; wg.Add(2) go inc() go inc() wg.Wait() fmt.Println(i)&#125; Two goroutines write to i i should equal 2, but this doesn’t always happen. Because there are some possible interleavings Granularity of Concurrency Concurrency is at the machine code level i = i + 1 might be three machine instructions Interleaving machine instructions causes unexpected problems Interleaving machine instructions Both tasks read 0 for 1 value 4.4 MutexCorrect Sharing Don‘t let 2 goroutines write to a shared variable at the same time! Need to restrict possible interleavings Access to shared variables cannot be interleaved Code segements in different goroutines which cannot execute concurrently(Mutual Exclusion) Writing to shared variables should be mutually exclusive Sync.Mutex A Mutex ensures mutual exclusion Uses a binary semaphore Flag up - shared variable is in use Flag down - shared variable is available 4.5 Mutex Methods#####Sync.Mutex Methods Lock() method puts the flag up Shared variable in use If lock is already taken by a goroutine, Lock() blocks until the flag is put down Unlock() method puts the flag down Done using shared variable When Unlock() is called, a blocked Lock() can proceed Using Sync.Mutex Increment operation is now mutually exclutive 1234567var i int = 0var mut sync.Mutexfunc inc() &#123; mut.Lock() i = i + 1 mut.Unlock()&#125; 4.6 Once SynchronizationSynchronous Initialization Initialization must happen once must happen before everything else How do you perform initialization with multiple goroutines? Could perform initialization before starting the goroutines? Sync.Once Has one method, once.Do(f) Function if is executed only one time Even if it is called in multiple goroutines All calls to once.Do() block until the first returns Ensures that initialization executes first #####Sync.Once Example Make two goroutines, initialization only once Each goroutine executes dostuff() 12345678var wg sync.WaitGroupfunc main() &#123; wg.Add(2) go dostuff() go dostuff() wg.Wait()&#125; Using Sync.Once setup() should execute only once “hello” should not print until setup() returns 123456789var on sync.Oncefunc setup() &#123; fmt.Println("Init")&#125;func dostuff() &#123; on.Do(setup) fmt.Println("hello") wg.Done()&#125; Execution Result 123Init //Result of setup()hello //Result of one goroutinehello //Result of the other goroutine Init appears only once Init appears before hello is printed 4.7 DeadlockSynchronization Dependencies Synchronization causes the execution of different goroutines to depend on each other G2 cannot continue until G1 does something Deadlock Circular dependencies cause all involved goroutines to block G1 waits for G2 G2 waits for G1 Can be caused by waiting on channels Deadlock Example12345func dostuff(c1 chan int, c2 chan int) &#123; &lt;- c1 c2 &lt;- 1 wg.Done()&#125; Read from first channel Wait for write onto first channel Write to second channel Wait for read from second channel 12345678func main() &#123; ch1 := make(chan int) ch2 := make(chan int) wg.Add(2) go dostuff(ch1, ch2) go dostuff(ch2, ch1) wg.Wait()&#125; dostuff() argument order is swapped Each goroutine blocked on channel read Deadlock Detection Golang runtime automatically detects when all goroutines are deadlocked Cannot detect when a subset of goroutines are deadlocked 4.8 Dining PhilosophersDining Philosophers Problem Classical problem involvingconcurrency and synchronization Problem: 5 philosophers sitting at a round table 1 chopstick is placed between each adjacent pair Want to eat rice from their plate, but needs two chopsticks Only one philosopher can hold a chopstick at a time Not enough chopsticks for everyone to eat at once Each chopstick is a mutex Each philosopher is associated with a goroutine and two chopsticks Chopsticks and Philosophers 1234567type Chops struct &#123; sync.Mutex&#125;type Philo struct &#123; leftCS, rightCS *Chops&#125; Philosopher Eat Method 1234567891011func(p Philo) eat() &#123; for &#123; p.leftCS.Lock() p.rightCS.Lock() fmt.Println("eating") p.rightCS.Unlock() p.leftCS.Unlock() &#125;&#125; Initialization in Main 12345678CSticks := make([]*Chops, 5)for i := 0; i &lt; 5; i++ &#123; CSticks[i] = new(ChopS)&#125;philos := make([]*Philo, 5)for i := 0; i &lt; 5; i++ &#123; philos[i] = &amp;Philo&#123;Csticks[i], Csticks[(i + 1) % 5]&#125;&#125; Initialize chopsticks and philosophers Notice (i + 1) % 5] Start the Dining in Main 123for i := 0; i &lt; 5; i++ &#123; go philos[i].eat()&#125; Start each philosopher eating Would also need to wait in the main Deadlock Problem 12345p.leftCS.Lock()p.rightCS.Lock()fmt.Println("eating")p.leftCS.Unlock()p.rightCS.Unlock() All philosophers might lock their left chopsticks concurrently All chopsticks would be locked Noone can lock their right chopsticks Deadlock Solution Each philosopher picks up lowest numbered chopstick first 1234567for i := 0; i &lt; 5; i++ &#123; if i &lt; (i + 1) % 5 &#123; philos[i] = &amp;Philo&#123;Csticks[i], Csticks[(i + 1) % 5]&#125; &#125; else &#123; philos[i] = &amp;Philo&#123;Csticks[(i + 1) % 5], Csticks[i]&#125; &#125;&#125; Philosopher 4 picks up chopstick 0 before chopstick 4 Philosopher 4 blocks allowing philosopher 3 to eat No deadlock, but philosopher 4 may starve]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker搭建FTP]]></title>
    <url>%2F2020%2F04%2F22%2FDocker%E6%90%AD%E5%BB%BAFTP%2F</url>
    <content type="text"><![CDATA[Docker 镜像拉取1docker pull fauria/vsftpd 创建容器直接添加用户名和密码创建容器，需要修改用户名、密码、宿主机IP： 1docker run -d -p 20:20 -p 21:21 -p 21100-21110:21100-21110 -v /Ftpfile:/home/vsftpd -e FTP_USER=user -e FTP_PASS=userpwd -e PASV_ADDRESS=&lt;宿主机ip&gt; -e PASV_MIN_PORT=21100 -e PASV_MAX_PORT=21110 --name vsftpd --restart=always fauria/vsftpd 或创建容器后再设置用户名和密码： 1docker run -d -p 20:20 -p 21:21 -p 21100-21110:21100-21110 -v /Ftpfile:/home/vsftpd -e PASV_ADDRESS=&lt;宿主机ip&gt; -e PASV_MIN_PORT=21100 -e PASV_MAX_PORT=21110 --name vsftpd --restart=always fauria/vsftpd 进入容器并修改账号密码1docker exec -i -t vsftpd bash 1vi /etc/vsftpd/virtual_users.txt 奇数行为用户名，偶数行为密码 创建用户文件夹1mkdir -p /home/vsftpd/&lt;新用户名&gt; 刷新用户配置1/usr/bin/db_load -T -t hash -f /etc/vsftpd/virtual_users.txt /etc/vsftpd/virtual_users.db 退出容器并重启1exit 1docker restart vsftpd 验证访问 ftp://&lt;宿主机ip&gt; 优化 支持中文字符集 1docker exec -i -t vsftpd bash 1vi /etc/profile 添加 LANG12```source /etc/profile 退出并重启容器]]></content>
      <categories>
        <category>web</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go Garbage Collection]]></title>
    <url>%2F2019%2F10%2F30%2FGo-Garbage-Collection%2F</url>
    <content type="text"><![CDATA[前言go 语言的 GC 代码可以在源码文件 src/runtime/mgc.go 看到，其注释看门见山地概括了 go 的 GC： The GC runs concurrently with mutator threads, is type accurate (aka precise), allows multiple GC thread to run in parallel. It is a concurrent mark and sweep that uses a write barrier. It is non-generational and non-compacting. Allocation is done using size segregated per P allocation areas to minimize fragmentation while eliminating locks in the common case. 总结为一句话就是：go 的 GC 算法是非分代、非紧缩、写屏障的三色并发标记清理算法。 非分代：go GC 并没有像 Java 一样分新生代和老年代，所以也不存在 minor gc 和 full gc 之分 非紧缩：go GC 之后不会整理内存，清理内存碎片 写屏障：类似于 Java 的 G1(Garbage-First) 垃圾收集器中的并发标记，写屏障保证了 GC 期间用户线程 (mutator) 进行的内存更改能被监视，避免了标记遗漏和错误标记 标记清除(Mark And Sweep)算法该算法主要有两个步骤：标记和清除。标记阶段对对象进行可达性分析，并进行标记。清除阶段则回收未被标记的对象。 图片所示就是标记清除的全过程，标记清除虽然简单，但是这也带来了很多问题： STW(stop the world): 标记清除在执行1之前要停止程序，并在4之后恢复运行，执行 GC 期间需要暂停用户线程，造成程序卡顿 标记对象时需要扫描整个 heap，费时费力 清除数据后会产生不连续的 heap 碎片 三个问题中，STW 是首要问题，于是 go 使用了三色并发标记法来解决。 三色并发标记法：基本步骤三色并发标记法为对象定义了三种颜色(状态)： 黑色：对象在这次 GC 中已标记，且这个对象包含的子对象也已标记 灰色：对象在这次 GC 中已标记， 但这个对象包含的子对象未标记 白色：对象在这次 GC 中未标记 步骤解析： 程序创建的对象都标记为白色 gc 开始：扫描所有可到达的对象，标记为灰色 从灰色对象中找到其引用对象标记为灰色，把灰色对象本身标记为黑色 监视对象中的内存修改，并持续上一步的操作，直到灰色标记的对象不存在 gc 回收白色对象 最后，将所有黑色对象变为白色，并重复以上所有过程 三色并发标记法：GC 和用户线程的并行mark and sweep 的 STW 操作，就是 runtime 把所有的线程全部冻结掉，所有的线程全部冻结意味着用户逻辑是暂停的。这样所有的对象都不会被修改了，这时候去扫描是绝对安全的。 go 如何减短这个过程呢？mark and sweep 包含两部分逻辑：标记和清除。 我们知道三色标记法中最后只剩下的黑白两种对象，黑色对象是程序恢复后接着使用的对象，如果不碰触黑色对象，只清除白色的对象，肯定不会影响程序逻辑。所以清除操作和用户逻辑可以并发。 但是标记操作和用户逻辑也是并发的，用户逻辑会时常生成对象或者改变对象的引用，那么标记和用户逻辑如何并发呢？ 并发问题一：process 新生成对象的时候，GC 该如何操作呢？如下图，process 在标记阶段生成一个新对象，我们可能会这么认为： 但是这样显然是不对的，因为按照三色标记法的步骤，这样新生成的对象 A 最后会被清除掉，这样会影响程序逻辑。Golang 为了解决这个问题，引入了写屏障这个机制。 写屏障：该屏障之前的写操作和之后的写操作相比，先被系统其它组件感知。 通俗的讲：就是在 gc 跑的过程中，可以监控对象的内存修改，并对对象进行重新标记。(实际上也是超短暂的 STW，然后对对象进行标记) 在上述情况中，新生成的对象，一律都标为灰色！ 即下图： 并发问题二：灰色或者黑色对象的引用改为白色对象的时候，Golang 是该如何操作？看如下图，一个黑色对象引用了曾经标记的白色对象。 这时候，写屏障机制被触发，向 GC 发送信号，GC 重新扫描对象并标位灰色。 因此，GC 一旦开始，无论是创建对象还是对象的引用改变，都会先变为灰色。 Go GC 的未来尽管目前 go GC 相比以往版本已经大有改进，效率也今非昔比，但是仍存在一些痛点，所以 go 团队有意在未来的版本尝试 Java 上比较先进的分代思想。 分代思想主要是能解决当前 go 的 GC 频繁问题，在标记阶段 go 需要一定的 CPU 资源来 Mark Scan 所有对象，导致 GC 的 CPU 消耗比较高。 另外，相对于增加 CPU 消耗(比如写屏障)的方案， Go 团队会更倾向于占用内存多一些方案。因为 Go 团队认为，CPU 的摩尔定律发展已经减缓，18个月翻倍减缓为2年，4年…而内存容量和价格的摩尔定律仍在继续。一个稍微更占用内存的解决方案比更占用 CPU 的解决方案拥有更好的扩展性。]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2019%2F10%2F20%2Fgit%2F</url>
    <content type="text"><![CDATA[First Step1234git config --global user.name "xxx"git config --global user.email "xxx@gmail.com"git config --global color.ui true Create repo12git initgit add . .git 目录说明： hooks：记录一些校验用的 goal 值 info：配置，如 .gitignore objects：主要目录，只要这个不丢所有信息都可以还原 refs：记录分支 remotes：远端分支 1git commit -m ’first commit‘ 此时 objects 和 refs 会略有变化 Clone One1git clone repourl A Basic Workflow Edit file：vim / emacs / etc Stage the changes：git add (file) Review your changes：git status / git diff Commit the changes：git commit Branching and MergingBranching创建分支 1git branch xxx 查看分支状态 1git branch 切换分支 1git checkout xxx(分支号) 删除分支（-d 只可删除已被 merge 的空指针，-D 强制删除分支指针） 12git branch -d xxxgit branch -D xxx Merging合并分支 1git merge xxx 可视化解决冲突 1git mergetool Collaboration拉取远端代码 1git clone xxx 提交代码到远端，如果远端被别人修改了，此时会 push 失败 1git push origin master 失败的话就要先把远端的新版代码 fetch 下来 1git fetch 然后把 fetch 下来的新代码和自己改的代码 merge 之后就可以 push 了，或者直接 pull，不建议使用，git pull = git fetch + git merge 或者用 rebase 保证线性历史（下图等价于 git pull –rebase = git fetch + git rebase） 查看1git log xxx(默认 HEAD) –oneline：简单参数展示 –graph：简易图像 –all：把所有分支展示 –decorate：把分支名也展示 返回两图差集： 1git log A ^B 多人协作常用流程 新建远程分支 git clone git branch -avv fit checkout -b dev origin/… git push origin &lt;本地分支名&gt;:&lt;远程分支名&gt; others: 与远程代码同步 1git pull origin &lt;分支名&gt; 更新远程路径列表 1git remote update origin --prune xxx 添加远程路径 1git remote add origin xxx]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2F2019%2F10%2F18%2FElasticSearch%2F</url>
    <content type="text"><![CDATA[定义”一个便于检索的数据库“ 基于 Lucene 的开源搜索引擎，分布式、可扩展、每个字段都能被索引。 能做什么？ 海量搜索 NoSQL 数据库 对海量数据进行进实时的处理 日志数据分析，ELK 技术，ES 进行复杂的数据分析 架构设计设计目标：为了更好的写入（索引）和查询（搜索） 自底向上： 网关，es 索引的持久化存储方式 Lucene，为 es 提供 api 工具包 索引模块，搜索模块，字段映射，river 数据同步 自动发现，es 的自动发现节点机制 通信层，es 和客户端的交互方式 Restful api，封装的一些 crud 操作 Restful api对于向 es 插入数据、检索数据、删除数据等操作，es 提供了 Java api 和 RESTful api 两种方式来与之通信。 transport通信模块，节点间数据传输都依赖于该模块，它有两种实现，一种是基于 netty 实现的 nettytransport，主要用于节点间的通信。另一种是 localtransport，主要是用于同一个 jvm 上的节点通信。 discovery script自动发现模块，集群中节点的自动发现和 Master 节点的选举。p2p 通信。 Master 节点维护集群的全局状态（比如节点加入和离开时进行 shard 的重新分配） Master 节点选取，bully 算法+延迟选举+半数选票 故障检测：主节点-&gt;其他节点，其他节点-&gt;主节点 重要模块Index 模块：对写入数据的管理和组织 Search 模块：用来提供信息搜索 Mapping映射模块：类似于数据库表字段的定义，并且它决定了各个字段能否被搜索，以及搜索方式 River 模块：es 的一个数据源，向 es 同步数据。river data -&gt; es Lucene一个全文检索引擎框架，java 类库，提供底层 api，每个 es 节点上都有一个 Lucene 引擎支持。 gateway存储索引的文件系统，支持多种文件类型 核心原理几个重要概念 index type document mapping node shard segment serach 索引 类型 文档 映射 节点 分片 段 倒排索引 从数据库角度来看 Relational database Elasticsearch Database Index Table Type Row Document Column Field Schema Mapping Index Everything can index SQL Query DSL SELECT * FROM table … GET http://… UPDATE table SET … PUT http://… 索引 indexes 将数据存储于一个或多个索引中，索引是具有类似特性的文档的集合。类比传统的关系型数据库领域来说，索引相当于 SQL 中的一个数据库。 类型 type类型是索引内部的逻辑分类或分区（category/partition），一个索引内部可定义一个或多个类型（type）。通常，会为具有一组共同字段的文档定义一个类型。 文档 document文档是可被索引的基础信息单元，它是包含了一个或多个域的容器，基于 JSON 格式进行表示。 映射 mapping用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、分词器等属性。映射可以分为静态映射和动态映射。 静态映射：事先定义字段的数据类型、分词器等属性。 动态映射：根据写入的字段进行类型推测。 节点 node运行了单个实例的 es 主机称为节点（一个 es 实例就是一个 node），可以存储数据、参与集群索引及搜索操作。 分片 shardes 提供了将索引划分成多少份的能力，这些份就叫做分片。相当于一桶水用了 N 个杯子装。每个分片其内部都是一个全功能且独立的索引（Lucene Index）。创建索引时，用户可指定其分片的数量，默认数量为5个，一个分片只能存放 2,147,483,519 个 docs。 主分片和备分片：主分片和备分片不会出现在同一个节点上（防止单点故障） 分片和节点的配合，让 es 的分布式愿望得以实现。 node + shard 实现 es 的扩容 Node 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点 整体负载降低。性能提升 我们这个拥有6个分片（3个主分片和3个副本分片）的索引可以最大扩容到6个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。 那如何实现更多的扩容？如果我们想要扩容超多6个节点怎么办呢？ 主分片的数量是确定的，增加备分片的数量，实现更高的吞吐量。 段 segmentElasticsearch 中的每个分片包含多个 segment，每一个 segment 都是一个倒排索引；在查询时，会把所有的 segment 查询结果汇总归并后作为最终的分片查询结果，然后返回。 倒排索引也常被称为反向索引，是一种索引方法，它是文档检索系统中最常用的数据结构。 索引原理es 索引本质：让数据安全快速写入，并且能快速搜索到。 es 的索引过程：对文档的增删改查，以及文档对应的倒排索引的更新。 对原文档的操作（写、读、更新）写操作写操作必须在 primary shard （主分片）完全成功后才能拷贝至其对应的 replicas （附属分片）上，默认情况下主分片等待所有备份完成索引后才返回客户端。 客户端向 Node 1 发送索引文档请求。 Node 1 根据文档 ID (_id 字段) 计算(对 id 做哈希然后取模)出该文档应该属于 shard0，然后请求路由到 Node 3 的 P0 分片上。 Node 3 在 P0 上执行了请求。如果请求成功，则将请求并行的路由至 Node 1，Node 2 的 R0 上。当所有的 Replicas 报告成功后，Node 3 向请求的 Node (Node 1) 发送成功报告，Node 1 再报告至 Client。 当客户端收到执行成功后，操作已经在 Primary shard 和所有的 replicas shards 上执行成功了。 读操作一个文档可以在 primary shard 和所有的 replica shards 上读取 客户端发送 Get 请求到 Node 1。 Node 1 根据文档 ID (_id 字段) 计算出该文档应该属于 shard0，且 shard0 的所有拷贝存在于所有3个节点上。这次，他将路由至 Node 2。 Node 2 将文档返回给 Node 1，Node 1 将文档返回给客户端。对于读请求，请求节点 (Node 1) 将在每次请求到来时选择一个不同的 replica shard 来达到负载均衡。使用轮询策略轮询所有的 replica shards。 更新操作结合了以上的两个操作：读、写 客户端发送更新操作请求至 Node 1。 Node 1 将请求路由至 Node 3，Primary shard 所在的位置。 Node 3 从 P0 读取文档，改变 source 字段的 JSON 内容，然后试图重新对修改后的数据在 P0 做索引。如果此时这个文档已经被其他的进程修改了，那么它将重新执行 3 步骤，这个过程如果超过了 retryon_conflict 设置的次数，就放弃。 如果 Node 3 成功更新了文档，它将并行地把新版本的文档同步到 Node 1 和 Node 2 的 replica shards 重新建立索引。一旦所有的 replica shards 报告成功，Node 3 向被请求的节点(Node 1)返回成功，然后 Node 1 向客户端返回成功。 生成倒排索引如果仅仅只是生成文档，那么 es 的搜索性能会很低，所以，在建立索引时，会产生一个对应的倒排索引（Inverted Index）。 es 引擎把文档数据写入到倒排索引的数据结构中，建立起分词 (Term) -&gt; 文档 (Document) 的映射关系。 这些倒排索引会存放在段 (segment) 中，段的写入会落盘，缓存 buffer 会实时更新。 倒排索引包括两个部分： 有序的数据字典 Dictionary（包括单词 Term 和它出现的频率） 单词 Term 对应的 Postings（即存在这个单词的文件） 为了优化搜索，segment 不仅只提供了倒排，还提供了 document values field cache，解决：排序、聚合等问题。 列式存储：将索引中某一个字段值全部读取到内存中进行操作，用空间换时间。 es 的搜索机制搜索过程 搜索过程由两个阶段组成：查询阶段，获取阶段 查询阶段(Query Phase)：在此阶段，协调节点将搜索请求路由到索引(index)中的所有分片(shards)（包括：主要或副本）。分片独立执行搜索，并根据相关性分数创建一个优先级排序结果。所有分片将匹配的文档和相关分数的文档 ID 返回协调节点。协调节点创建一个新的优先级队列，并对全局结果进行排序。可以有很多文档匹配结果，但默认情况下，每个分片将前10个结果发送到协调节点，协调创建优先级队列。 获取阶段(Fetch Phase)：在协调节点对所有结果进行排序，并通过文档 id，从分片中得到原始文档，再返回到协调节点。 搜索相关性： 文档指定字段，与 query 相关性越强，文档的得分越高 评分默认算法：tf / idf（术语频率/逆文档频率）]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP和RPC]]></title>
    <url>%2F2019%2F09%2F19%2FHTTP%E5%92%8CRPC%2F</url>
    <content type="text"><![CDATA[Why RPC？在 HTTP 和 RPC 的选择上，可能有些人是迷惑的，主要是因为有些 RPC 框架配置复杂，如果走 HTTP 也能完成同样的功能，那么为什么要选择 RPC？ RPC，即 Remote Procedure Call，远程过程调用，主要是基于 TCP/IP 协议。而 HTTP 服务主要是基于 HTTP 协议的。我们都知道 HTTP 协议是在传输层协议 TCP 之上的，所以就效率来看的话，RPC 当然是要更胜一筹。另外， HTTP 的最大优势在于双端异构下的无障碍传输，但由于公司内部服务基本不存在异构的情况，所以这个优点微乎其微。所以，RPC 主要用于公司内部的服务调用，性能消耗低，传输效率高，服务治理方便。HTTP 主要用于对外的异构环境，浏览器接口调用，APP 接口调用，第三方接口调用等。 与 HTTP 的异同传输协议RPC：可基于 TCP 也可基于 HTTP HTTP：HTTP协议 传输效率RPC：使用自定义的 TCP 协议，可以让请求报文体积更小，或者使用 HTTP2 协议，也可以很好的减少报文的体积，提高传输效率 HTTP：如果是基于 HTTP1.1 的协议，请求中会包含很多无用的内容，如果是基于 HTTP2.0，那么简单的封装以下是可以作为一个 RPC 来使用的 性能消耗（主要在于序列化和反序列化的耗时）RPC：可以基于 thrift 实现高效的二进制传输 HTTP：超文本传输协议，大部分是通过 json 来实现的，字节大小和序列化耗时都比 thrift 要更消耗性能 负载均衡RPC：基本都自带了负载均衡策略 HTTP：需要配置 Nginx，HAProxy 来实现 其他rpc 框架一般还包含以下 HTTP 没有的高级功能： 服务治理（下游服务新增，重启，下线时如何不影响上游调用者） 服务熔断/降级 流量监控等等 RPC 框架 ——基于 Thrift 服务的 Kite一个 rpc 调用，一般分为以下几步 发送方将请求序列化 发送方通过网络发送 接收方通过网络接受 接收方反序列化得到请求 当然在实际使用中还有很多额外的工作要做，服务端要监听端口，客户端要进行链接，服务端还要选择如何处理请求，多线程，线程通信等等一系列工作，需要处理有很多。 ThriftThrift 通过分层的方法，把整个过程分为四层，每一层解决一个问题，上下层之间提供服务。 server：完成端口的监听，当有链接到来时为其创建 transport，protocol，并调用相应的 processor 处理 processor：对外提供一个统一的 process(in, out protocol) 接口 protocol：完成序列化 transport: 完成具体的传输功能（通过网络发送，写入磁盘等） 一个大概的处理过程例子如下：（我觉得图里的序列化和反序列化写反了） 因为 thrift 采用了分层，使得各层之间可以互相独立，所以如图中标注，虚线以下的代码是静态生成的，这部分代码，主要是通过传入的 TProtocol 完成反序列化，然后将得到的请求传递给用户注册进的 Handler 去处理，处理完以后，再通过 TProtocol 序列化应答发送回去。下面这部分，就是工具通过 idl 生成的，一般称为 stub。stub 通过传入的 TProtocol 完成读取数据，反序列化，调用 handler 处理，序列化应答，发送功能。 虚线以上，完成了链接的建立，在用链接创建 TTrannport，用 TTransport 创建 TProtocol。这部分决定了链接如何建立，序列化格式是什么。最后将封装了序列化/反序列化操作的 TProtocol 传递给 stub。 Thrift 这种分层的设计很好的将具体的序列化/反序列化操作与普通的服务端链接建立，数据读取，协议格式等进行了解藕，服务端可以专心在虚线以上部分的建设，其余的交给 stub。 Kitekite 框架，其实完成的就是上一节说的那幅图中的虚线以上部分。对照图，可以分为三部分： 为新链接建立 TProtocol 对象 把用户的 Handler 注册到 TProcessor 中 把 TProcessor 注册到 kite 框架中 构造 TProtocol 对象 kite 直接使用了 golang 标准库 net。net.Listen 监听，然后直接开启一个 for loop 开始 Accept。 Accept 完成链接建立以后得到 net.Conn 对象。 开启协程处理接下来的步骤。 用 net.Conn 构造 TTransport 对象，再构造 TProtocol 对象。 这几步和上图中描述的是差不多的。接下来是把 Handler 注册到 stub 中，并且把 stub 注册到 kite 框架中。stub 代码是工具已经生成的，对外提供了注册接口。Handler 代码是 kitool 工具生成的，注册操作也是 kitool 生成的代码中完成的。 kitool 工具的角色其实有两个功能： 为 thrift 生成的 stub 代码，生成 Handler 将 stub 注册到 kite 框架 Handler 注册kitool 默认生成了一个 Handler，然后调用了 stub 的注册完成注册，这一部分比较简单 1Hello.NewHelloServiceProcessor(&amp;HelloServiceHandler&#123;&#125;) stub 注册stub 是通过工具从 idl 中生成的，独立于 kite 框架。框架中提供了接口将外部的 stub 注册进来。在 kite 框架中有一个 export 的全局变量 Processor thrift.TProcessor，给这个变量赋值就完成了 stub 的注册。 实际上这个操作是由 kitool 生成的代码完成的。通过 kitool 生成的项目在项目根目录下有一个 kite.go 文件，这个文件里就完成了 stub 的注册： 123func init() &#123; kite.Processor = Hello.NewHelloServiceProcessor(&amp;HelloServiceHandler&#123;&#125;)&#125; 最后，把 TProtocol 交给 Processor 处理即可。 Middleware可以发现，到目前位置，kite 框架看起来是比较“简单”的，但是似乎有很多 routine work 多没有提到，比如限流，日志记录，监控上报等等。这就要提到 middleware（中间件）了。kite 通过使用中间件将框架主体与这些 routine work 进行了解藕。kite框架主体，只关注底层请求的接入，routine work 全都集中在了 middleware 当中。这一部分是在 kitool 生成的 Handler 中完成的。 这里先引入两个类型： EndPoint Middleware 1234// EndPoint represent one method for calling from remote.type EndPoint func(ctx context.Context, req interface&#123;&#125;) (resp interface&#123;&#125;, err error)// Middleware deal with input EndPoint and output EndPointtype Middleware func(EndPoint) EndPoint 可以发现，EndPoint 是一个函数类型，middleware 实际上是一个高级函数，入参是 EndPoint，返回值也是 EndPoint。有这两个类型就可以写出类似这样的代码： 12345678func MyMW(next EndPoint) EndPoint &#123; return func(ctx contex.Context, req interface&#123;&#125;) (resp interface&#123;&#125;, err error) &#123; //do something before next rsp, err := next(ctx, req) //do something after next return rsp, err &#125;&#125; 这里的 MyMW 返回了一个新的 EndPoint，实际上是一个闭包，wrap 了 next。当执行下面的代码时，实际上执行的是 MyMW 返回的新 EndPoint，这个 EndPoint 可以在执行 next 之前/之后，执行一些 pre/after 操作。 1newNext := MyMW(next) 并且，由于 middleware 的入参和返回值都是同一个类型，因此 middleware 还可以串联起来： 123n1 := MyMW1(next)n2 := MyMW2(n1)n3 := MyM3(n2) 可以发现，使用中间件我们可以在原始的函数之外包上很多层的逻辑。回到框架，kite 就是使用这样的方法在最原始的业务处理函数之外包裹整个过程不需要侵入框架或则侵入业务处理函数中做任何的修改，以一种方便，可扩展，可维护的方式拓展了框架的功能。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>rpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud]]></title>
    <url>%2F2019%2F06%2F27%2FSpring-Cloud%2F</url>
    <content type="text"><![CDATA[一、Spring Cloud 是什么 二、Eureka 服务注册与发现 三、Feign 服务消费者 四、Ribbon 负载均衡 五、Hystrix 断路器 六、Zuul 路由网关 七、组件功能总结 八、Spring Cloud Config 分布式配置中心 一、Spring Cloud 是什么Spring Cloud 是基于 Spring Boot 提供了一套微服务解决方案，包括服务注册与发现，配置中心，全链路监控，服务网关，负载均衡，熔断器等组件，除了基于 NetFlix 的开源组件做高度抽象封装之外，还有一些选型中立的开源组件。Spring Cloud = 分布式微服务架构下的一站式解决方案，是各个微服务架构落地技术的集合体，俗称微服务全家桶。 微服务化微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合,每一个微服务提供单个业务功能的服务，一个服务做一件事，从技术角度看就是一种小而独立的处理过程，类似进程概念，能够自行单独启动或销毁，拥有自己独立的数据库。 Spring Boot 和 Spring Cloud 的关系 Spring Boot 专注于快速方便的开发单个个体微服务。 Spring Cloud 是关注全局的微服务协调整理治理框架，它将 Spring Boot 开发的一个个单体微服务整合并管理起来，为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务。 Spring Boot 可以离开 Spring Cloud 独立使用开发项目，但是 Spring Cloud 离不开 Spring Boot，属于依赖的关系。 二、Eureka 服务注册与发现Eureka 是什么Eureka 是 Netflix 的一个子模块，也是核心模块之一。Eureka 是一个基于 REST 的服务，用于定位服务，以实现云端中间层服务发现和故障转移。 服务注册与发现对于微服务架构来说是非常重要的，有了服务发现与注册，只需要使用服务的标识符，就可以访问到服务，而不需要修改服务调用的配置文件了。功能类似于 dubbo 的注册中心，比如 Zookeeper。 抽象理解Eureka 相当于一个商业中心，所有的微服务相当于入驻的商户，需要在 Eureka 注册。此外，商户之间通信是基于 REST 的，这就需要服务发现暴露接口（即对于注册进 Eureka 里面的微服务，可以通过服务发现来获取该服务的信息）。 Eureka 实现原理基础架构Eureka 采用了 C-S 的设计架构。Eureka Server 作为服务注册功能的服务器，它是服务注册中心。而系统中的其他微服务使用 Eureka 的客户端连接到 Eureka Server 并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。Spring Cloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。 Eureka Server 提供服务注册服务，各个节点启动后，会在 Eureka Server 中进行注册，这样 Eureka Server 中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观的看到。 Eureka Client 是一个 Java 客户端，用于简化 Eureka Server 的交互，客户端同时也具备一个内置的、使用轮询 (round-robin) 负载算法的负载均衡器。在应用启动后，将会向 Eureka Server 发送心跳(默认周期为30秒)。如果 Eureka Server 在多个心跳周期内没有接收到某个节点的心跳，Eureka Server 将会从服务注册表中把这个服务节点移除（默认90秒）。 三大角色 Eureka Server 提供服务注册和发现。 Service Provider 服务提供方将自身服务注册到 Eureka，从而使服务消费方能够找到。 Service Consumer 服务消费方从 Eureka 获取注册服务列表，从而能够消费服务。 Eureka 自我保护某时刻某一个微服务不可用了，Eureka 不会立刻清理，依旧会对该微服务的信息进行保存。 默认情况下，如果 Eureka Server 在一定时间内没有接收到某个微服务实例的心跳，Eureka Server 将会注销该实例（默认90秒）。但是当网络分区故障发生时，微服务与 Eureka Server 之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka 通过“自我保护模式”来解决这个问题： 当 Eureka Server 节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。在自我保护模式中，Eureka Server 会保护服务注册表中的信息，不再注销任何服务实例。当它收到的心跳数重新恢复到阈值以上时，该 Eureka Server 节点就会自动退出自我保护模式。 综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模式，可以让 Eureka 集群更加的健壮、稳定。在 Spring Cloud 中，可以使用 eureka.server.enable-self-preservation = false 禁用自我保护模式。 三、Feign 服务消费者Feign 是什么Feign 是一个声明式 WebService 客户端。使用 Feign 能让编写 Web Service 客户端更加简单, 它的使用方法是定义一个接口，然后在上面添加注解，同时也支持 JAX-RS 标准的注解。Feign 也支持可拔插式的编码器和解码器。Spring Cloud 对 Feign 进行了封装，使其支持了 Spring MVC 标准注解和 HttpMessageConverters。Feign 可以与 Eureka 和 Ribbon 组合使用以支持负载均衡。 Feign 旨在使编写 Java Http 客户端变得更容易。如果只使用 Ribbon + RestTemplate，利用 RestTemplate 对 http 请求的封装处理，形成了一套模版化的调用方法。但是在实际开发中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会针对每个微服务自行封装一些客户端类来包装这些依赖服务的调用。所以，Feign 在此基础上做了进一步封装，由他来帮助我们定义和实现依赖服务接口的定义。在 Feign 的实现下，我们只需创建一个接口并使用注解的方式来配置它(以前是 Dao 接口上面标注 Mapper 注解,现在是一个微服务接口上面标注一个 Feign 注解即可)，即可完成对服务提供方的接口绑定，简化了使用 Spring Cloud Ribbon 时，自动封装服务调用客户端的开发量。 Feign 集成了 Ribbon。利用 Ribbon 维护了服务列表信息，并且通过轮询实现了客户端的负载均衡。而与 Ribbon 不同的是，通过 Feign 只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用。 Feign 架构说明Feign 是如何做到这么神奇的呢？很简单，Feign 的一个关键机制就是使用了动态代理。 首先，如果你对某个接口定义了 @FeignClient 注解，Feign 就会针对这个接口创建一个动态代理。 接着你要是调用那个接口，本质就是会调用 Feign 创建的动态代理，这是核心中的核心。 Feign 的动态代理会根据你在接口上的 @RequestMapping 等注解，来动态构造出你要请求的服务的地址。 最后针对这个地址，发起请求、解析响应。 四、Ribbon 负载均衡Ribbon 是什么Spring Cloud Ribbon 是基于 Netflix Ribbon 实现的一套客户端负载均衡的工具。 简单的说，Ribbon 是 Netflix 发布的开源项目，主要功能是提供客户端的软件负载均衡算法，将 Netflix 的中间层服务连接在一起。Ribbon 客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出 Load Balancer（简称LB）后面所有的机器，Ribbon 会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们也很容易使用 Ribbon 实现自定义的负载均衡算法。 LB，即负载均衡 (Load Balance)，在微服务或分布式集群中经常用的一种应用。负载均衡简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的 HA。常见的负载均衡有软件 Nginx，LVS，硬件 F5 等。相应的在中间件，例如：dubbo 和 Spring Cloud 中均给我们提供了负载均衡，Spring Cloud 的负载均衡算法可以自定义。 集中式 LB：即在服务的消费方和提供方之间使用独立的 LB 设施(可以是硬件，如 F5，也可以是软件，如 nginx)，由该设施负责把访问请求通过某种策略转发至服务的提供方。 进程内 LB：将 LB 逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。 Ribbon 就属于进程内 LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。 Ribbon 架构说明 Ribbon 在工作时分成两步：第一步先选择 Eureka Server ,它优先选择在同一个区域内负载较少的 server。第二步再根据用户指定的策略，在从 server 取到的服务注册列表中选择一个地址。其中 Ribbon 提供了多种策略：比如轮询、随机和根据响应时间加权。 五、Hystrix 断路器分布式面临问题：服务雪崩复杂分布式体系结构中的应用程序有数十个依赖关系，每个依赖关系在某些时候将不可避免地失败。 多个微服务之间调用的时候，假设微服务 A 调用微服务 B 和微服务 C，微服务 B 和微服务 C 又调用其它的微服务，这就是所谓的“扇出”。如果扇出的链路上某个微服务的调用响应时间过长或者不可用，对微服务 A 的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”。 对于高流量的应用来说，单一的后端依赖可能会导致所有服务器上的所有资源都在几秒钟内饱和。比失败更糟糕的是，这些应用程序还可能导致服务之间的延迟增加，备份队列，线程和其他系统资源紧张，导致整个系统发生更多的级联故障。这些都表示需要对故障和延迟进行隔离和管理，以便单个依赖关系的失败，不能取消整个应用程序或系统。 Hystrix 是什么Hystrix 是一个用于处理分布式系统的延迟和容错的开源库，在分布式系统里，许多依赖不可避免的会调用失败，比如超时、异常等，Hystrix 能够保证在一个依赖出问题的情况下，不会导致整体服务失败，避免级联故障，以提高分布式系统的弹性。 “断路器”本身是一种开关装置，当某个服务单元发生故障之后，通过断路器的故障监控（类似熔断保险丝），向调用方返回一个符合预期的、可处理的备选响应（FallBack），而不是长时间的等待或者抛出调用方无法处理的异常，这样就保证了服务调用方的线程不会被长时间、不必要地占用，从而避免了故障在分布式系统中的蔓延，乃至雪崩。 Hystrix 功能服务熔断熔断机制是应对雪崩效应的一种微服务链路保护机制。当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回”错误”的响应信息。当检测到该节点微服务调用响应正常后恢复调用链路。在 Spring Cloud 框架里熔断机制通过 Hystrix 实现。Hystrix 会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。熔断机制的注解是 @HystrixCommand。一旦调用服务方法失败并抛出了错误信息后，会自动调用 @HystrixCommand 标注好的 fallbackMethod 调用类中的指定方法。 服务降级整体资源快不够了，忍痛将某些服务先关掉，待渡过难关，再开启回来。服务降级处理是在客户端实现完成的，与服务端没有关系。 服务实时监控除了隔离依赖服务的调用以外，Hystrix 还提供了准实时的调用监控（Hystrix Dashboard），Hystrix 会持续地记录所有通过 Hystrix 发起的请求的执行信息，并以统计报表和图形的形式展示给用户，包括每秒执行多少请求多少成功，多少失败等。Netflix 通过 hystrix-metrics-event-stream 项目实现了对以上指标的监控。Spring Cloud 也提供了 Hystrix Dashboard 的整合，对监控内容转化成可视化界面。 Hystrix 实现原理如下图，Hystrix 会搞很多个小小的线程池，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。 六、Zuul 路由网关Zuul 是什么Zuul 包含了对请求的路由和过滤两个最主要的功能：其中路由功能负责将外部请求转发到具体的微服务实例上，是实现外部访问统一入口的基础而过滤器功能则负责对请求的处理过程进行干预，是实现请求校验、服务聚合等功能的基础。 Zuul 和 Eureka 进行整合，将 Zuul 自身注册为 Eureka 服务治理下的应用，同时从 Eureka 中获得其他微服务的消息，也即以后的访问微服务都是通过 Zuul 跳转后获得。 注意：Zuul 服务最终还是会注册进 Eureka Zuul = 代理 + 路由 + 过滤 七、组件功能总结Eureka：各个服务启动时，Eureka Client 都会将服务注册到 Eureka Server，并且 Eureka Client 还可以反过来从 Eureka Server 拉取注册表，从而知道其他服务在哪里。 Ribbon：服务间发起请求的时候，基于 Ribbon 做负载均衡，从一个服务的多台机器中选择一台。 Feign：基于 Feign 的动态代理机制，根据注解和选择的机器，拼接请求 URL 地址，发起请求。 Hystrix：发起请求是通过 Hystrix 的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题。 Zuul：如果前端、移动端要调用后端系统，统一从 Zuul 网关进入，由 Zuul 网关转发请求给对应的服务。 八、Spring Cloud Config 分布式配置中心背景微服务意味着要将单体应用中的业务拆分成一个个子服务，每个服务的粒度相对较小，因此系统中会出现大量的服务。由于每个服务都需要必要的配置信息才能运行，所以一套集中式的、动态的配置管理设施是必不可少的。 Spring Cloud Config 是什么Spring Cloud Config 为微服务架构中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置。作用如下： 集中管理配置文件 不同环境不同配置，动态化的配置更新，分环境部署比如：dev/test/prod/beta/release 运行期间动态调整配置，不再需要每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息 当配置发生变动时，服务不需要重启即可感知到配置的变化并应用新的配置 将配置信息以 REST 接口的形式暴露 Spring Cloud Config 架构Spring Cloud Config 分为客户端和服务端两部分。 服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密/解密信息等访问接口。 客户端则是通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息。 配置服务器默认使用 git 来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过 git 客户端工具来方便地管理和访问配置内容。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池]]></title>
    <url>%2F2019%2F06%2F23%2FJava%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[Executor 类图 线程池原理线程池中的核心线程数，当提交一个任务时，线程池创建一个新线程执行任务，直到当前线程数等于corePoolSize；如果当前线程数为 corePoolSize，继续提交的任务被保存到阻塞队列中，等待被执行；如果阻塞队列满了，那就创建新的线程执行当前任务；直到线程池中的线程数达到 maxPoolSize，这时再有任务来，只能执行 reject() 处理该任务。 线程池种类 newFixedThreadPool() 初始化一个指定线程数的线程池，其中 corePoolSize == maxiPoolSize，使用 LinkedBlockingQuene 作为阻塞队列 特点：即使当线程池没有可执行任务时，也不会释放线程。 newCachedThreadPool() 初始化一个可以缓存线程的线程池，默认缓存 60s，线程池的线程数可达到 Integer.MAX_VALUE，即 2147483647，内部使用 SynchronousQueue 作为阻塞队列； 特点：在没有任务执行时，当线程的空闲时间超过 keepAliveTime，会自动释放线程资源；当提交新任务时，如果没有空闲线程，则创建新线程执行任务，会导致一定的系统开销； 因此，使用时要注意控制并发的任务数，防止因创建大量的线程导致而降低性能。 newSingleThreadExecutor() 初始化只有一个线程的线程池，内部使用 LinkedBlockingQueue 作为阻塞队列。 特点：如果该线程异常结束，会重新创建一个新的线程继续执行任务，唯一的线程可以保证所提交任务的顺序执行。 newScheduledThreadPool() 特点：初始化的线程池可以在指定的时间内周期性的执行所提交的任务，在实际的业务场景中可以使用该线程池定期的同步数据。 ThreadPoolExecutor() 默认线程池，可控制参数比较多，实际上是前面四种的模板，自定义时只需传入自己想要的参数即可。 初始化示例 1234567// 使用Executors静态方法进行初始化ExecutorService service = Executors.newSingleThreadExecutor();// 常用方法service.execute(new Thread());service.submit(new Thread());service.shutDown();service.shutDownNow(); 常用方法 execute 与 submit 的区别 execute 只能提交 Runnable 类型的任务，而 submit 既能提交 Runnable 类型任务也能提交 Callable 类型任务。 execute 会直接抛出任务执行时的异常，submit 会吃掉异常，可通过 Future 的 get 方法将任务执行时的异常重新抛出。 submit 有返回值，而 execute 没有。 shutDown 与 shutDownNow 的区别 shutDown：当线程池调用该方法时,线程池的状态则立刻变成 SHUTDOWN 状态。此时，则不能再往线程池中添加任何任务，否则将会抛出 RejectedExecutionException异常。但是，此时线程池不会立刻退出，直到添加到线程池中的任务都已经处理完成，才会退出。 shutDownNow：相当于调用每个线程的 interrupt() 方法。 其他方法 prestartAllCoreThreads()：提前创建并启动所有核心线程。 setCorePoolSize() 和 setMaximumPoolSize()：动态调整线程池容量大小。 关键参数 corePoolSize：核心线程数 maximumPoolSize：最大线程数 keepAliveTime：线程存活时间（在 corePore &lt; * &lt; maxPoolSize 情况下有用） workQueue：阻塞队列（用来保存等待被执行的任务） handler：当拒绝处理任务时的策略 四大阻塞队列（workQueue） ArrayBlockingQueue：基于数组结构的有界阻塞队列，按 FIFO 排序任务 LinkedBlockingQuene：基于链表结构的阻塞队列，按 FIFO 排序任务 SynchronousQuene：一个不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于 ArrayBlockingQuene PriorityBlockingQuene：具有优先级的无界阻塞队列 四大拒绝策略（handler） ThreadPoolExecutor.AbortPolicy：默认，队列满了丢弃任务并抛出 RejectedExecutionException 异常 ThreadPoolExecutor.DiscardPolicy：队列满了丢弃任务，但是不抛出异常 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试加入队列（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：如果添加到线程池失败，那么主线程会自己去执行该任务 合理设置线程池大小线程等待时间所占比例越高，需要越多线程。线程 CPU 时间所占比例越高，需要越少线程。 一般需要根据任务的类型来配置线程池大小： 如果是 CPU 密集型任务，就需要尽量压榨 CPU，参考值可以设为 NCPU+1；如果是 IO 密集型任务，参考值可以设置为 2*NCPU。 补充：CPU 密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠 CPU 的运算能力；涉及到网络、磁盘 IO 的任务都是 IO 密集型任务，这类任务的特点是 CPU 消耗很少，任务的大部分时间都在等待 IO 操作完成（因为 IO 的速度远远低于 CPU 和内存的速度）。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql]]></title>
    <url>%2F2019%2F06%2F11%2Fmysql%2F</url>
    <content type="text"><![CDATA[一、事务 二、范式 三、存储引擎 四、数据类型 五、索引 六、查询性能优化 七、分库分表 八、主从复制与读写分离 九、锁 一、事务1.什么是事务？事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。MySQL 默认采用自动提交模式。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询都会被当做一个事务自动提交。事务简单来说：一个Session中所进行所有的操作，要么同时成功，要么同时失败。 2.ACID 原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）事务前后数据的完整性必须保持一致。 隔离性（Isolation）事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。 3.事务隔离级别数据库定义了4个隔离级别： 序列化：Serializable【可避免脏读，不可重复读，虚读】 可重复读：Repeatable read【可避免脏读，不可重复读】 已提交读：Read committed【可避免脏读】 未提交读：Read uncommitted【级别最低，什么都避免不了】 分别对应Connection类中的4个常量 TRANSACTION_READ_UNCOMMITTED TRANSACTION_READ_COMMITTED TRANSACTION_REPEATABLE_READ TRANSACTION_SERIALIZABLE 脏读：一个事务读取到另外一个事务未提交的数据。 不可重复读：一个事务读取到另外一个事务已经提交的数据，也就是说一个事务可以看到其他事务所做的修改。 虚读(幻读)：是指在一个事务内读取到了别的事务插入的数据，导致前后读取不一致。 简单总结：脏读是不可容忍的，不可重复读和虚读在一定的情况下是可以的【做统计的肯定就不行】 二、范式数据库中的范式指的是数据库的设计规范，范式具有包含关系。一个数据库设计如果符合第二范式，一定也符合第一范式。如果符合第三范式，一定也符合第二范式…范式逐级加严，一般数据库只需满足 3NF 即可。 1.第一范式：属性不可分当关系模式 R 的所有属性都不能在分解为更基本的数据单位时，称 R 是满足第一范式的，简记为 1NF。 1NF 是针对于数据表的列的规范，即数据表的每一列都是不可分割的原子数据项，而不能是数组，集合，记录等非原子数据项，说白了就是，不能把好几列的数据合在一起，且每一列的数据都是不可分割的。 2.第二范式：每个非主属性完全函数依赖于键码如果关系模式 R 满足第一范式，并且 R 的所有非主属性都完全依赖于 R 的每一个候选关键属性，称 R 满足第二范式，简记为 2NF。 2NF 基于第一范式，非码属性必须完全依赖码，即非主键数据必须依赖主键数据。“码”(主键)是数据表用来唯一区分实例或记录的数据项，若没有，可人为添加。对于第一范式针对列来说，第二范式则是针对于行的规范。第二范式需要确保数据库表中每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 3.第三范式：非主属性不传递函数依赖于键码设 R 是一个满足第一范式条件的关系模式，X 是 R 的任意属性集，如果 X 非传递依赖于 R 的任意一个候选关键字，称 R 满足第三范式，简记为 3NF。 3NF 需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。 三、存储引擎这里主要介绍两个搜索引擎：InnoDB 和 MyISAM 1.InnoDBMySQL 5.5 及之后版本的默认存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。 特性 InnoDB为事务性存储引擎 完全支持事物的 ACID 特性 Redo log （实现事务的持久性） 和 Undo log（为了实现事务的原子性，存储未完成事务log，用于回滚） InnoDB支持行级锁 行级锁可以最大程度的支持并发 行级锁是由存储引擎层实现的 实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（Next-Key Locking）防止幻影读 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升 应用场景 可靠性要求比较高，或者要求事务 表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。 2.MyISAMMySQL 5.5 版本之前的默认存储引擎，在 5.0 以前最大表存储空间最大 4G，5.0 以后最大 256TB。 MyISAM 存储引擎由 .myd（数据）和 .myi（索引文件）组成，.frm文件存储表结构（所有存储引擎都有） 特性 并发性和锁级别 （对于读写混合的操作不好，为表级锁，写入和读互斥） 表损坏修复 MyISAM 表支持的索引类型（全文索引） MyISAM 支持表压缩（压缩后，此表为只读，不可以写入。使用 myisampack 压缩） 应用场景 没有事务 只读类应用（插入不频繁，查询非常频繁） 空间类应用（唯一支持空间函数的引擎） 做很多 count 的计算 3.InnoDB 和 MyISAM 区别 对比项 MyISAM InnoDB 主外键 不支持 支持 事务 不支持 支持 行表锁 表锁，即使操作一条记录也会锁住整个表，不适合高并发的操作 行锁，操作时只锁某一行，不对其他行有影响，适合高并发的操作 缓存 只缓存索引，不缓存真实数据 不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响 表空间 小 大 关注点 性能 事务 其他 MyISAM 表是保存成文件的形式，在跨平台的数据转移中使用 MyISAM 存储会省去不少的麻烦。MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 InnoDB 表比 MyISAM 表更安全，可以在保证数据不会丢失的情况下，切换非事务表到事务表（alter table tablename type=innodb）。InnoDB 支持在线热备份。 应用场景 MyISAM 管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的 SELECT 查询，那么 MyISAM 是更好的选择。 InnoDB 用于事务处理应用程序，具有众多特性，包括 ACID 事务支持。如果应用中需要执行大量的 INSERT 或 UPDATE 操作，则应该使用 InnoDB，这样可以提高多用户并发操作的性能。 四、数据类型1.整型TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。 2.浮点型FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。 FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。 3.字符串主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。 VARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。 VARCHAR 会保留字符串末尾的空格，而 CHAR 会删除。 4.时间和日期MySQL 提供了两种相似的日期时间类型：DATATIME 和 TIMESTAMP。 DATATIME能够保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。 它与时区无关。 默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATATIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。 TIMESTAMP和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年 到 2038 年。 它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。 MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。 默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。 应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。 五、索引1.索引使用场景索引能够轻易将查询性能提升几个数量级。 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效。 对于中到大型的表，索引就非常有效。 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。 2.B+ 树B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 主要特点：内节点只存指针，叶子节点只存数据，有序 3.为什么选 B+ 树？红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B Tree 作为索引结构，主要有以下两个原因： 更少的检索次数 平衡树检索数据的时间复杂度等于树高 h，而树高大致为 O(h)=O(logdN)，其中 d 为每个节点的出度。 红黑树的出度为 2，而 B Tree 的出度一般都非常大。红黑树的树高 h 很明显比 B Tree 大非常多，因此检索的次数也就更多。 B+Tree 相比于 B-Tree 更适合外存索引，因为 B+Tree 内节点去掉了 data 域，因此可以拥有更大的出度，检索效率会更高。 利用磁盘预读特性 为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。 操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。并且可以利用预读特性，相邻的节点也能够被预先载入。 4.索引分类B+ 树索引 B+Tree 索引是大多数 MySQL 存储引擎的默认索引类型。 因为不再需要进行全表扫描，只需要对树进行搜索即可，因此查找速度快很多。除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。 InnoDB 的 B+Tree 索引分为主索引和辅助索引。 主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。 哈希索引InnoDB 引擎有一个特殊的功能叫 “自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 哈希索引能以 O(1) 时间进行查找，但是失去了有序性，它具有以下限制： 无法用于排序与分组。 只支持精确查找，无法用于部分查找和范围查找。 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 空间数据索引（R-Tree）MyISAM 存储引擎支持空间数据索引，可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 5.索引种类 聚簇索引：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。 主键索引：加速查询 + 列值唯一（不可以有null）+ 表中只有一个 一种唯一性索引，必须指定为 primary key。一个表可以有多个唯一索引，但只能有一个主键。主键一定是唯一，唯一不一定是主键。且主键索引可以被其他表引用当外键，唯一索引不可以。 非聚簇索引：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。 普通索引：仅加速查询 唯一索引：加速查询 + 列值唯一（可以有null） 组合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 全文索引：对文本的内容进行分词，进行搜索，可以在char、varchar或text类型的列上创建。 6.联合索引什么是联合索引？两个或更多个列上的索引被称作联合索引，联合索引又叫复合索引。对于复合索引：Mysql 从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。 例如索引是 key index (a,b,c)，可以支持[a]、[a,b]、[a,b,c] 3种组合进行查找，但不支 [b,c] 进行查找。当最左侧字段是常量引用时，索引就十分有效。 命名规则 需要加索引的字段，要在 where 条件中 数据量少的字段不需要加索引 如果 where 条件中是OR关系，加索引不起作用 符合最左原则 7.索引的特点 可以加快数据库的检索速度 降低数据库插入、修改、删除等维护的速度 只能创建在表上，不能创建到视图上 既可以直接创建又可以间接创建 可以在优化隐藏中使用索引 使用查询处理器执行SQL语句，在一个表上，一次只能使用一个索引 优点 创建唯一性索引，保证数据库表中每一行数据的唯一性 大大加快数据的检索速度，这是创建索引的最主要的原因 加速数据库表之间的连接，特别是在实现数据的参考完整性方面特别有意义 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间 通过使用索引，可以在查询中使用优化隐藏器，提高系统的性能 缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加 索引需要占用物理空间，除了数据表占用数据空间之外，每一个索引还要占一定的物理空间，如果建立聚簇索引，那么需要的空间就会更大 当对表中的数据进行增加、删除和修改的时候，索引也需要维护，降低数据维护的速度 8.索引失效的情况 如果MySQL估计使用全表扫秒比使用索引快，则不使用索引。 例：如果列 key 均匀分布在 1 和 100 之间，下面的查询使用索引就不是很好： 1select * from table_name where key&gt;1 and key&lt;90; 如果条件中有 or，即使其中有条件带索引也不会使用。 例：如果在 key1 上有索引而在 key2 上没有索引，则该查询也不会走索引： 1select * from table_name where key1=&apos;a&apos; or key2=&apos;b&apos;; 复合索引，如果索引列不是复合索引的第一部分，则不使用索引。（即不符合最左前缀） 例：复合索引为(key1,key2)，则下列查询将不会使用索引： 1select * from table_name where key2=&apos;b&apos;; 如果 like 是以 % 开始的，则该列上的索引不会被使用。 例：下列查询即使 key1 上存在索引，也不会被使用如果列类型是字符串，那一定要在条件中使用引号引起来，否则不会使用索引。 1select * from table_name where key1 like &apos;%a&apos;; 如果列为字符串，则 where 条件中必须将字符常量值加引号，否则即使该列上存在索引，也不会被使用。 例：如果key1列保存的是字符串，即使key1上有索引，也不会被使用。 1select * from table_name where key1=1; 如果使用 MEMORY/HEAP 表，并且 where 条件中不使用 “=” 进行索引列，那么不会用到索引，head 表只有在 “=” 的条件下才会使用索引。 9.在什么情况下适合建立索引？ 为经常出现在关键字 order by、group by、distinct 后面的字段，建立索引。 在 union 等集合操作的结果集字段上，建立索引。其建立索引的目的同上。 为经常用作查询选择 where 后的字段，建立索引。 在经常用作表连接 join 的属性上，建立索引。 考虑使用索引覆盖。对数据很少被更新的表，如果用户经常只查询其中的几个字段，可以考虑在这几个字段上建立索引，从而将表的扫描改变为索引的扫描。 10.主键、外键和索引的区别 定义 作用 个数 主键 唯一标识一条记录，不能有重复的，不允许为空 用来保证数据完整性 主键只能有一个 外键 表的外键是另一表的主键，外键可以有重复的，可以是空值 用来和其他表建立联系用的 一个表可以有多个外键 索引 该字段没有重复值，但可以有一个空值 是提高查询排序的速度 一个表可以有多个惟一索引 11. SQL 约束有哪几种？ NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 六、查询性能优化1.用 Explain 进行分析Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。 比较重要的字段有： select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 2.优化数据访问 减少请求的数据量 只返回必要的列：最好不要使用 SELECT * 语句。 只返回必要的行：使用 LIMIT 语句来限制返回的数据。 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。 减少服务器端扫描的行数 最有效的方式是使用索引来覆盖查询。 3.重构查询方式 切分大查询 一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 1DELEFT FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); 12345rows_affected = 0do &#123; rows_affected = do_query( &quot;DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000&quot;)&#125; while rows_affected &gt; 0 分解大连接查询 将一个大连接查询（JOIN）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 1234SELECT * FROM tabJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag=&apos;mysql&apos;; 123SELECT * FROM tag WHERE tag=&apos;mysql&apos;;SELECT * FROM tag_post WHERE tag_id=1234;SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 七、分库分表简单来说，数据的切分就是通过某种特定的条件，将我们存放在同一个数据库中的数据分散存放到多个数据库（主机）中，以达到分散单台设备负载的效果，即分库分表。 数据的切分根据其切分规则的类型，可以分为如下两种切分模式。 垂直（纵向）切分：把单一的表拆分成多个表，并分散到不同的数据库（主机）上。 水平（横向）切分：根据表中数据的逻辑关系，将同一个表中的数据按照某种条件拆分到多台数据库（主机）上。 1.水平切分水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。 当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。 Sharding 优点 单库单表的数据保持在一定的量级，有助于性能的提高 切分的表的结构相同，应用层改造较少，只需要增加路由规则即可 提高了系统的稳定性和负载能力 Sharding 缺点 切分后，数据是分散的，很难利用数据库的 join 操作，跨库 join 性能较差 拆分规则难以抽象 分片事务的一致性难以解决 数据扩容的难度和维护量极大 Sharding 策略 哈希取模：hash(key) % NUM_DB 比如按照 userId mod 64，将数据分布在64个服务器上 范围：可以是 ID 范围也可以是时间范围 比如每台服务器计划存放一个亿的数据,先将数据写入服务器 A.一旦服务器 A 写满,则将数据写入服务器 B,以此类推. 这种方式的好处是扩展方便.数据在各个服务器上分布均匀 映射表：使用单独的一个数据库来存储映射关系 Sharding 存在的问题及解决方案 事务问题：使用分布式事务来解决，比如 XA 接口。 JOIN：可以将原来的 JOIN 查询分解成多个单表查询，然后在用户程序中进行 JOIN。 ID 唯一性 使用全局唯一 ID：GUID。 为每个分片指定一个 ID 范围。 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)。 2.垂直切分垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。 在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。 垂直切分的优点 拆分后业务清晰，拆分规则明确 系统之间进行整合或扩展很容易 按照成本、应用的等级、应用的类型等将表放到不同的机器上，便于管理 便于实现动静分离、冷热分离的数据库表的设计模式 数据维护简单 垂直切分的缺点 部分业务表无法关联（Join），只能通过接口方式解决，提高了系统的复杂度 受每种业务的不同限制，存在单库性能瓶颈，不易进行数据扩展和提升性能 事务处理复杂 3.垂直切分和水平切分的共同点 存在分布式事务的问题 存在跨节点 Join 的问题 存在跨节点合并排序、分页的问题 存在多数据源管理的问题 八、主从复制与读写分离主从复制主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。 I/O 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的中继日志中。 SQL 线程 ：负责读取中继日志并重放其中的 SQL 语句。 读写分离主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 MySQL 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以配置 MyISAM 引擎，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 九、锁MySQL/InnoDB 的加锁，一直是一个面试中常问的话题。例如，数据库如果有高并发请求，如何保证数据完整性？产生死锁问题如何排查并解决？在工作过程中，也会经常用到，乐观锁，排它锁等。 注：MySQL 是一个支持插件式存储引擎的数据库系统。下面的所有介绍，都是基于 InnoDB 存储引擎，其他引擎的表现，会有较大的区别。 版本查看 1select version(); 存储引擎查看 MySQL 给开发者提供了查询存储引擎的功能，我这里使用的是 MySQL5.6.4，可以使用： 1SHOW ENGINES 1.乐观锁假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。当读取数据时，将 version 字段的值一同读出，数据每更新一次，对此 version 值加1。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的 version 值进行比对，如果数据库表当前版本号与第一次取出来的 version 值相等，则予以更新，否则认为是过期数据。例： a. 数据库表设计三个字段，分别是 id,value,version 1select id,value,version from TABLE where id=#&#123;id&#125; b. 每次更新表中的value字段时，为了防止发生冲突，需要这样操作 123update TABLEset value=2,version=version+1where id=#&#123;id&#125; and version=#&#123;version&#125;; 2.悲观锁假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 悲观锁就是在操作数据时，认为此操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作，这点跟 Java 中的 synchronized 很相似，所以悲观锁需要耗费较多的时间。另外与乐观锁相对应的，悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。 说到这里，由悲观锁涉及到的另外两个锁概念就出来了，它们就是共享锁与排它锁。共享锁和排它锁是悲观锁的不同的实现，它俩都属于悲观锁的范畴。以排它锁为例： 要使用悲观锁，我们必须关闭 mysql 数据库的自动提交属性，因为 MySQL 默认使用 autocommit 模式，也就是说，当你执行一个更新操作后，MySQL 会立刻将结果进行提交。我们可以使用命令设置 MySQL 为非 autocommit 模式： 1234567891011121314151617set autocommit=0;# 设置完autocommit后，我们就可以执行我们的正常业务了。具体如下：# 1. 开始事务 (三者选一就可以)begin; / begin work; / start transaction;# 2. 查询表信息select status from TABLE where id=1 for update;# 3. 插入一条数据insert into TABLE (id,value) values (2,2);# 4. 修改数据为update TABLE set value=2 where id=1;# 5. 提交事务commit;/commit work; 3.共享锁共享锁又称读锁（read lock），是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 如果事务 T 对数据 A 加上共享锁后，则其他事务只能对 A 再加共享锁，不能加排他锁。获得共享锁的事务只能读数据，不能修改数据。 打开第一个查询窗口 1234#三者选一就可以begin; / begin work; / start transaction;SELECT * from TABLE where id = 1 lock in share mode; 然后在另一个查询窗口中，对 id 为 1 的数据进行更新 1update TABLE set name=&quot;www.souyunku.com&quot; where id =1; 此时，操作界面进入了卡顿状态，过了超时间，提示错误信息 如果在超时前，执行 commit，此更新语句就会成功。 12[SQL]update test_one set name=&quot;www.souyunku.com&quot; where id =1;[Err] 1205 - Lock wait timeout exceeded; try restarting transaction 加上共享锁后，也提示错误信息 123update test_one set name=&quot;www.souyunku.com&quot; where id =1 lock in share mode;[SQL]update test_one set name=&quot;www.souyunku.com&quot; where id =1 lock in share mode;[Err] 1064 - You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;lock in share mode&apos; at line 1 在查询语句后面增加 lock in share mode，MySQL 会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 加上共享锁后，对于 update,insert,delete 语句会自动加排它锁。 4.排它锁排他锁 exclusive lock（也叫 writer lock）又称写锁。 排它锁是悲观锁的一种实现，在上面悲观锁也介绍过。 若事务 1 对数据对象 A 加上 X 锁，事务 1 可以读 A 也可以修改 A，其他事务不能再对 A 加任何锁，直到事物 1 释放 A 上的锁。这保证了其他事务在事物 1 释放 A 上的锁之前不能再读取和修改 A。排它锁会阻塞所有的排它锁和共享锁。 读取为什么要加读锁呢：防止数据在被读取的时候被别的线程加上写锁。 使用方式：在需要执行的语句后面加上 for update 就可以了。 5.行锁行锁又分共享锁和排他锁，由字面意思理解，就是给某一行加上锁，也就是一条记录加上锁。 注意：行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。 共享锁： 名词解释：共享锁又叫做读锁，所有的事务只能对其进行读操作不能写操作，加上共享锁后在事务结束之前其他事务只能再加共享锁，除此之外其他任何类型的锁都不能再加了。 12#结果集的数据都会加共享锁SELECT * from TABLE where id = &quot;1&quot; lock in share mode; 排他锁： 名词解释：若某个事物对某一行加上了排他锁，只能这个事务对其进行读写，在此事务结束之前，其他事务不能对其进行加任何锁，其他进程可以读取，不能进行写操作，需等待其释放。 1select status from TABLE where id=1 for update; 可以参考之前演示的共享锁，排它锁语句 由于对于表中 id 字段为主键，就也相当于索引。执行加锁时，会将 id 这个索引为 1 的记录加上锁，那么这个锁就是行锁。 6.表锁如何加表锁 innodb 的行锁是在有索引的情况下,没有索引的表是锁定全表的。 Innodb中的行锁与表锁 前面提到过，在 Innodb 引擎中既支持行锁也支持表锁，那么什么时候会锁住整张表，什么时候或只锁住一行呢？ 只有通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！ 在实际应用中，要特别注意 InnoDB 行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。 行级锁都是基于索引的，如果一条 SQL 语句用不到索引是不会使用行级锁的，会使用表级锁。行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。 7. 死锁死锁（Deadlock） 所谓死锁：是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。由于资源占用是互斥的，当某个进程提出申请资源后，使得有关进程在无外力协助下，永远分配不到必需的资源而无法继续运行，这就产生了一种特殊现象死锁。 解除正在死锁的状态有两种方法： 第一种： 查询是否锁表 1show OPEN TABLES where In_use &gt; 0; 查询进程（如果您有SUPER权限，您可以看到所有线程。否则，您只能看到您自己的线程） 1show processlist 杀死进程id（就是上面命令的id列） 1kill id 第二种： 查看当前的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX; 查看当前锁定的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 查看当前等锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; 杀死进程 1kill 进程ID 如果系统资源充足，进程的资源请求都能够得到满足，死锁出现的可能性就很低，否则就会因争夺有限的资源而陷入死锁。其次，进程运行推进顺序与速度不同，也可能产生死锁。 产生死锁的四个必要条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 虽然不能完全避免死锁，但可以使死锁的数量减至最少。将死锁减至最少可以增加事务的吞吐量并减少系统开销，因为只有很少的事务回滚，而回滚会取消事务执行的所有工作。由于死锁时回滚而由应用程序重新提交。 下列方法有助于最大限度地降低死锁： 按同一顺序访问对象 避免事务中的用户交互 保持事务简短并在一个批处理中 使用低隔离级别 使用绑定连接]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络基础]]></title>
    <url>%2F2019%2F06%2F11%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[总结自 SnailClimb 和 frank-lam 的博文。 一、OSI七层模型、TCP/IP四层模型和五层协议 1. 五层协议(1) 应用层提供用户接口，特指能够发起网络流量的程序，比如客户端程序：QQ，MSN，浏览器等；服务器程序：web 服务器，邮件服务器，流媒体服务器等等。 在互联网中应用层协议很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等等。数据单位为报文。 域名系统 域名系统 (Domain Name System 缩写 DNS，Domain Name 被译为域名)是因特网的一项核心服务，它作为可以将域名和 IP 地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的 IP 数串。（百度百科）例如：一个公司的 Web 网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM 公司的域名是 www.ibm.com、Oracle 公司的域名是 www.oracle.com、Cisco公司的域名是 www.cisco.com 等。 HTTP 协议 超文本传输协议（HTTP，HyperText Transfer Protocol) 是互联网上应用最为广泛的一种网络协议。所有的 WWW（万维网） 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。 (2) 传输层负责向两台主机进程之间的通信提供通用的数据传输服务。由于应用层协议很多，定义通用的传输层协议就可以支持不断增多的应用层协议。传输层向用户提供可靠的端到端服务，透明地传送报文。 传输层主要两种协议 传输控制协议 TCP提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。 TCP 主要提供完整性服务，UDP 主要提供及时性服务。 UDP 的主要特点 UDP 是无连接的； UDP 使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）； UDP 是面向报文的； UDP 没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如直播，实时视频会议等）； UDP 支持一对一、一对多、多对一和多对多的交互通信； UDP 的首部开销小，只有8个字节，比 TCP 的20个字节的首部要短。 TCP 的主要特点 TCP 是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）； 每一条 TCP 连接只能有两个端点，每一条 TCP 连接只能是点对点的（一对一）； TCP 提供可靠交付的服务。通过 TCP 连接传送的数据，无差错、不丢失、不重复、并且按序到达； TCP 提供全双工通信。TCP 允许通信双方的应用进程在任何时候都能发送数据。TCP 连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据； 面向字节流。TCP 中的 “流”（Stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和 TCP 的交互是一次一个数据块（大小不等），但 TCP 把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。 (3) 网络层为主机间提供数据传输服务，而运输层协议是为主机中的进程提供服务。网络层把运输层传递下来的报文段或者用户数据报封装成分组和包进行传送。（负责选择最佳路径规划 IP 地址） 在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称数据报。 网络层的另一个任务就是选择合适的路由，使源主机运输层所传下来的分株，能通过网络层中的路由器找到目的主机。路由器查看数据包目标 IP 地址，根据路由表为数据包选择路径。路由表中的类目可以人工添加（静态路由）也可以动态生成（动态路由）。 (4) 数据链路层不同的网络类型，发送数据的机制不同，数据链路层就是将数据包封装成能够在不同的网络传输的帧。能够进行差错检验，但不纠错，监测出错误丢掉该帧。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。 (5) 物理层在物理层上所传送的数据单位是比特。物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 总结在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的 TCP/IP 并不一定单指 TCP 和 IP 这两个具体的协议，而往往表示互联网所使用的整个 TCP/IP 协议族。 2. ISO 七层模型中表示层和会话层功能是什么？ 表示层 ：数据压缩、加密以及数据描述。这使得应用程序不必担心在各台主机中表示/存储的内部格式（二进制、ASCII，比如乱码）不同的问题。 会话层 ：建立会话，如 session 认证、断点续传。通信的应用程序之间建立、维护和释放面向用户的连接。通信的应用程序之间建立会话，需要传输层建立1个或多个连接。 说明：五层协议没有表示层和会话层，而是将这些功能留给应用程序开发者处理。 3. 数据在各层之间的传递过程在向下的过程中，需要添加下层协议所需要的首部或者尾部，而在向上的过程中不断拆开首部和尾部。 路由器只有下面三层协议，因为路由器位于网络核心中，不需要为进程或者应用程序提供服务，因此也就不需要运输层和应用层。 交换机只有下面两层协议 二、TCP 三次握手和四次挥手三次握手 为了准确无误地把数据送达目标处，TCP协议采用了三次握手策略。 客户端–发送带有 SYN 标志的数据包 服务端–发送带有 SYN/ACK 标志的数据包 客户端–发送带有 ACK 标志的数据包 为什么要三次握手？三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己接收正常，对方发送正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送接收正常 所以三次握手就能确认双发收发功能都正常，缺一不可。 为什么要传回 SYN？接收端传回发送端所发送的 SYN 是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 传了 SYN,为啥还要传 ACK？双方通信无误必须是两者互相发送信息都无误。传了 SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要 ACK 信号来进行验证。 四次挥手 MSL 是 Maximum Segment Lifetime 英文的缩写，中文可以译为 “报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。2MSL = 2*2mins = 4mins 断开一个 TCP 连接则需要“四次挥手”。 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个 FIN 给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加1 为什么要四次挥手？客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 TIME_WAIT客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由： 确保最后一个确认报文段能够到达。如果 B 没收到 A 发送来的确认报文段，那么就会重新发送连接释放请求报文段，A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文段都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文段。 三、TCP 和 UDP 的区别 UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等。 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的运输服务（TCP 的可靠体现在 TCP 在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。 四、TCP 协议如何保证可靠传输 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 TCP的接收端会丢弃重复的数据。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 拥塞控制：当网络拥塞时，减少数据的发送。 停止等待协议 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。超时重传：当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 流量控制：TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 停止等待协议 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组； 在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认； 1. 无差错情况 2. 出现差错情况（超时重传） 停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 3. 确认丢失和确认迟到 确认丢失：确认消息在传输过程丢失 当 A 发送 M1 消息，B 收到后，B 向 A 发送了一个 M1 确认消息，但却在传输过程中丢失。而A 并不知道，在超时计时过后，A 重传 M1 消息，B 再次收到该消息后采取以下两点措施： 丢弃这个重复的M1消息，不向上层交付。 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。 确认迟到 ：确认消息在传输过程中迟到 A 发送 M1 消息，B 收到并发送确认。在超时时间内没有收到确认消息，A 重传 M1 消息，B 仍然收到并继续发送确认消息（B 收到了 2 份 M1）。此时 A 收到了 B 第二次发送的确认消息。接着发送其他数据。过了一会，A 收到了 B 第一次发送的对 M1 的确认消息（A 也收到了2份确认消息）。处理如下： A 收到重复的确认后，直接丢弃。 B 收到重复的 M1 后，也直接丢弃重复的 M1。 自动重传请求 ARQ 协议停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求 ARQ。 优点：简单。 缺点：信道利用率低。 连续 ARQ 协议连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。 优点： 信道利用率高，容易实现，即使确认丢失，也不必重传。 缺点： 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。 滑动窗口 TCP 利用滑动窗口实现流量控制的机制。 滑动窗口（Sliding window）是一种流量控制技术。早期的网络通信中，通信双方不会考虑网络的拥挤情况直接发送数据。由于大家不知道网络拥塞状况，同时发送数据，导致中间节点阻塞掉包，谁也发不了数据，所以就有了滑动窗口机制来解决此问题。 TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据，例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。 流量控制 TCP 利用滑动窗口实现流量控制。 流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 拥塞控制 在某段时间，若对网络中某资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏——产生拥塞(congestion)。 出现资源拥塞的条件：对资源需求的总和 &gt; 可用资源 若网络中有许多资源同时产生拥塞，网络的性能就要明显变坏，整个网络的吞吐量将随输入负荷的增大而下降。 拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。 拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP 的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 慢开始 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为1，每经过一个传播轮次，cwnd 加倍。 拥塞避免 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢增大，即每经过一个往返时间 RTT 就把发送放的 cwnd 加1。 快重传和快恢复 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。 五、在浏览器中输入url地址 -&gt; 显示主页的过程 六、状态码 类别 原因短语 1xx Informational (信息性状态码) 接收的请求正在处理 2xx Success (成功状态码) 请求正常处理完毕 3xx Redirection (重定向状态码) 需要进行附加操作以完成请求 4xx Client Error (客户端错误状态码) 服务器无法处理请求 5xx Server Error (服务器错误状态码) 服务器处理请求出错 详见博文 HTTP中常用的状态码) 七、各协议与 HTTP 协议之间的关系 八、HTTP 的长连接和短连接在 HTTP/1.0 中默认使用短连接。也就是说，客户端和服务器每进行一次 HTTP 操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源（如 JavaScript 文件、图像文件、CSS 文件等），每遇到这样一个 Web 资源，浏览器就会重新建立一个 HTTP 会话。 而从 HTTP/1.1 起，默认使用长连接，用以保持连接特性。使用长连接的 HTTP 协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。 Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如 Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接。 九、HTTP 和 HTTPS 的区别 https 协议需要到 ca 申请证书，一般免费证书较少，因而需要一定费用。 http 是超文本传输协议，信息是明文传输，https 则是具有安全性的 ssl 加密传输协议。 http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http 的连接很简单，是无状态的；https 协议是由 SSL + http 协议构建的可进行加密传输、身份认证的网络协议，比 http 协议安全。 十、SSL 的四次握手 客户端请求建立 SSL 链接，并向服务端发送一个随机数（client random）和客户端支持的加密方法（比如 RSA），此时是明文传输的。 服务端选择客户端支持的一种加密算法并生成另一个随机数（server random），并将授信的服务端证书和公钥下发给客户端。 客户端收到服务端的回复，会校验服务端证书的合法性，若合法，则生成一个新的随机数 premaster secret 并通过服务端下发的公钥及加密方法进行加密，然后发送给服务端。 服务端收到客户端的回复，利用已知的加解密方式进行解密，同时利用 client random、server random 和 premater secret 通过一定算法生成对称加密 key - session key。 此后，数据传输即通过对称加密方式进行加密传输。 从以上过程可以看到 https 实际上是用了对称加密技术和非对称加密技术，非对称加密解密速度慢，但安全性高，用来加密对称加密的密钥；而对称加密虽然安全性低，但解密速度快，可以用于传输数据的加密。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 基础知识]]></title>
    <url>%2F2019%2F06%2F09%2FSpring-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Spring IOC 和 AOPSpring 的 IOC 容器是 Spring 的核心，Spring AOP 是 Spring 框架的重要组成部分。 IOC1. IOC 是什么？ IOC (Inversion Of Control)控制反转，包含了两个方面：控制和反转 控制：当前对象对内部成员的控制权。 反转：这种控制权不由当前对象管理了，由其他(类,第三方容器)来管理。 IOC 的意思是控件反转也就是由容器控制程序之间的关系，这也是 Spring 的优点所在，把控件权交给了外部容器，之前的写法，由程序代码直接操控，而现在控制权由应用代码中转到了外部容器，控制权的转移是所谓反转。换句话说之前用 new 的方式获取对象，现在由 Spring 给你，至于怎么给你就是 DI 了。 IOC 容器是 Spring 用来实现 IOC 的载体， IOC 容器实际上就是个 Map（key，value），Map 中存放的是各种对象。 2. IOC 实现原理 创建 xml 配置文件，配置要创建的对象类。 通过反射创建实例。 获取需要注入的接口实现类并将其赋值给该接口。 3. 优点 解耦合，开发更方便组织分工 高层不依赖于底层（依赖倒置） 让应用更容易测试 因为把对象生成放在了 XML 里定义，所以当我们需要换一个实现子类将会变成很简单（一般这样的对象都是实现于某种接口的），只要修改 XML 就可以了，这样我们甚至可以实现对象的热插拨 AOP1. AOP是什么？ AOP（Aspect Oriented Programming ）称为面向切面编程，扩展功能不是修改源代码实现，在程序开发中主要用来解决一些系统层面上的问题，比如日志，事务，权限等待，Struts2 的拦截器设计就是基于 AOP 的思想，是个比较经典的例子。 面向切面编程（aop）是对面向对象编程（oop）的补充。 面向切面编程提供声明式事务管理。 AOP就是典型的代理模式的体现。 2. AOP 实现原理 动态代理（利用反射和动态编译将代理模式变成动态的） JDK 的动态代理 JDK 内置的 Proxy 动态代理可以在运行时动态生成字节码，而没必要针对每个类编写代理类 JDK Proxy 返回动态代理类，是目标类所实现接口的另一个实现版本，它实现了对目标类的代理（如同 UserDAOProxy 与 UserDAOImp 的关系） cglib动态代理 CGLibProxy 返回的动态代理类，则是目标代理类的一个子类（代理类扩展了 UserDaoImpl 类） cglib 继承被代理的类，重写方法，织入通知，动态生成字节码并运行 两种实现的区别 JDK 动态代理只能对实现了接口的类生成代理，而不能针对类 cglib 是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法因为是继承，所以该类或方法最好不要声明成 final JDK 代理是不需要以来第三方的库，只要 JDK 环境就可以进行代理 cglib 必须依赖于 cglib 的类库，但是它需要类来实现任何接口代理的是指定的类生成一个子类，覆盖其中的方法，是一种继承 3. 优点 各个步骤之间的良好隔离性 源代码无关性 松耦合 易扩展 代码复用 依赖注入DI 是什么？ DI(Dependency Injection) ，即依赖注入，是 Spring 中实现 IOC 的方式。所谓依赖注入，就是把底层类作为参数传入上层类，实现上层类对下层类的控制。DI 依赖注入，向类里面属性注入值 ，依赖注入不能单独存在，需要在 IOC 基础上完成操作。 3种注入方式（使用注解） field 注入，简单易用，但可能会出现依赖循环，且无法适用于 IOC 容器以外的环境。 1234567891011@Controllerpublic class FooController &#123; @Autowired //@Inject private FooService fooService; //简单的使用例子，下同 public List&lt;Foo&gt; listFoo() &#123; return fooService.list(); &#125;&#125; 使用 setter 方法注入，比构造器注入轻量，另外 setter 方式能让类在之后重新配置或重新注入。 1234567891011@Controllerpublic class FooController &#123; private FooService fooService; //使用方式上同，略 @Autowired public void setFooService(FooService fooService) &#123; this.fooService = fooService; &#125;&#125; 使用构造器注入，Spring 官方建议使用构造器注入，因为其能保证组件不可变，并且确保需要的依赖不为空。此外，构造器注入的依赖总是能够在返回客户端（组件）代码的时候保证完全初始化的状态。 123456789101112@Controllerpublic class FooController &#123; private final FooService fooService; @Autowired public FooController(FooService fooService) &#123; this.fooService = fooService; &#125; //使用方式上同，略&#125; Spring IOC 初始化过程 Resource 资源定位。这个 Resouce 指的是 BeanDefinition 的资源定位。这个过程就是容器找数据的过程，就像水桶装水需要先找到水一样。 BeanDefinition 的载入过程。这个载入过程是把用户定义好的 Bean 表示成 IOC 容器内部的数据结构，而这个容器内部的数据结构就是 BeanDefition。 向 IOC 容器注册这些 BeanDefinition 的过程，这个过程就是将前面的 BeanDefition 保存到 HashMap 中的过程。 Spring AOP 的使用AOP（Aspect Oriented Programming ）称为面向切面编程，扩展功能不是修改源代码实现，在程序开发中主要用来解决一些系统层面上的问题，比如日志，事务，权限等待， Struts2 的拦截器设计就是基于 AOP 的思想，是个比较经典的例子。 Joinpoint（连接点）：类里面可以被增强的方法，这些方法称为连接点 Pointcut（切入点）：所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义 Advice（通知/增强）：所谓通知是指拦截到 Joinpoint 之后所要做的事情就是通知。通知分为前置通知，后置通知，异常通知，最终通知，环绕通知（切面要完成的功能） Aspect（切面）：是切入点和通知（引介）的结合 Introduction（引介）：引介是一种特殊的通知在不修改类代码的前提下，Introduction可以在运行期为类动态地添加一些方法或 Field Target（目标对象）：代理的目标对象（要增强的类） Weaving（织入）：是把增强应用到目标的过程，把 advice 应用到 target 的过程 Proxy（代理）：一个类被 AOP 织入增强后，就产生一个结果代理类 Spring 的 AOP 常用的是拦截器 一般拦截器都是实现 HandlerInterceptor，其中有三个方法 preHandle、postHandle、afterCompletion。 preHandle：执行 controller 之前执行 postHandle：执行完 controller，return modelAndView 之前执行，主要操作 modelAndView 的值 afterCompletion：controller 返回后执行 Spring 的事务管理事务管理可以帮助我们保证数据的一致性，对应企业的实际应用很重要。 Spring 的事务机制包括声明式事务和编程式事务。 编程式事务管理：Spring 推荐使用 TransactionTemplate，实际开发中使用声明式事务较多。 声明式事务管理：将我们从复杂的事务处理中解脱出来，获取连接，关闭连接、事务提交、回滚、异常处理等这些操作都不用我们处理了，Spring 都会帮我们处理。 声明式事务管理使用了 AOP 面向切面编程实现的，本质就是在目标方法执行前后进行拦截。在目标方法执行前加入或创建一个事务，在执行方法执行后，根据实际情况选择提交或是回滚事务。 如何使用？ Spring 事务管理主要包括3个接口，Spring 的事务主要是由它们(PlatformTransactionManager，TransactionDefinition，TransactionStatus)三个共同完成的。 PlatformTransactionManager：事务管理器，主要用于平台相关事务的管理。 主要有三个方法：commit 事务提交；rollback 事务回滚；getTransaction 获取事务状态。 TransactionDefinition：事务定义信息，用来定义事务相关的属性，给事务管理器 PlatformTransactionManager 使用。 该接口有四个主要方法： getIsolationLevel：获取隔离级别； getPropagationBehavior：获取传播行为； getTimeout：获取超时时间； isReadOnly：是否只读（保存、更新、删除时属性变为false–可读写，查询时为true–只读） 事务管理器能够根据这个返回值进行优化，这些事务的配置信息，都可以通过配置文件进行配置。 TransactionStatus：事务具体运行状态，事务管理过程中，每个时间点事务的状态信息。 一些方法： hasSavepoint()：返回这个事务内部是否包含一个保存点 isCompleted()：返回该事务是否已完成，也就是说，是否已经提交或回滚 isNewTransaction()：判断当前事务是否是一个新事务 声明式事务的优缺点： 优点：不需要在业务逻辑代码中编写事务相关代码，只需要在配置文件配置或使用注解（@Transaction），这种方式没有侵入性。 缺点：声明式事务的最细粒度作用于方法上，如果像代码块也有事务需求，只能变通下，将代码块变为方法。 Spring 事务隔离级别及传播行为传播行为事务的第一个方面是传播行为。传播行为定义关于客户端和被调用方法的事务边界。Spring 定义了7中传播行为。 支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 TransactionDefinition.PROPAGATION_REQUIRED。 隔离级别 隔离级别 含义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别。 ISOLATION_READ_UNCOMMITTED 允许读取尚未提交的更改。可能导致脏读、幻影读或不可重复读。 ISOLATION_READ_COMMITTED 允许从已经提交的并发事务读取。可防止脏读，但幻影读和不可重复读仍可能会发生。 ISOLATION_REPEATABLE_READ 对相同字段的多次读取的结果是一致的，除非数据被当前事务本身改变。可防止脏读和不可重复读，但幻影读仍可能发生。 ISOLATION_SERIALIZABLE 完全服从 ACID 的隔离级别，确保不发生脏读、不可重复读和幻影读。这在所有隔离级别中也是最慢的，因为它通常是通过完全锁定当前事务所涉及的数据表来完成的。 Spring 中的 Bean如何创建 Spring 容器？容器是 Spring 的核心，在基于 Spring 的应用里，应用对象生存于 Spring 容器中。容器负责创建对象，装配它们，配置它们并管理它们的整个生命周期，从生存到死亡（类似从 new 到 finalize() ）。Spring 可以归为两种不同的类型： bean 工厂(由 org.springframework.beans.factory.BeanFactory 接口定义)是最简单的容器，提供基本的 DI 功能。 应用上下文(由 org.springframework.context.ApplicationContext 接口定义)基于 BeanFactory 构建，并提供应用框架级别的服务，例如从属性文件解析文本信息以及发布应用事件给感兴趣的事件监听者。 一般来说，bean 工厂太低级了，应用上下文的使用更为广泛。 使用应用上下文 常见五种类型： AnnotationConfigApplicationContext：从一个或多个基于 java 的配置类中加载 Spring 应用上下文 AnnotationConfigWebApplicationContext：从一个或多个基于 java 的配置类中加载Spring Web 应用上下文 ClassPathXmlApplicationContext：从路径下的一个或多个 XML 配置文件中加载上下文定义，把应用上下文的定义文件作为类资源 FileSystemXmlApplicationContext：从文件系统下的一个或多个 XML 配置文件中加载上下文定义 XmlWebApplicationContext：从 Web 应用下的一个或多个 XML 配置文件中加载上下文定义 ApplicationContext 上下文的生命周期 实例化一个 Bean，也就是我们通常说的 new； 按照 Spring 上下文对实例化的 Bean 进行配置，也就是 IOC 注入 如果这个 Bean 实现了 BeanNameAware 接口，会调用它实现的 setBeanName(String beanId) 方法，此处传递的是 Spring 配置文件中 Bean 的 ID 如果这个 Bean 实现了 BeanFactoryAware 接口，会调用它实现的 setBeanFactory() ，传递的是 Spring 工厂本身（可以用这个方法获取到其他 Bean） 如果这个 Bean 实现了 ApplicationContextAware 接口，会调用 setApplicationContext(ApplicationContext) 方法，传入 Spring 上下文，该方式同样可以实现步骤4，但比4更好，因为 ApplicationContext 是 BeanFactory 的子接口，有更多的实现方法 如果这个 Bean 关联了 BeanPostProcessor 接口，将会调用postProcessBeforeInitialization(Object obj, String s) 方法，BeanPostProcessor 经常被用作是 Bean 内容的更改，并且由于这个是在 Bean 初始化结束时调用 After 方法，也可用于内存或缓存技术 如果这个 Bean 在 Spring 配置文件中配置了 init-method 属性会自动调用其配置的初始化方法 如果这个 Bean 关联了 BeanPostProcessor 接口，将会调用 postAfterInitialization(Object obj, String s) 方法； 注意：以上工作完成以后就可以用这个 Bean 了，那这个 Bean 是一个 single 的，所以一般情况下我们调用同一个 ID 的 Bean 会是在内容地址相同的实例 当 Bean 不再需要时，会经过清理阶段，如果 Bean 实现了 DisposableBean 接口，会调用其实现的 destroy 方法 最后，如果这个 Bean 的 Spring 配置中配置了 destroy-method 属性，会自动调用其配置的销毁方法 以上10步骤可以作为面试或者笔试的模板，另外这里描述的是应用 Spring 上下文 Bean 的生命周期，如果应用 Spring 的工厂也就是 BeanFactory 的话去掉第5步就Ok了。 Bean 生命周期 Spring 对 Bean 进行实例化 Spring 将值和 Bean 的引用注入进 Bean 对应的属性中 如果 Bean 实现了 BeanNameAware 接口，Spring 将 Bean 的 ID 传递给 setBeanName() 方法 如果 Bean 实现了 BeanFactoryAware 接口，Spring 将调用 setBeanFactory(BeanFactory bf) 方法并把 BeanFactory 容器实例作为参数传入 实现 BeanFactoryAware 主要目的是为了获取 Spring 容器，如 Bean 通过 Spring 容器发布事件等 如果 Bean 实现了 ApplicationContextAwaer 接口，Spring 容器将调用 setApplicationContext(ApplicationContext ctx) 方法，将 bean 所在的应用上下文的引用传入进来 作用与 BeanFactory 类似都是为了获取 Spring 容器，不同的是 Spring 容器在调用 setApplicationContext 方法时会把它自己作为 setApplicationContext 的参数传入，而 Spring 容器在调用 setBeanFactory 前需要程序员自己指定（注入） setBeanFactory 里的 BeanFactory 参数 如果 Bean 实现了 BeanPostProcessor 接口，Spring 将调用它们的 postProcessBeforeInitialization() 预初始化方法 作用是在 Bean 实例创建成功后对进行增强处理，如对 Bean 进行修改，增加某个功能 如果 Bean 实现了 InitializingBean 接口，Spring 将调用它们的 afterPropertiesSet() 方法，作用与在配置文件中对 Bean 使用 init-method 声明初始化的作用一样，都是在 Bean 的全部属性设置成功后执行的初始化方法 如果 Bean 实现了 BeanPostProcessor 接口，Spring 将调用它们的 postProcessAfterInitialization() 后初始化方法 作用与6的一样，只不过6是在 Bean 初始化前执行的，而这个是在 Bean 初始化后执行的，时机不同 经过以上的工作后，Bean 将一直驻留在应用上下文中给应用使用，直到应用上下文被销毁 如果 Bean 实现了 DispostbleBean 接口，Spring 将调用它的 destory 方法，作用与在配置文件中对 Bean 使用 destory-method 属性的作用一样，都是在 Bean 实例销毁前执行的方法。 Bean 实例化的三种方式 使用类的无参构造创建（此种方式用的最多） 使用静态工厂创建对象 使用实例工厂创建对象 Bean 的作用域 类型 说明 单例(Singleton) 在整个应用中，只创建 bean 的一个实例。(默认) 原型(Prototype) 每次注入或者通过 Spring 应用上下文获取的时候，都会创建一个新的 bean 的实例。 会话(Session) 在 Web 应用中，为每个会话创建一个 bean 实例。 请求(Request) 在 Web 应用中，为每个请求创建一个 bean 实例。 BeanFactory 和 FactoryBean 的区别 BeanFactory 是个 Factory，也就是 IOC 容器或对象工厂，在 Spring 中，所有的 Bean 都是由 BeanFactory (也就是 IOC 容器)来进行管理的，提供了实例化对象和拿对象的功能。 FactoryBean 是个 Bean，这个 Bean 不是简单的 Bean，而是一个能生产或者修饰对象生成的工厂 Bean，它的实现与设计模式中的工厂模式和修饰器模式类似。 BeanFactory 和 ApplicationContext 的区别 区别 BeanFactory ApplicationContext 功能 BeanFactory 是 Spring 里面最低层的接口，提供了最简单的容器的功能，只提供了实例化对象和拿对象的功能。BeanFactory 需要在代码中通过手工调用 addBeanPostProcessor() 方法进行注册。 ApplicationContext 会利用 Java 反射机制自动识别出配置文件中定义的 BeanPostProcessor、 InstantiationAwareBeanPostProcessor 和 BeanFactoryPostProcessor 后置器，并自动将它们注册到应用上下文中。 装载 Bean BeanFactory 在初始化容器的时候并未实例化 Bean，直到第一次访问某个 Bean 时才实例化目标 Bean。 ApplicationContext 在初始化应用上下文的时候就实例化所有单实例的 Bean。 我们该用 BeanFactory 还是 ApplicationContent ？ BeanFactory 延迟实例化的优点： 应用启动的时候占用资源很少，对资源要求较高的应用，比较有优势，而且通过 Bean 工厂创建的 bean 生命周期会简单一些。 缺点：速度会相对来说慢一些，而且有可能会出现空指针异常的错误。 ApplicationContext 不延迟实例化的优点： 所有的 Bean 在启动的时候都加载，系统运行的速度快。 在启动的时候所有的 Bean 都加载了，我们就能在系统启动的时候，尽早的发现系统中的配置问题。 建议 web 应用，在启动的时候就把所有的 Bean 都加载了。 缺点：把费时的操作放到系统启动中完成，所有的对象都可以预加载，缺点就是消耗服务器的内存。 ApplicationContext 的其他特点 除了提供 BeanFactory 所支持的所有功能外，ApplicationContext 还有额外的功能 默认初始化所有的 Singleton，也可以通过配置取消预初始化。 继承 MessageSource，因此支持国际化。 资源访问，比如访问 URL 和文件（ResourceLoader）。 事件机制，（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的 web 层。 同时加载多个配置文件。 消息发送、响应机制（ApplicationEventPublisher）。 以声明式方式启动并创建 Spring 容器。 由于 ApplicationContext 会预先初始化所有的 Singleton Bean，于是在系统创建前期会有较大的系统开销，但一旦 ApplicationContext 初始化完成，程序后面获取 Singleton Bean 实例时候将有较好的性能。 也可以为 bean 设置 lazy-init 属性为 true，即 Spring 容器将不会预先初始化该 bean。 Spring 中的单例 bean 的线程安全问题了解吗？大部分时候我们并没有在系统中使用多线程，所以很少有人会关注这个问题。单例 Bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候，对这个对象的非静态成员变量的写操作会存在线程安全问题。 常见的有两种解决办法： 在 Bean 对象中尽量避免定义可变的成员变量（不太现实）。 在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 Spring中 autowire 和 resourse 关键字的区别@Resource 和 @Autowired 都是做 bean 的注入时使用，其实 @Resource 并不是 Spring 的注解，它的包是 javax.annotation.Resource，需要导入，但是 Spring 支持该注解的注入。 共同点两者都可以写在字段和 setter 方法上。两者如果都写在字段上，那么就不需要再写 setter 方法。 不同点@Autowired @Autowired 为 Spring 提供的注解，需要导入包 org.springframework.beans.factory.annotation.Autowired; 只按照 byType 注入。 12345678910public class TestServiceImpl &#123; // 下面两种@Autowired只要使用一种即可 @Autowired private UserDao userDao; // 用于字段上 @Autowired public void setUserDao(UserDao userDao) &#123; // 用于属性的方法上 this.userDao = userDao; &#125;&#125; @Autowired 注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许 null 值，可以设置它的 required 属性为 false。如果我们想使用按照名称（byName）来装配，可以结合 @Qualifier 注解一起使用。如下： 12345public class TestServiceImpl &#123; @Autowired @Qualifier("userDao") private UserDao userDao; &#125; @Resource @Resource 默认按照 byName 自动注入，由 J2EE 提供，需要导入包javax.annotation.Resource。 @Resource 有两个重要的属性：name 和 type，而 Spring 将@Resource 注解的 name 属性解析为 bean 的名字，而 type 属性则解析为 bean 的类型。所以，如果使用 name 属性，则使用 byName 的自动注入策略，而使用 type 属性时则使用 byType 自动注入策略。如果既不制定 name 也不制定 type 属性，这时将通过反射机制使用 byName 自动注入策略。 12345678910public class TestServiceImpl &#123; // 下面两种@Resource只要使用一种即可 @Resource(name="userDao") private UserDao userDao; // 用于字段上 @Resource(name="userDao") public void setUserDao(UserDao userDao) &#123; // 用于属性的setter方法上 this.userDao = userDao; &#125;&#125; 注：最好是将 @Resource 放在 setter 方法上，因为这样更符合面向对象的思想，通过 set、get 去操作属性，而不是直接去操作属性。 @Resource 装配顺序： 如果同时指定了 name 和 type，则从 Spring 上下文中找到唯一匹配的 bean 进行装配，找不到则抛出异常。 如果指定了 name，则从上下文中查找名称（id）匹配的 bean 进行装配，找不到则抛出异常。 如果指定了 type，则从上下文中找到类似匹配的唯一 bean 进行装配，找不到或是找到多个，都会抛出异常。 如果既没有指定 name，又没有指定 type，则自动按照 byName 方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配。 @Resource 的作用相当于 @Autowired，只不过 @Autowired 按照 byType 自动注入。@Resource 注解的使用性更为灵活，可指定名称，也可以指定类型 ；@Autowired 注解进行装配容易抛出异常，特别是装配的 bean 类型有多个的时候，而解决的办法是需要在增加 @Qualifier 进行限定。 Spring 常用注解一、组件类注解 注解 作用 @Component 标准一个普通的 Spring Bean 类 @Repository 标注一个 DAO 组件类 @Service 标注一个业务逻辑组件类 @Controller 标注一个控制器组件类 这些都是注解在平时的开发过程中出镜率极高，@Component、@Repository、@Service、@Controller 实质上属于同一类注解，用法相同，功能相同，区别在于标识组件的类型。@Component 可以代替 @Repository、@Service、@Controller，因为这三个注解是被 @Component 标注的。如下代码： 1234567@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Componentpublic @interface Controller &#123; String value() default "";&#125; 注意点 被注解的 java 类当做 Bean 实例，Bean 实例的名称默认是 Bean 类的首字母小写，其他部分不变。@Service 也可以自定义 Bean 名称，但是必须是唯一的！ 尽量使用对应组件注解的类替换 @Component 注解，在 Spring 未来的版本中，@Controller，@Service，@Repository 会携带更多语义。并且便于开发和维护！ 指定了某些类可作为 Spring Bean 类使用后，最好还需要让 Spring 搜索指定路径，在 Spring 配置文件加入如下配置： 12&lt;!-- 自动扫描指定包及其子包下的所有Bean类 --&gt;&lt;context:component-scan base-package="org.springframework.*"/&gt; 二、装配bean时常用的注解 注解 作用 @Autowired 属于 Spring 的 org.springframework.beans.factory.annotation 包下,可用于为类的属性、构造器、方法进行注值 @Resource 不属于 Spring 的注解，而是来自于 JSR-250 位于 javax.annotation 包下，使用该 annotation 为目标 bean 指定协作者 Bean @PostConstruct 实现初始化之前的操作 @PreDestroy 实现销毁 bean 之前进行的操作 注意点 使用 @Resource 也要注意添加配置文件到 Spring，如果没有配置 component-scan 12&lt;context:component-scan&gt; &lt;!--&lt;context:component-scan&gt;的使用，是默认激活&lt;context:annotation-config&gt;功能--&gt; 则一定要配置 annotation-config 1&lt;context:annotation-config/&gt; 三、@Component vs @Configuration and @Bean@Component vs @Configuration (类级) Spring 的官方团队说 @Component 可以替代 @Configuration 注解，事实上我们看源码也可以发现看到，如下： 1234567@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Component //看这里！！！public @interface Configuration &#123; String value() default ""; ... 虽然说可以替代但是两个注解之间还是有区别的！ @Configuration 中所有带 @Bean 注解的方法都会被动态代理，因此调用该方法返回的都是同一个实例。@Configuration 本质上还是 @Component，因此 &lt;context:component-scan/&gt; 或者 @ComponentScan 都能处理 @Configuration 注解的类。 @Configuration 注解的 bean 都已经变成了增强的类。示例： @Bean 注解方法执行策略 123456789101112@Configurationpublic class MyBeanConfig &#123; @Bean public Country country()&#123; return new Country(); &#125; @Bean public UserInfo userInfo()&#123; return new UserInfo(country()); &#125;&#125; 直接调用 country() 方法返回的是同一个实例，因为注解是 @Configuration 增强版本类，但是如果是变成 @Component 之后，此时返回的就不是一个实例了，每次都会创建一个实例。下例其实 new 了两次 Country： 123456789101112@Componentpublic class MyBeanConfig &#123; @Bean public Country country()&#123; return new Country(); &#125; @Bean public UserInfo userInfo()&#123; return new UserInfo(country()); &#125;&#125; 不过有一招可以让 @Component 也保证使用同一个实例——那就是用 @Autowired 来注入。 123456789101112131415@Componentpublic class MyBeanConfig &#123; @Autowired private Country country; @Bean public Country country()&#123; return new Country(); &#125; @Bean public UserInfo userInfo()&#123; return new UserInfo(country); &#125;&#125; @Configuration 标记的类必须符合下面的要求： 配置类必须以类的形式提供（不能是工厂方法返回的实例），允许通过生成子类在运行时增强（cglib 动态代理）。 配置类不能是 final 类（没法动态代理）。 配置注解通常为了通过 @Bean 注解生成 Spring 容器管理的类， 配置类必须是非本地的（即不能在方法中声明，不能是 private）。 任何嵌套配置类都必须声明为 static。 @Bean 方法可能不会反过来创建进一步的配置类（也就是返回的 bean 如果带有 @Configuration，也不会被特殊处理，只会作为普通的 bean）。 @Bean (方法级) @Bean 注解主要用于告诉方法，产生一个 Bean 对象，然后这个 Bean 对象交给 Spring 管理。产生这个 Bean 对象的方法 Spring 只会调用一次，随后这个 Spring 将会将这个 Bean 对象放在自己的 IOC 容器中。 当使用了 @Bean 注解，我们可以连续使用多种定义 bean 时用到的注解，譬如用 @Qualifier 注解定义工厂方法的名称，用 @Scope 注解定义该 bean 的作用域范围，譬如是 singleton 还是 prototype 等。 Spring 中新的 Java 配置支持的核心就是 @Configuration 注解的类。这些类主要包括 @Bean 注解的方法来为 Spring 的 IOC 容器管理的对象定义实例，配置和初始化逻辑。 使用 @Configuration 来注解类表示类可以被 Spring 的 IOC 容器所使用，作为 bean 定义的资源。 1234567@Configurationpublic class AppConfig &#123; @Bean public MyService myService() &#123; return new MyServiceImpl(); &#125;&#125; 这和 Spring 的 XML 文件中的非常类似。 123&lt;beans&gt; &lt;bean id="myService" class="com.acme.services.MyServiceImpl"/&gt;&lt;/beans&gt; @Bean 注解扮演了和元素相同的角色。 四、Spring MVC模块注解web模块常用到的注解 @Controller ：表明该类会作为与前端作交互的控制层组件，通过服务接口定义的提供访问应用程序的一种行为，解释用户的输入，将其转换成一个模型然后将试图呈献给用户。Spring MVC 使用 @Controller 定义控制器，它还允许自动检测定义在类路径下的组件（配置文件中配置扫描路径）并自动注册。 @RequestMapping ： 这个注解用于将 url 映射到整个处理类或者特定的处理请求的方法。可以只用通配符！既可以作用在类级别，也可以作用在方法级别。可以使用 value 属性指定具体路径，也可以用 method 属性标记所接受的请求类型。 @RequestParam ：将请求的参数绑定到方法中的参数上，有 required 参数，默认情况下，required = true，也就是该参数必须要传。如果该参数可以传可不传，可以配置required = false。 @PathVariable ： 该注解用于方法修饰方法参数，会将修饰的方法参数变为可供使用的 uri 变量（可用于动态绑定）。 @RequestBody ： 可以将请求体中的 JSON 字符串绑定到相应的 bean 上，当然，也可以将其分别绑定到对应的字符串上。 @ResponseBody ： @ResponseBody 与 @RequestBody 类似，它的作用是将返回类型直接输入到 HTTP response body 中。@ResponseBody 在输出 JSON 格式的数据时，会经常用到。 @RestController ：控制器实现了 REST 的 API，只为服务于 JSON，XML 或其它自定义的类型内容，@RestController 用来创建 REST 类型的控制器。@RestController = @Controller + @ResponseBody。 五、Spring 事务模块注解@Transactional 在处理 dao 层或 service 层的事务操作时，譬如删除失败时的回滚操作。使用 @Transactional 作为注解，但是需要在配置文件激活。 12&lt;!-- 开启注解方式声明事务 --&gt;&lt;tx:annotation-driven transaction-manager="transactionManager" /&gt; 举例： 12345678910111213@Servicepublic class CompanyServiceImpl implements CompanyService &#123; @Autowired private CompanyDAO companyDAO; @Transactional(propagation = Propagation.REQUIRED, readOnly = false, rollbackFor = Exception.class) public int deleteByName(String name) &#123; int result = companyDAO.deleteByName(name); return result; &#125; ...&#125; 属性 作用 readOnly 事务的读写属性，取 true 或者 false，true 为只读，默认为 false rollbackFor 回滚策略，当遇到指定异常时回滚。譬如上例遇到异常就回滚 timeout 设置超时时间，单位为秒 isolation 设置事务隔离级别，枚举类型，一共五种 Spring 中用了哪些设计模式Spring框架中使用到了大量的设计模式，下面列举了比较有代表性的： 代理模式：在 AOP 和 remoting 中被用的比较多。 单例模式：在 Spring 配置文件中定义的 bean 默认为单例模式。 模板方法：用来解决代码重复的问题。比如: RestTemplate, JmsTemplate, JpaTemplate。 工厂模式：BeanFactory 用来创建对象的实例。 适配器：Spring AOP 装饰器：Spring data hashmapper 观察者：Spring 时间驱动模型 回调：Spring ResourceLoaderAware 回调接口 Spring MVC的工作原理 客户端的所有请求都交给前端控制器 DispatcherServlet 来处理，它会负责调用系统的其他模块来真正处理用户的请求。 DispatcherServlet 把请求转发到 HandlerMapping 处理映射器。 找到具体映射之后，生成具体的对象或者拦截对象返回给 DispatcherServlet。 DispatcherServlet 请求 HandlerAdapter 适配器执行 Handler。 Handler(controller) 执行、调用处理器相应功能处理方法。 处理请求完毕后，返回 ModelAndView 给 DispatcherServlet。 DispatcherServlet 把 ModelAndView 交给 ViewResolver 视图解析器解析。 ViewResolver 视图解析器返回 view 给 DispatcherServlet。 DispatcherServlet 根据 view 进行渲染。(把 Model 填进视图) 返回响应给用户。 组件及作用 前端控制器 (DispatcherServlet) 接收请求，响应结果，相当于转发器，中央处理器。负责调用系统的其他模块来真正处理用户的请求。 有了 DispatcherServlet 减少了其他组件之间的耦合度 处理器映射器 (HandlerMapping) 作用：根据请求的 url 查找 Handler 处理器 (Handler) 注意：编写 Handler 时按照 HandlerAdapter 的要求去做，这样适配器才可以去正确执行 Handler 处理器适配器 (HandlerAdapter) 作用：按照特定规则（HandlerAdapter要求的规则）执行 Handler。 视图解析器 (ViewResolver) 作用：进行视图解析，根据逻辑视图解析成真正的视图 (View) 视图 (View) View 是一个接口实现类支持不同的 View 类型（jsp、pdf、图片、json字符串、XML、HTML等等） 注意：只需要程序员开发，处理器和视图。 Spring 注解的优点 可以充分利用 Java 的反射机制获取类结构信息，这些信息可以有效减少配置的工作。如使用 JPA 注释配置 ORM 映射时，我们就不需要指定 PO 的属性名、类型等信息，如果关系表字段和 PO 属性名、类型都一致，您甚至无需编写任务属性映射信息——因为这些信息都可以通过 Java 反射机制获取。 注释和 Java 代码位于一个文件中，而 XML 配置采用独立的配置文件，大多数配置信息在程序开发完成后都不会调整，如果配置信息和 Java 代码放在一起，有助于增强程序的内聚性。而采用独立的 XML 配置文件，程序员在编写一个功能时，往往需要在程序文件和配置文件中不停切换，这种思维上的不连贯会降低开发效率。 编译期校验，错误的注解在编译期间就会报错。注解在java代码中，从而避免了额外的文件维护工作。注解被编译成java字节码，消耗的内存小，读取速度快，往往比xml配置文件解析快几个数量级，利用测试和维护。 Spring AOP 和 AspectJ AOP 有什么区别？Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理 (Proxying)，而 AspectJ 基于字节码操作 (Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比 Spring AOP 快很多。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaWeb基础]]></title>
    <url>%2F2019%2F06%2F07%2FJavaWeb%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1. 什么是 Servlet？ 2. Servlet 类的继承关系 3. Servlet 实现方式 4. Tomcat 容器等级 5. Servlet 执行流程 6. Servlet 生命周期 7. Tomcat 装载 Servlet 的三种情况 8. forward 和 redirect 9. Jsp 和 Servlet 的区别 10. Tomcat 和 Servlet 的联系 11. cookie 和 session 12. JavaEE 中的三层结构和 MVC 13. RESTful 架构 1. 什么是 Servlet？Servlet 是在服务器上运行的小程序。一个 servlet 就是一个 Java 类，并且可以通过 “请求—响应” 编程模式来访问的这个驻留在服务器内存里的 servlet 程序。 2. Servlet 类的继承关系 3. Servlet 实现方式 实现 javax.servlet.Servlet 接口 继承 javax.servlet.GenericServlet 类 继承 javax.servlet.http.HttpServlet 类 通常会去继承 HttpServlet 类来完成 Servlet 4. Tomcat 容器等级Tomcat 的容器分为4个等级，Servlet 的容器管理 Context 容器，一个 Context 对应一个 Web 工程。 5. Servlet 执行流程主要描述了从浏览器到服务器，再从服务器到浏览器的整个执行过程。 1.浏览器请求 浏览器向服务器请求时，服务器不会直接执行我们的类，而是到 web.xml 里寻找路径名。 ① 浏览器输入访问路径后，携带了请求行，头，体 ② 根据访问路径找到已注册的 servlet 名称 ③ 根据映射找到对应的 servlet 名 ④ 根据根据 servlet 名找到我们全限定类名，既我们自己写的类 2.服务器创建对象 ① 服务器找到全限定类名后，通过反射创建对象，同时也创建了 servletConfig，里面存放了一些初始化信息（注意服务器只会创建一次 servlet 对象，所以 servletConfig 也只有一个） 3.调用 init 方法 ① 对象创建好之后，首先要执行 init 方法，但是我们发现我们自定义类下没有 init 方法，所以程序会到其父类 HttpServlet 里找 ② 我们发现 HttpServlet 里也没有 init 方法，所以继续向上找，既向其父类 GenericServlet 中继续寻找，在 GenericServlet 中我们发现了 init 方法，则执行 init 方法（对接口 Servlet 中的 init 方法进行了重写） 注意： 在 GenericServlet 中执行 public void init(ServletConfig config) 方法的时候，又调用了自己无参无方法体的 init() 方法，其目的是为了方便开发者，如果开发者在初始化的过程中需要实现一些功能，可以重写此方法。 4.调用 service 方法 接着，服务器会先创建两个对象：ServletRequest 请求对象和 ServletResponse 响应对象，用来封装浏览器的请求数据和封装向浏览器的响应数据 ① 接着服务器会默认在我们写的类里寻找 service(ServletRequest req, ServletResponse res) 方法，但是 DemoServlet 中不存在，那么会到其父类中寻找 ② 到父类 HttpServlet 中发现有此方法，则直接调用此方法，并将之前创建好的两个对象传入 ③ 然后将传入的两个参数强转，并调用 HttpServlet 下的另外个 service 方法 ④ 接着执行 service(HttpServletRequest req, HttpServletResponse resp) 方法，在此方法内部进行了判断请求方式，并执行 doGet 和 doPost，但是 doGet 和 doPost 方法已经被我们自己重写了，所以会执行我们重写的方法 看到这里，你或许有疑问：为什么我们不直接重写 service 方法？ 因为如果重写 service 方法的话，我们需要将强转，以及一系列的安全保护判断重新写一遍，会存在安全隐患。 4.向浏览器响应 6. Servlet 生命周期 加载和实例化：Servlet 容器负责加载和实例化 Servlet 对象。 初始化：void init(ServletConfig servletConfig) Servlet 对象创建之后马上执行的初始化方法，只执行一次。 请求处理：void service(ServletRequest servletRequest, ServletResponse servletResponse) 每次处理请求都是在调用这个方法，它会被调用多次 销毁：void destroy() 在 Servlet 被销毁之前调用，负责释放 Servlet 对象占用的资源的方法 服务器执行流程 Servlet 类由自己编写，但对象由服务器来创建，并由服务器来调用相应的方法。 服务器启动时 ( web.xml 中配置 load-on-startup=1，默认为0 ) 或者第一次请求该 Servlet 时，就会初始化一个 Servlet 对象，也就是会执行初始化方法 init(ServletConfig conf)。 该 Servlet 对象去处理所有客户端请求，在 service(ServletRequest req，ServletResponse res) 方法中执行。 最后服务器关闭时，才会销毁这个 Servlet 对象，执行 destroy() 方法。 一些问题 Servlet 何时创建？ 答：默认第一次访问 Servlet 时创建该对象(调用 init() 方法) Servlet何时销毁？答：服务器关闭 Servlet 就销毁了(调用 destroy() 方法) 每次访问必须执行的方法是什么？答：public void service(ServletRequest arg0, ServletResponse arg1) 7. Tomcat 装载 Servlet 的三种情况 Servlet 容器启动时自动装载某些 Servlet，实现它只需要在 web.xml 文件中的 &lt;servlet&gt;&lt;/servlet&gt; 之间添加以下代码： 1&lt;load-on-startup&gt;1&lt;/load-on-startup&gt; 其中，数字越小表示优先级越高。启动和关闭 Tomcat：优先级高的先启动也先关闭。 客户端首次向某个 Servlet 发送请求。 Servlet 类被修改后，Tomcat 容器会重新装载 Servlet。 8. forward 和 redirectServlet 中主要有两种实现跳转的方式：forward 与 redirect 方式。 forward 是服务器内部的重定向，服务器直接访问目标地址的 URL，把那个 URL 的响应内容读取过来，而客户端并不知道，因此在客户端浏览器的地址栏中不会显示转向后的地址，还是原来的地址。由于在整个定向的过程中用的是同一个 Request，因此 forward 会将 Request 的信息带到被定向的 JSP 或 Servlet 中使用。 redirect 则是客户端的重定向，是完全的跳转，即客户端浏览器会获取到跳转后的地址，然后重新发送请求，因此浏览器中会显示跳转后的地址。同事，由于这种方式比 forward 方式多了一次网络请求，因此其效率要低于 forward 方式。需要注意的是，客户端的重定向可以通过设置特定的 HTTP 头或改写 JavaScript 脚本实现。 鉴于以上的区别，一般当 forward 方式可以满足需求时，尽可能地使用 forward 方式。但在有些情况下，例如，需要跳转到下一个其他服务器上的资源，则必须使用 redirect 方式。 引申：filter的作用是什么？主要实现什么方法？ filter 使用户可以改变一个 request 并且修改一个 response。filter 不是一个 Servlet，它不能产生一个 response，但它能够在一个 request 到达 Servlet 之前预处理 request，也可以在离开 Servlet 时处理 response。filter 其实是一个 “Servlet Chaining” (Servler 链)。 一个 filter 的作用包括以下几个方面： 在 Servlet 被调用之前截获 在 Servlet 被调用之前检查 Servlet Request 根据需要修改 Request 头和 Request 数据 根据需要修改 Response 头和 Response 数据 在 Servlet 被调用之后截获 9. Jsp 和 Servlet 的区别不同之处在哪？ Servlet 在 Java 代码中通过 HttpServletResponse 对象动态输出 HTML 内容 JSP 在静态 HTML 内容中嵌入 Java 代码，Java 代码被动态执行后生成 HTML 内容 各自的特点 Servlet 能够很好地组织业务逻辑代码，但是在 Java 源文件中通过字符串拼接的方式生成动态 HTML 内容会导致代码维护困难、可读性差 JSP 虽然规避了 Servlet 在生成 HTML 内容方面的劣势，但是在 HTML 中混入大量、复杂的业务逻辑同样也是不可取的 通过 MVC 双剑合璧 既然 JSP 和 Servlet 都有自身的适用环境，那么能否扬长避短，让它们发挥各自的优势呢？答案是肯定的——MVC(Model-View-Controller)模式非常适合解决这一问题。 MVC模式（Model-View-Controller）是软件工程中的一种软件架构模式，把软件系统分为三个基本部分：模型（Model）、视图（View）和控制器（Controller）： Controller——负责转发请求，对请求进行处理 View——负责界面显示 Model——业务功能编写（例如算法实现）、数据库设计以及数据存取操作实现 在 JSP/Servlet 开发的软件系统中，这三个部分的描述如下所示： Web 浏览器发送 HTTP 请求到服务端，被 Controller(Servlet) 获取并进行处理（例如参数解析、请求转发） Controller(Servlet) 调用核心业务逻辑——Model部分，获得结果 Controller(Servlet) 将逻辑处理结果交给 View（JSP），动态输出 HTML 内容 动态生成的 HTML 内容返回到浏览器显示 MVC 模式在 Web 开发中的好处是非常明显，它规避了 JSP 与 Servlet 各自的短板，Servlet 只负责业务逻辑而不会通过 out.append() 动态生成 HTML 代码；JSP 中也不会充斥着大量的业务代码。这大大提高了代码的可读性和可维护性。 10. Tomcat 和 Servlet 的联系Tomcat 是 Web 应用服务器，是一个 Servlet/JSP 容器。Tomcat 作为 Servlet 容器，负责处理客户请求，把请求传送给 Servlet，并将 Servlet 的响应传送回给客户。而 Servlet 是一种运行在支持 Java 语言的服务器上的组件。Servlet 最常见的用途是扩展 Java Web 服务器功能，提供非常安全的，可移植的，易于使用的 CGI 替代品。 ① Tomcat 将 HTTP 请求文本接收并解析，然后封装成 HttpServletRequest 类型的 request 对象，所有的 HTTP 头数据读可以通过 request 对象调用对应的方法查询到。 ② Tomcat 同时会要响应的信息封装为 HttpServletResponse 类型的 response 对象，通过设置 response 属性就可以控制要输出到浏览器的内容，然后将 response 交给 tomcat，Tomcat 就会将其变成响应文本的格式发送给浏览器。 Java Servlet API 是 Servlet 容器(tomcat) 和 servlet 之间的接口，它定义了 serlvet 的各种方法，还定义了 Servlet 容器传送给 Servlet 的对象类，其中最重要的就是 ServletRequest 和 ServletResponse。所以说我们在编写 servlet 时，需要实现 Servlet 接口，按照其规范进行操作。 11. cookie 和 session什么是 cookie？Cookie 是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现 Session 的一种方式。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。 什么是 session？Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 二者区别 区别 cookie session 作用范围 保存在客户端(浏览器) 保存在服务器端 存取方式 只能保存 ASCII 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等 有效期 可设置为长时间保持，比如我们经常使用的默认登录功能 一般失效时间较短，客户端关闭或者 Session 超时都会失效 隐私策略 存储在客户端，比较容易遭到不法获取 存储在服务端，安全性相对 Cookie 要好一些 存储大小 单个 Cookie 保存的数据不能超过 4K 可存储数据远高于 Cookie 为什么需要 cookie 和 session，他们有什么关联？说起来为什么需要 Cookie ，这就需要从浏览器开始说起，我们都知道浏览器是没有状态的(HTTP 协议无状态)，这意味着浏览器并不知道是张三还是李四在和服务端打交道。这个时候就需要有一个机制来告诉服务端，本次操作用户是否登录，是哪个用户在执行的操作，那这套机制的实现就需要 Cookie 和 Session 的配合。 既然服务端是根据 Cookie 中的信息判断用户是否登录，那么如果浏览器中禁止了 Cookie，如何保障整个机制的正常运转。 第一种方案，每次请求中都携带一个 SessionID 的参数，也可以 Post 的方式提交，也可以在请求的地址后面拼接 xxx?SessionID=123456...。 第二种方案，Token 机制。Token 机制多用于 App 客户端和服务器交互的模式，也可以用于 Web 端做用户状态管理。 Token 的意思是“令牌”，是服务端生成的一串字符串，作为客户端进行请求的一个标识。Token 机制和 Cookie 和 Session 的使用机制比较类似。 当用户第一次登录后，服务器根据提交的用户信息生成一个 Token，响应时将 Token 返回给客户端，以后客户端只需带上这个 Token 前来请求数据即可，无需再次登录验证。 如何考虑分布式 Session 问题？在互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。 分布式 Session 一般会有以下几种解决方案： Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。 Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。 共享 Session，服务端无状态话，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。 12. JavaEE 中的三层结构和 MVC做企业应用开发时，经常采用三层架构分层：表示层、业务层、持久层。 表示层：负责接收用户请求、转发请求、显示数据等。 业务层：负责组织业务逻辑。 持久层：负责持久化业务对象。 这三个分层，每一层都有不同的模式，就是架构模式。表示层最常用的架构模式就是 MVC。 MVC 是客户端的一种设计模式，所以他天然就不考虑数据如何存储的问题。作为客户端，只需要解决用户界面、交互和业务逻辑就好了。在 MVC 模式中，View 负责的是用户界面，Controller 负责交互，Model 负责业务逻辑。至于数据如何存储和读取，当然是由 Model 调用服务端的接口来完成。 在三层架构中，并没有客户端/服务端的概念，所以表示层、业务层的任务其实和 MVC 没什么区别，而持久层在 MVC 里面是没有的。 总结：MVC = 表示层 + 业务层，但不包括持久层。 13. RESTful 架构什么是REST？REST 是所有 Web 应用都应该遵守的架构设计指导原则。 面向资源是 REST 最明显的特征，对于同一个资源的一组不同的操作。对于每个资源只能执行一组有限的操作。（7个HTTP方法：GET/POST/PUT/DELETE/PATCH/HEAD/OPTIONS） 常用操作 HTTP方法 功能 GET select，从服务器取出资源（一项或多项）。 POST create，在服务器新建一个资源。 PUT update，在服务器更新资源（客户端提供改变后的完整资源）。 DELETE delete，从服务器删除资源。]]></content>
      <categories>
        <category>java</category>
        <category>jsp</category>
        <category>Servlet</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jsp</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秒杀系统服务器优化思路]]></title>
    <url>%2F2019%2F06%2F01%2F%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[一、Tomcat 优化(Tomcat8)内存优化 catalina.sh1JAVA_OPTS="-server -Xms2048M -Xmx2048M -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=5 -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$CATALINA_HOME/logs/heap.dump" 并发优化参考 ${tomcat}/webapps/docs/config/http.html 主要参数 作用 maxConnections 最大连接数，The maximum number of connections that the server will accept and process at any given time. acceptCount 最大接收数，The maximum queue length for incoming connection requests when all possible request processing threads are in use. maxThreads 工作线程，The maximum number of request processing threads to be created by this Connector. minSpareThreads 最小空闲的工作线程（初始化线程数），The minimum number of threads always kept running. 其他优化 参考 ${tomcat}/webapps/docs/config/host.html autoDeploy：This flag value indicates if Tomcat should check periodically for new or updated web applications while Tomcat is running. 参考 ${tomcat}/webapps/docs/config/http.html enableLookups：false 参考 ${tomcat}/webapps/docs/config/context.html reloadable：false connector：apr优化 详见 http://apr.apache.org/，这是一种全新的网络调度模型，打破了传统的 BIO 和 NIO限制。 注意：开启了 apr 之后，JVM 用到的 native 内存会增大，因此要适当调大 Metaspace 空间，添加 JVM 选项： 12-XX:MetaspaceSize=128mJAVA_OPTS="-server -Xms2048M -Xmx2048M -XX:MetaspaceSize=128M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$CATALINA_HOME/logs/heap.dump" 二、nginx 优化文档地址：http://nginx.org/en/docs/ 工作线程数和并发连接数1worker_rlimit_nofile 20480; #每个进程打开的最大的文件数=worker_connections*2是安全的，受限于操作系统(/etc/security/limits.conf) 12345vi /etc/security/limits.conf* hard nofile 204800* soft nofile 204800* soft core unlimited* soft stack 204800 123456worker_processes 4; #cpu,如果nginx单独在一台机器上，一般为核数的1~2倍events &#123; worker_connections 10240; #每一个进程打开的最大连接数，包含了nginx与客户端和upstream之间的连接 multi_accept on; #可以一次建立多个连接 use epoll;&#125; 操作系统优化配置文件 /etc/sysctl.conf 12345sysctl -w net.ipv4.tcp_syncookies=1; #防止一个套接字在有过多试图连接到达时引起过载sysctl-w net.core.somaxconn=1024; #默认128，连接队列sysctl-w net.ipv4.tcp_fin_timeout=10; #timewait的超时时间sysctl -w net.ipv4.tcp_tw_reuse=1; #os直接使用timewait的连接sysctl -w net.ipv4.tcp_tw_recycle=0; #回收禁用 Keepalive 长连接nginx 与 upstream server： 1234upstream server_pool&#123; server localhost:8080 weight=1 max_fails=2 fail_timeout=30s; keepalive 300; #300个长连接&#125; 同时要在 location 中设置： 12345location / &#123; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade";&#125; 客户端与 nginx(默认是打开的)： 123keepalive_timeout 60s; #长连接的超时时间keepalive_requests 100; #100个请求之后就关闭连接，可以调大keepalive_disable msie6; #ie6禁用 启用压缩1234567gzip on;gzip_http_version 1.1;gzip_disable "MSIE [1-6]\.(?!.*SV1)";gzip_proxied any;gzip_types text/plain text/css application/javascript application/x-javascript application/json application/xml application/vnd.ms-fontobject application/x-font-ttf application/svg+xml application/x-icon;gzip_vary on; #Vary: Accept-Encodinggzip_static on; #如果有压缩好的，直接使用 状态监控123456location = /nginx_status &#123; stub_status on; access_log off; allow &lt;YOURIPADDRESS&gt;; deny all;&#125; 输出结果： 1234Active connections: 1 server accepts handled requests 17122 17122 34873 Reading: 0 Writing: 1 Waiting: 0 Active connections: 当前实时的并发连接数accepts: 收到的总连接数handled: 处理的总连接数requests: 处理的总请求数Reading: 当前有多少个读，读取客户端的请求Writing: 当前有多少个写，向客户端输出Waiting: 当前有多少个长连接 (reading + writing)reading – nginx reads request headerwriting – nginx reads request body, processes request, or writes response to a clientwaiting – keep-alive connections, actually it is active - (reading + writing) 实时请求信息统计 ngxtophttps://github.com/lebinh/ngxtop 安装 python-pip 12yum install epel-releaseyum install python-pip 安装 ngxtop 1pip install ngxtop 使用 指定配置文件：ngxtop -c ./conf/nginx.conf 查询状态是200：ngxtop -c ./conf/nginx.conf --filter &#39;status == 200&#39; 查询哪个 ip 访问最多：ngxtop -c ./conf/nginx.conf --group-by remote_addr 三、LVS 四层负载均衡是什么LVS：linux virtual server 相关文章： http://www.linuxvirtualserver.org/http://zh.linuxvirtualserver.org/ http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/http://www.linuxvirtualserver.org/whatis.html 工作模式 VS/NAT：修改报文头信息 VS/TUNE：IP隧道 VS/DR：必须得在同一个网段(一般用这个) 八种调度算法轮询，加权轮询，最小连接，加权最小连接，局部最小连接，带复制的局部最小连接，目标地址散列，原地址散列 四、Keepalived 高可用http://www.keepalived.org/ Keepalived 双机热备]]></content>
      <categories>
        <category>projects</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>projects</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BIO,NIO和AIO]]></title>
    <url>%2F2019%2F05%2F20%2FBIO-NIO%E5%92%8CAIO%2F</url>
    <content type="text"><![CDATA[前言在 Java 中，有三种 IO 模型: BIO, NIO, AIO。介绍这三种 IO 模型之前，需要介绍一下同步，异步与阻塞，非阻塞的概念，然后再从 Java 和 Linux OS 的角度去分析 BIO, NIO 和 AIO。 同步和异步同步同步就是发起一个调用后，被调用者未处理完请求之前，调用不返回。 异步异步就是发起一个调用后，立刻得到被调用者的回应表示已接收到请求，但是被调用者并没有返回结果，此时我们可以处理其他的请求，被调用者通常依靠事件，回调等机制来通知调用者其返回结果。 区别同步与异步最大的区别就是被调用方的执行方式和返回时机，同步指的是被调用方做完事情之后再返回，异步指的是被调用方先返回，然后再做事情，做完之后再想办法通知调用方。 阻塞和非阻塞阻塞阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。 非阻塞非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。 同步、异步和阻塞、非阻塞的区别阻塞和同步不是一回事，同步，异步与阻塞，非阻塞针对的对象是不一样的，阻塞,非阻塞是说的调用者，同步，异步说的是被调用者。 BIO、NIO、AIO概览 BIO(Blocking I/O)：BIO 也就是传统的同步阻塞 IO 模型，对应 Java.io 包，它提供了很多 IO 功能，比如输入输出流，对文件进行操作。在网络编程( Socket 通信)中也同样进行 IO 操作。 NIO(New I/O): NIO 是一种同步非阻塞的 I/O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。 AIO: AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。 Linux的5种 I/O模型上面简单介绍了 Java 中的三种 IO 模型，三种模型提供的与 IO 有关的 API，在文件处理时，底层实际上是依赖操作系统层面的 IO 操作实现的，比如在 Linux 2.6 以后，Java 中的 NIO 和 AIO 都是通过 epoll 来实现的，关于 epoll 等概念后面也会阐述。 而实际上在 Linux(Unix) 操作系统中，共有五种 IO 模型，分别是：阻塞 IO 模型、非阻塞 IO 模型、IO 复用模型、信号驱动 IO 模型以及异步 IO 模型，而4种都是同步的，只有最后一种是异步的。 阻塞IO模型 - BIO 一个输入操作通常包括两个不同的阶段： 等待数据准备好 从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达，当所等待分组到达时，它被复制到内核中的某个缓冲区，第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 从上图可以看出，应用进程通过系统调用 recvfrom 去接收数据，而由于内核数据没有准备好，应用进程就会阻塞，直到内核准备好数据并将其从内核复制到应用进程的缓冲区中或者发生错误才返回。最常见的错误就是系统调用被信号中断。进程从调用 recvfrom 开始到它返回的整段时间内是被阻塞的。 Linux下的阻塞式 I/O 模型就对应了 Java下的 BIO 模型，BIO 的底层实现是调用操作系统的 API 去执行的，也就是调用操作系统的 Socket 套接字。 非阻塞式I/O模型 - NIO 应用进程通过系统调用 recvfrom 不断的去和内核交互，直到内核数据报准备好，而如果内核无数据准备好，转而立即返回一个 EWOULDBLOCK 的错误，过一段时间再次发送 recvfrom 请求，在此期间进程可以做其他事情，不用一直等待，这就是非阻塞。 当一个应用进程循环调用 recvfrom 时，我们称之为轮询(polling)，应用进程持续轮询内核，以查看某个操作是否就绪。Java 的 NIO 映射到 Linux 操作系统就是如上图所示的非阻塞 I/O 模型。 I/O复用模型 IO 多路复用使用 select/poll/epoll 函数，多个进程的 IO 都可以注册在同一个 select 上，当用户进程调用该 select 时，select 去监听所有注册好的 IO,如果所有被监听的 IO 需要的数据都没有准备好，那么 select 调用进程会被阻塞，只要任意一个 IO 的数据报套接字变为可读，即数据报已经准备好，select 就返回套接字可读这一条件，然后调用 recvfrom 把所读数据报复制到应用进程缓冲区。 强调一点就是，IO 多路复用模型并没有涉及到非阻塞，进程在发出 select 后，要一直阻塞等待其监听的所有 IO 操作至少有一个数据准备好才返回，强调阻塞状态，不存在非阻塞。 而在 Java NIO 中也可以实现多路复用，主要是利用多路复用器 Selector，与这里的 select函数类型，Selector 会不断轮询注册在其上的通道 Channel，如果有某一个 Channel 上面发生读或写事件，这个 Channel 处于就绪状态，就会被 Selector 轮询出来。关于 Java NIO 实现多路复用更多的介绍请查询相关文章。 I/O 多路复用的主要应用场景 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字 服务器需要同时处理多种网络协议的套接字 I/O多路复用的系统调用函数 目前支持 I/O 多路复用的系统调用函数有 select，pselect，poll，epoll。在 Linux 网络编程中，很长一段时间都使用 select 做轮询和网络事件通知。然而因为 select 的一些固有缺陷导致它的应用受到了很大的限制，比如 select 单个进程打开的最大句柄数是有限的。最终在 Linux 2.6 选择 epoll 替代了 select，Java NIO 和 AIO 底层就是用 epoll。 信号驱动式I/O模型 应用进程预先向内核安装一个信号处理函数，然后立即返回，进程继续工作，不阻塞，当数据报准备好读取时，内核就为该进程产生一个信号通知进程，然后进程再调用 recvfrom 读取数据报。 信号驱动式IO不是异步的 信号驱动式 IO 在数据准备阶段是异步的，当内核中有数据报准备后再通知进程，但是在调用 recvfrom 操作进行数据拷贝时是同步的，所以总体来说，整个 IO 过程不能是异步的。 异步I/O模型 - AIO 应用进程调用 aio_read 函数，给内核传递描述符，缓存区指针，缓存区大小和文件偏移，并告诉内核当整个操作完成时如何通知进程，然后该系统调用立即返回，而且在等待 I/O 完成期间，我们的进程不被阻塞，进程可以去干其他事情，然后内核开始等待数据准备，数据准备好以后再拷贝数据到进程缓冲区，最后通知整个 IO 操作已完成。 Java 的 AIO 提供了异步通道 API，其操作系统底层实现就是这个异步 I/O 模型。 与信号驱动式I/O的区别 主要区别在于: 信号驱动式 I/O 是由内核通知我们何时去启动一个 I/O 操作，而异步 I/O 模型是由内核通知我们 I/O 操作何时完成。 5种I/O模型的比较 由上图可以再次看出，IO操作主要分为两个阶段: 等待数据报准备阶段 数据拷贝阶段 前4种 IO 模型都是同步 IO 模型，为什么说都是同步的，因为它们在第二步数据拷贝阶段都是阻塞的，这会导致整个请求进程存在阻塞的情况，所以是同步的，而异步 IO 模型不会导致请求进程阻塞。 I/O复用的实现select/poll/epoll 都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 select有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义。 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 pollpollfd 使用链表实现。 select 和 poll 比较 功能 select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。 select 会修改描述符，而 poll 不会； select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 的描述符类型使用链表实现，没有描述符数量的限制； poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。 速度 select 和 poll 速度都比较慢。 select 和 poll 每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。 select 和 poll 的返回结果中没有声明哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程都需要使用轮询的方式来找到 I/O 完成的描述符。 可移植性 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 epollepoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。 epoll工作模式epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。 LT 模式 当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 ET 模式 和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。 很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。 select 应用场景 select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。 select 可移植性更好，几乎被所有主流平台所支持。 poll 应用场景 poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 epoll 应用场景 只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。 需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2019%2F04%2F27%2FRedis%2F</url>
    <content type="text"><![CDATA[一、概述 二、数据类型 三、常用基本操作 四、配置文件 五、持久化 六、事务 七、发布订阅 八、复制 九、Jedis 一、概述Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。 键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。 Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 二、数据类型 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 对单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 string string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如 jpg 图片或者序列化的对象 。 string 类型是 Redis 最基本的数据类型，一个 redis 中字符串 value 最多可以是 512M。 list Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。它的底层实际是个链表。 set Redis 的 set 是 string 类型的无序集合。它是通过 HashTable 实现实现的。 hash Redis hash 是一个键值对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 类似 Java 里面的 Map&lt;String,Object&gt;。 zset sorted set：有序集合。 Redis zset 和 set 一样也是 string 类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个 double 类型的分数。 redis 正是通过分数来为集合中的成员进行从小到大的排序。zset 的成员是唯一的,但分数 (score) 却可以重复。 Redis常见数据结构操作命令http://redisdoc.com/ 三、常用基本操作key 指令 作用 keys * 查看所有 key exists key的名字 判断某个 key 是否存在 move key db 移动 key 到 db expire key 秒数 为 key 设置过期时间 ttl key 查看还有所少秒过期，-1表示永不过期，-2表示已过期 type key 查看 key 的类型 string (单值单 value) 指令 作用 set/get/del/append/strlen 增、查、删、追加、返回长度 lncr/decr/incrby/decrby 加1，减1，加n，减n(数字才能加减，n路:incrby k1 n) getrange/setrange getrange: 获取指定区间范围内的值，从零到负一表示全部。setrange: 设置指定区间范围内的值，格式是setrange key 开始位 值(例:setrange k1 0 xxx) setex key 秒数 值 即 set with expire，设置带过期时间的 key，动态设置 setnx key value 即 set if not exist，只有在 key 不存在时设置 key 的值 mset/mget/msetnx mset: 同时设置一个或多个 key-value 对。mget: 获取所有(一个或多个)给定 key 的值。msetnx: 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 getset 即先 get 再 set，将给定 key 的值设为 value ，并返回 key 的旧值(old value)。 list (单值多 value) 指令 作用 lpush/rpush/lrange lpush: 从左侧按顺序添加(例: lpush list01 1 2 3 4 5)rpush: 从右侧按顺序添加(例: rpush list02 1 2 3 4 5)lrange: 按范围查询(lrange 列表名 0 -1 查询全部) lpop/rpop lpop: 左出；rpop: 右出(lpop/rpop 列表名) lindex 通过索引获取列表中的元素 lindex key index llen 返回列表 key 的长度 lerm key count value 删除 count 个等于 value 的 key，如果 count 为0则删除全部 ltrim key startindex endindex 截取指定索引区间的元素 rpoplpush 源列表 目的列表 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 lset key index value 通过索引设置列表元素的值 linsert key before/after value1 value2 在 list 某个已有值的前后再添加具体值 性能分析 它是一个字符串链表，left、right 都可以插入添加；如果键不存在，创建新的链表；如果键已存在，新增内容；如果值全移除，对应的键也就消失了。链表的操作无论是头和尾效率都极高，但假如是对中间元素进行操作，效率就很惨淡了。 set (单值多 value) 参数 作用 sadd/smembers/sismember 添加、返回集合中的所有的成员(smembers key)、判断成员元素是否是集合的成员(sismember key value) scard key 返回集合中元素的数量 srem key value1…valueN 移除集合中的一个或多个成员元素，不存在的成员元素会被忽略。 srandmember key [count] 返回集合中的一个随机元素。如果 count 为正数，且小于集合基数，那么命令返回一个包含 count 个元素的数组，数组中的元素各不相同。如果 count 大于等于集合基数，那么返回整个集合。如果 count 为负数，那么命令返回一个数组，数组中的元素可能会重复出现多次，而数组的长度为 count 的绝对值。 spop key [count] 用于移除集合中的指定 key 的一个或多个随机元素，移除后会返回移除的元素。 smove key1 key2 在key1里某个值 作用是将 key1 里的某个值赋给 key2 sdiff FIRST_KEY OTHER_KEY1..OTHER_KEYN 返回给定集合之间的差集。差集的结果来自前面的 FIRST_KEY ,而不是后面的 OTHER_KEY1，也不是整个 FIRST_KEY OTHER_KEY1..OTHER_KEYN 的差集。 sunion KEY KEY1..KEYN 返回给定集合的并集。不存在的集合 key 被视为空集。 sinter KEY KEY1..KEYN 返回给定所有给定集合的交集。 不存在的集合 key 被视为空集。 当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。 hash (KV 模式不变，但 V 是一个键值对) 参数 作用 hset/hget/hmset/hmget/hgetall/hdel 设值、获取、同时设值多个、获取所有给定字段的值、获取在哈希表中指定 key 的所有字段和值、删除一个或多个哈希表字段 hlen key 获取哈希表中字段的数量 hexists key field 查看哈希表 key 中，指定的字段是否存在 hkeys key/hvals key 获取所有哈希表中的字段、获取哈希表中所有值 hincrby key field increment/hincrbyfloatkey field increment 为哈希表 key 中的指定字段的整数值加上增量 increment 、为哈希表 key 中的指定字段的浮点数值加上增量 increment 。 hsetnx key field value 只有在字段 field 不存在时，设置哈希表字段的值。 zset (sorted set，在set基础上，加一个score值。之前set是k1 v1 v2 v3，现在zset是k1 score1 v1 score2 v2) 参数 作用 zadd/zrange 向有序集合添加一个或多个成员，或者更新已存在成员的分数、通过索引区间返回有序集合成指定区间内的成员 zrangebyscore key min max [WITHSCORES] [LIMIT] 返回有序集合中指定分数区间的成员列表。 zrem key member [member …] 移除有序集中的一个或多个成员，不存在的成员将被忽略。 zcard key/zcount key min max/zrank key member/zscore key member 获取有序集合的成员数、计算在有序集合中指定区间分数的成员数、返回有序集合中指定成员的索引、返回有序集中，成员的分数值 zrevrank key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 zrevrange key start stop [WITHSCORES] 返回有序集中指定区间内的成员，通过索引，分数从高到底 zrevrangebyscore key max min [WITHSCORES] 返回有序集中指定分数区间内的成员，分数从高到低排序 四、配置文件 Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程daemonize no 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定pidfile /var/run/redis.pid 指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字port 6379 绑定的主机地址bind 127.0.0.1 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能timeout 300 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verboseloglevel verbose 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/nulllogfile stdout 设置数据库的数量，默认数据库为0，可以使用SELECT 命令在连接上指定数据库iddatabases 16 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合save Redis默认配置文件中提供了三个条件：save 900 1save 300 10save 60 10000分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大rdbcompression yes 指定本地数据库文件名，默认值为dump.rdbdbfilename dump.rdb 指定本地数据库存放目录dir ./ 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步slaveof 当master服务设置了密码保护时，slav服务连接master的密码masterauth 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH 命令提供密码，默认关闭requirepass foobared 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息maxclients 128 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区maxmemory 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为noappendonly no 指定更新日志文件名，默认为appendonly.aofappendfilename appendonly.aof 指定更新日志条件，共有3个可选值：no：表示等操作系统进行数据缓存同步到磁盘（快）always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）everysec：表示每秒同步一次（折衷，默认值）appendfsync everysec 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制）vm-enabled no 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享vm-swap-file /tmp/redis.swap 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0vm-max-memory 0 Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值vm-page-size 32 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。vm-pages 134217728 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4vm-max-threads 4 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启glueoutputbuf yes 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法hash-max-zipmap-entries 64hash-max-zipmap-value 512 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍）activerehashing yes 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件include /path/to/local.conf 五、持久化 Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 RDB (Redis DataBase)是什么在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的 Snapshot 快照，它恢复时是将快照文件直接读到内存里。(rdb 保存的是 dump.rdb 文件) Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 整个过程中，主进程是不进行任何 IO 操作的，这就确保了极高的性能。 如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那 RDB 方式要比AOF 方式更加的高效。RDB 的缺点是最后一次持久化后的数据可能丢失。 forkfork 的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等）数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程。 如何触发 RDB 快照 配置文件中默认的快照配置，1分钟改1w次或5分钟改10次或15分钟改1次(可以冷拷贝后重新使用cp dump.rdb dump_new.rdb) 命令 save 或者 bgsave Save：save 时只管保存，其它不管，全部阻塞。 BGSAVE：Redis 会在后台异步进行快照操作，快照同时还可以响应客户端请求。可以通过 lastsave 命令获取最后一次成功执行快照的时间 执行 flushall 命令，也会产生 dump.rdb 文件，但里面是空的，无意义 如何恢复将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可 CONFIG GET dir 获取目录 优劣优势：适合大规模的数据恢复，对数据完整性和一致性要求不高 劣势：在一定间隔时间做一次备份，所以如果 redis 意外 down 掉的话，就 会丢失最后一次快照后的所有修改；fork 的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑 如何停止动态停止所有 RDB 保存规则的方法：redis-cli config set save “” AOF (Append Only File)是什么以日志的形式来记录每个写操作，将 redis 执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，redis 启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。(AOF 保存的是 appendonly.aof 文件) AOF 启动/修复/恢复正常恢复 启动：修改默认的 appendonly no，改为 yes 将有数据的 AOF 文件复制一份保存到对应目录( config get dir ) 恢复：重启 redis 然后重新加载 异常恢复 启动：修改默认的 appendonly no，改为 yes 备份被写坏的 AOF 文件 修复：redis-check-aof –fix 进行修复 恢复：重启 redis 然后重新加载 rewrite是什么 AOF 采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制，当 AOF 文件的大小超过所设定的阈值时，Redis 就会启动 AOF 文件的内容压缩，只保留可以恢复数据的最小指令集。可以使用命令 bgrewriteaof。 重写原理 AOF 文件持续增长而过大时，会 fork 出一条新进程来将文件重写(也是先写临时文件最后再 rename)，遍历新进程的内存中数据，每条记录有一条的 Set 语句。重写 AOF 文件的操作，并没有读取旧的 AOF 文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的 AOF 文件，这点和快照有点类似。 触发机制 redis 会记录上次重写时的 AOF 大小，默认配置是当 AOF 文件大小是上次 rewrite 后大小的一倍且文件大于 64M 时触发。 优劣优势：每修改同步：appendfsync always 同步持久化 每次发生数据变更会被立即记录到磁盘 性能较差但数据完整性比较好；每秒同步：appendfsync everysec 异步操作，每秒记录 如果一秒内宕机，有数据丢失；不同步：appendfsync no 从不同步。 劣势：相同数据集的数据而言 aof 文件要远大于 rdb 文件，恢复速度慢于 rdb；aof 运行效率要慢于 rdb，每秒同步策略效率较好，不同步效率和 rdb 相同。 总结(用哪个) RDB 持久化方式能够在指定的时间间隔能对你的数据进行快照存储 AOF 持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，AOF 命令以 redis 协议追加保存每次写的操作到文件末尾。redis 还能对 AOF 文件进行后台重写，使得 AOF 文件的体积不至于过大。 只做缓存：如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。 同时开启两种持久化方式 在这种情况下，当 redis 重启的时候会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整。 RDB 的数据不实时，同时使用两者时服务器重启也只会找 AOF 文件。那要不要只使用 AOF 呢？作者建议不要，因为 RDB 更适合用于备份数据库(AOF 在不断变化不好备份)，快速重启，而且不会有 AOF 可能潜在的 bug，留着作为一个万一的手段。 性能建议 因为 RDB 文件只用作后备用途，建议只在 Slave 上持久化 RDB 文件，而且只要15分钟备份一次就够了，只保留 save 900 1 这条规则。 如果 Enalbe AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只 load 自己的 AOF 文件就可以了。代价一是带来了持续的 IO，二是 AOF rewrite 的最后将 rewrite 过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少 AOF rewrite 的频率，AOF 重写的基础大小默认值64M 太小了，可以设到 5G 以上。默认超过原大小 100% 大小时重写可以改到适当的数值。 如果不 Enable AOF ，仅靠 Master-Slave Replication 实现高可用性也可以。能省掉一大笔 IO 也减少了 rewrite 时带来的系统波动。代价是如果 Master/Slave 同时倒掉，会丢失十几分钟的数据，启动脚本也要比较两个 Master/Slave 中的 RDB 文件，载入较新的那个。新浪微博就选用了这种架构。 六、事务是什么可以一次执行多个命令，本质是一组命令的集合。一个事务中的所有命令都会序列化，按顺序地串行化执行而不会被其它命令插入，不许加塞。 能干嘛一个队列中，一次性、顺序性、排他性的执行一系列命令。 场景 正常执行 放弃事务 全体连坐(一条有错，全军覆没) 冤头债主(exec 才发现的错误，不影响事务中其他语句的执行) watch 监控 场景：初始化信用卡可用余额和欠额 无加塞篡改，先监控再开启 multi，保证两笔金额变动在同一个事务内。 有加塞篡改，监控了 key，如果 key 被修改了，后面一个事务的执行失效。 unwatch 一旦执行了 exec 之前加的监控锁都会被取消掉了 小结 Watch 指令，类似乐观锁，事务提交时，如果 Key 的值已被别的客户端改变，比如某个 list 已被别的客户端 push/pop 过了，整个事务队列都不会被执行。 通过 WATCH 命令在事务执行之前监控了多个 Keys，倘若在 WATCH 之后有任何 Key 的值发生了变化，EXEC 命令执行的事务都将被放弃，同时返回 Nullmulti-bulk 应答以通知调用者事务执行失败。 指令 指令 作用 DISCARD 取消事务，放弃执行事务块内的所有命令。 EXEC 执行所有事务块内的命令。 MULTI 标记一个事务块的开始。 UNWATCH 取消 WATCH 命令对所有 key 的监视。 WATCH key [key …] 监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 三阶段开启：以 MULTI 开始一个事务。 入队：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面。 执行：由 EXEC 命令触发事务。 三特性单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念：队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在”事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题。 不保证原子性：redis 同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚。(部分支持事务：全体连坐和冤头债主) 七、发布订阅是什么进程间的一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 实际中不会用 redis 做消息中间件 八、复制是什么行话：也就是我们所说的主从复制，主机数据更新后根据配置和策略，自动同步到备机的 master/slaver 机制，Master以写为主，Slave以读为主。 能干嘛读写分离、容灾恢复 怎么玩 配从(库)不配主(库) 从库配置：slaveof 主库IP 主库端口 每次与 master 断开之后，都需要重新连接，除非你配置进 redis.conf 文件 info replication 修改配置文件细节操作 拷贝多个 redis.conf 文件 开启 daemonize yes pid 文件名字 (pidfile) 指定端口 (port) log 文件名字 (logfile) dump.rdb 名字 (dbfilename) 常用技能 一主二仆 Init 一个 Master 两个 Slave 日志查看：主机日志、备机日志、info replication(查看状态) 一些问题 1 切入点问题？slave1、slave2 是从头开始复制还是从切入点开始复制?比如从 k4 进来，那之前的123是否也可以复制？（✔️） 2 从机是否可以写？set可否？（❌，读写分离呀，从机不可写。） 3 主机 shutdown 后情况如何？从机是上位还是原地待命？（从机原地待命。） 4 主机又回来了后，主机新增记录，从机还能否顺利复制？(可以，主机回来自动恢复) 5 其中一台从机 down 后情况如何？依照原有它能跟上大部队吗？(从机只要和 master 断开就要重连) 薪火相传 上一个 Slave 可以是下一个 Slave 的 Master，Slave 同样可以接收其他 Slaves 的连接和同步请求，那么该 Slave 作为了链条中下一个的 Master，可以有效减轻 Master 的写压力。 中途变更转向:会清除之前的数据，重新建立拷贝最新的。 slaveof 新主库IP 新主库端口 反客为主 SLAVEOF no one 使当前数据库停止与其他数据库的同步，转成主数据库 复制原理 slave 启动成功连接到 master 后会发送一个 sync 命令。 Master 接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master 将传送整个数据文件到 slave，以完成一次完全同步。 全量复制：而 slave 服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master 继续将新的所有收集到的修改命令依次传给 slave,完成同步。 但是只要是重新连接 master，一次完全同步（全量复制）将被自动执行。 哨兵模式 (sentinel)反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。一组 sentinel 能同时监控多个 Master。 使用步骤 调整结构，6379带着80、81 自定义的 /myredis 目录下新建 sentinel.conf 文件，名字绝不能错 配置哨兵，填写内容 sentinel monitor 被监控数据库名字(自己起名字) 127.0.0.1 6379 1 上面最后一个数字1，表示主机挂掉后 salve 投票看让谁接替成为主机，得票数多少后成为主机 启动哨兵 redis-sentinel /myredis/sentinel.conf 上述目录依照各自的实际情况配置，可能目录不同 正常主从演示，原有的master挂了，投票新选 重新主从继续开工，info replication查查看 问题：如果之前的 master 重启回来，会不会双 master 冲突？(❌，和反客为主不大一样，老 master 回来会变成新 master 的 slave) 复制的缺点复制延时：由于所有的写操作都是先在 Master 上操作，然后同步更新到 Slave 上，所以从 Master 同步到 Slave 机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave 机器数量的增加也会使这个问题更加严重。 九、Jedispackagecommons-pool-1.6.jar、jedis-2.1.0.jar 连通性测试返回 pong 则为连通 12345678910import redis.clients.jedis.Jedis;public class Main &#123; public static void main(String[] args) &#123; //连接本地的 Redis 服务 Jedis jedis = new Jedis("127.0.0.1",6379); // 查看服务是否运行，打出pong表示OK System.out.println("connection is OK==========&gt;: " + jedis.ping()); &#125;&#125; 日常使用123456789101112131415161718192021import java.util.Set;import redis.clients.jedis.Jedis;public class TestAPI &#123; public static void main(String[] args) &#123; Jedis jedis = new Jedis("127.0.0.1",6379); jedis.set("k1","v1"); jedis.set("k2","v2"); jedis.set("k3","v3"); System.out.println(jedis.get("k3")); Set&lt;String&gt; sets = jedis.keys("*"); System.out.println(sets.size()); //后续请参考脑图，家庭作业，敲一遍...... &#125;&#125; 事务加锁事务场景模拟 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import redis.clients.jedis.Jedis;import redis.clients.jedis.Transaction;public class TestTX &#123; public boolean transMethod() throws InterruptedException &#123; Jedis jedis = new Jedis("127.0.0.1", 6379); int balance;// 可用余额100元（set balance 100） int debt;// 欠额(set debt 0) int amtToSubtract = 10;// 实刷额度10元 jedis.watch("balance"); // jedis.set("balance","5"); // 此句不该出现，模拟其他程序已经修改了该条目 // 模拟系统停顿7秒，期间若有改动，则该程序能够监控到变化 Thread.sleep(7000); balance = Integer.parseInt(jedis.get("balance")); if (balance &lt; amtToSubtract) &#123; jedis.unwatch(); System.out.println("modify"); return false; &#125; else &#123; System.out.println("***********transaction"); Transaction transaction = jedis.multi(); transaction.decrBy("balance", amtToSubtract); transaction.incrBy("debt", amtToSubtract); transaction.exec(); balance = Integer.parseInt(jedis.get("balance")); debt = Integer.parseInt(jedis.get("debt")); System.out.println("*******" + balance); System.out.println("*******" + debt); return true; &#125; &#125; /** * 通俗点讲，watch命令就是标记一个键，如果标记了一个键， * 在提交事务前如果该键被别人修改过，那事务就会失败，这种情况通常可以在程序中 * 重新再尝试一次。 * 首先标记了键balance，然后检查余额是否足够，不足就取消标记，并不做扣减； * 足够的话，就启动事务进行更新操作， * 如果在此期间键balance被其它人修改， 那在提交事务（执行exec）时就会报错， * 程序中通常可以捕获这类错误再重新执行一次，直到成功。 * * @throws InterruptedException */ public static void main(String[] args) throws InterruptedException &#123; TestTX test = new TestTX(); boolean retValue = test.transMethod(); System.out.println("main retValue-------: " + retValue); &#125;&#125; 主从复制6379,6380启动，先各自先独立，主写从读。 123456789101112131415import redis.clients.jedis.Jedis;public class TestMS &#123; public static void main(String[] args) &#123; Jedis jedis_M = new Jedis("127.0.0.1",6379); Jedis jedis_S = new Jedis("127.0.0.1",6380); jedis_S.slaveof("127.0.0.1",6379); jedis_M.set("class","1122V2"); String result = jedis_S.get("class"); System.out.println(result); &#125;&#125; JedisPool获取 Jedis 实例需要从 JedisPool 中获取。 用完 Jedis 实例需要返还给 JedisPool。 如果 Jedis 在使用过程中出错，则也需要还给 JedisPool。 连接池（懒汉式单例）： 123456789101112131415161718192021222324252627282930313233343536373839import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class JedisPoolUtil &#123; private static volatile JedisPool jedisPool = null; private JedisPoolUtil()&#123;&#125; public static JedisPool getJedisPoolInstance() &#123; if(null == jedisPool) &#123; synchronized (JedisPoolUtil.class) &#123; if(null == jedisPool) &#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxActive(1000); poolConfig.setMaxIdle(32); poolConfig.setMaxWait(100*1000); poolConfig.setTestOnBorrow(true); jedisPool = new JedisPool(poolConfig,"127.0.0.1",6379); &#125; &#125; &#125; return jedisPool; &#125; public static void release(JedisPool jedisPool,Jedis jedis) &#123; if(null != jedis) &#123; jedisPool.returnResourceObject(jedis); &#125; &#125;&#125; 测试连接池： 12345678910111213141516171819202122import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;public class TestPool &#123; public static void main(String[] args) &#123; JedisPool jedisPool = JedisPoolUtil.getJedisPoolInstance(); JedisPool jedisPool2 = JedisPoolUtil.getJedisPoolInstance(); System.out.println(jedisPool == jedisPool2); Jedis jedis = null; try &#123; jedis = jedisPool.getResource(); jedis.set("aa","bb"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; JedisPoolUtil.release(jedisPool, jedis); &#125; &#125;&#125; 配置 JedisPool 的配置参数大部分是由 JedisPoolConfig 的对应项来赋值的。 maxActive：控制一个 pool 可分配多少个 jedis 实例，通过 pool.getResource() 来获取；如果赋值为 -1，则表示不限制；如果 pool 已经分配了 maxActive 个 jedis 实例，则此时 pool 的状态为 exhausted。 maxIdle：控制一个 pool 最多有多少个状态为 idle(空闲) 的 jedis 实例。 whenExhaustedAction：表示当 pool 中的 jedis 实例都被 allocated 完时，pool 要采取的操作，默认有三种： WHEN_EXHAUSTED_FAIL –&gt; 表示无jedis实例时，直接抛出 NoSuchElementException WHEN_EXHAUSTED_BLOCK –&gt; 则表示阻塞住，或者达到 maxWait 时抛出JedisConnectionException WHEN_EXHAUSTED_GROW –&gt; 则表示新建一个jedis实例，也就说设置的maxActive无用； maxWait：表示当 borrow 一个 jedis 实例时，最大的等待时间，如果超过等待时间，则直接抛 JedisConnectionException。 testOnBorrow：获得一个 jedis 实例的时候是否检查连接可用性（ping()）；如果为 true，则得到的 jedis 实例均是可用的。 testOnReturn：return 一个 jedis 实例给 pool 时，是否检查连接可用性（ping()）； testWhileIdle：如果为 true，表示有一个 idle object evitor 线程对 idle object 进行扫描，如果 validate 失败，此 object 会被从 pool 中 drop 掉；这一项只有在 timeBetweenEvictionRunsMillis 大于0时才有意义。 timeBetweenEvictionRunsMillis：表示 idle object evitor 两次扫描之间要 sleep 的毫秒数。 numTestsPerEvictionRun：表示 idle object evitor 每次扫描的最多的对象数。 minEvictableIdleTimeMillis：表示一个对象至少停留在 idle 状态的最短时间，然后才能被 idle object evitor 扫描并驱逐；这一项只有在 timeBetweenEvictionRunsMillis 大于0时才有意义。 softMinEvictableIdleTimeMillis：在 minEvictableIdleTimeMillis 基础上，加入了至少 minIdle 个对象已经在 pool 里面了。如果为 -1，evicted 不会根据 idle time 驱逐任何对象。如果 minEvictableIdleTimeMillis&gt;0，则此项设置无意义，且只有在 timeBetweenEvictionRunsMillis 大于0时才有意义； lifo：borrowObject 返回对象时，是采用 DEFAULT_LIFO（last in first out，即类似cache的最频繁使用队列），如果为 False，则表示 FIFO 队列； 其中 JedisPoolConfig 对一些参数的默认设置如下： 1234testWhileIdle = trueminEvictableIdleTimeMills = 60000timeBetweenEvictionRunsMillis = 30000numTestsPerEvictionRun = -1]]></content>
      <categories>
        <category>java</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP基础]]></title>
    <url>%2F2019%2F04%2F22%2FJSP%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[九大内置对象request 该对象代表了客户端的请求信息，主要用于接受通过 HTTP 协议传送到服务器的数据。 request 对象的作用域为一次请求。 response 该对象是对客户端的响应，主要是将 JSP 容器处理过的对象传回到客户端。response 对象的作用域只在 JSP 页面内有效。 session 会话 该对象是客服端和服务器的一次会话。服务器为每个用户都生成一个 session 对象，用于保存该用户的信息，跟踪用户的操作状态。直到客户端断开连接或者，session 时间到期后断开。 application 全局对象 该对象是存在于整个应用中，开始于服务器启动，结束于服务器关闭。这个对象中可以保存信息在应用任何地方都可以用。 out 输出对象 该对象是 JspWriter 类的实例,是向客户端浏览器输出内容经常要用到的对象 。就只能作用在当前页面。用完过后要及时清除缓冲区的内容，腾出空间，还要记得关闭输出流。 pageContext JSP页面容器 该对象可以获取当前 jsp 页面任何范围的参数。比如 out、request、reponse、session、application 等对象。 config 配置对象 该对象的作用就是当一个 Servlet 初始化时，容器（Tomcat、JRun、Resin等）把某些信息；服务器的一些信息等通过 config 对象传递给这个 Servlet。 page 当前JSP页面对象 该对象就是指向的当前 jsp 页面本身。 exception 异常对象 对象就是在页面运行过程中出了异常来显示异常信息的，但是必须在页面中设置 isErrorPage=”true” 才能使用。 四大作用域四大作用域范围从小到大 pageContext &lt; request &lt; session &lt; application 生命周期 pageContext：存在 page 中的变量，只作用于当前的 jsp 页面，当发生跳转、重定向、定时刷新时，将随之销毁。 request：存在 request 中的变量，作用于一次 HTTP 请求到服务器处理结束，返回响应的整个过程，该变量可以随着 forward 的方式跳转到多个 jsp 中，一但刷新页面，它们将重新计算。 session：存在 session 中的变量，作用于一次会话中，从打开浏览器到关闭浏览器过程中，将一直累加。（若想在再次打开浏览器时，变量仍然存在，则可以将 session 的 JSESSIONID 存到 cookie 中，在给 cookie 一个存活时间） application：存在 application 中的变量，作用于整个应用中，即从应用启动到应用结束，如果不进行手工删除，它们将一直可以使用，而且这些变量所有用户均可使用。 作用范围 pageContext：用户请求的当前页面。 request：用户请求访问的当前组件，以及和当前 web 组件共享同一用户请求的 web 组件。 session：同一个 Http 会话中的 web 组件共享。 application：整个 web 应用的所有 web 组件共享，即只要是同一个服务器下的均可使用。 常用方法out 方法 作用 println() 向客户端输出各种类型的数据 close() 关闭输出流 flush() 输出缓冲区数据 clearBuffer() 清除缓冲区数据，并且把数据输出到客户端 clear() 清除缓冲区数据，但是不把数据输出到客户端 request 方法 作用 getMethod() 返回客户端向服务器端传送数据的方法 getParameter(String paramName) 返回客户端传向服务器端传递的参数值 getParameterNames() 获得客户端传递给服务器端的所有参数的名字 getParameterValues(String name) 获得指定参数的所有值 getRequestURL() 获得发出请求字符串的客户端地址 getRemoteAddr() 获取客户端 IP 地址 getRemoteHost() 获取客户端机器名称 getServerName() 获取服务器名字 getSession() 获取session对象 getServerPort() 获取服务器端端口 response 方法 作用 getCharacterEncoding() 返回响应用的是何种字符编码 getOutputStream() 返回响应的一个二进制输出流 getWriter() 返回可以向客户端输出字符的一个对象 setContentLength(int len) 设置响应头长度 addCookie(Cookie cookie) 添加一个Cookie对象，用于在客户端保存特定的信息 sendRedirect(String url) 重新定向客户端的请求 sendError(int) 向客户端发送错误信息，int指服务器的错误码 addHeader(String name, String value) 添加HTTP头信息，该Header信息将发送到客户端 七大动作 动作 作用 jsp:include 在页面被请求的时候引入一个文件。 jsp:useBean 寻找或者实例化一个 JavaBean。 jsp:setProperty 设置 JavaBean 的属性。 jsp:getProperty 输出某个 JavaBean 的属性。 jsp:forward 把请求转到一个新的页面。 jsp:plugin 根据浏览器类型为 Java 插件生成 OBJECT 或 EMBED 标记。 jsp:element 定义动态 XML 元素 jsp:attribute 设置动态定义的 XML 元素属性。 jsp:body 设置动态定义的 XML 元素内容。 jsp:text 在 JSP 页面和文档中使用写入文本的模板 三大指令 指令 作用 &lt;%@ page … %&gt; 定义网页依赖属性，比如脚本语言、error页面、缓存需求等等 &lt;%@ include … %&gt; 包含其他文件 &lt;%@ taglib … %&gt; 引入标签库的定义]]></content>
      <categories>
        <category>java</category>
        <category>jsp</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jsp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP]]></title>
    <url>%2F2019%2F04%2F14%2FSpring-AOP%2F</url>
    <content type="text"><![CDATA[本文转载自 刘半仙 大神，这篇文章写得太好了，对 AOP 的原理介绍由浅入深。 为什么会有面向切面编程（AOP）？我们知道 Java 是一个面向对象（OOP）的语言，但它有一些弊端，比如当我们需要为多个不具有继承关系的对象引入一个公共行为，例如日志、权限验证、事务等功能时，只能在在每个对象里引用公共行为。这样做不便于维护，而且有大量重复代码。AOP 的出现弥补了 OOP 的这点不足。 为了阐述清楚 Spring AOP，我们将从下方面进行讨论： 代理模式 静态代理原理及实践 动态代理原理及实践 Spring AOP原理及实战 代理模式这段话比较官方，但我更倾向于用自己的语言理解：比如 A 对象要做一件事情，在没有代理前，自己来做；在对 A 代理后，由 A 的代理类 B 来做。代理其实是在原实例前后加了一层处理，这也是 AOP 的初级轮廓。代理模式：为其他对象提供一种代理以控制对这个对象的访问。 静态代理原理及实践静态代理模式：静态代理说白了，就是在程序运行前就已经存在代理类的字节码文件、代理类和原始类的关系在运行前就已经确定。废话不多说，我们看一下代码。为了方便阅读，博主把单独的 class 文件合并到接口中，读者可以直接复制代码运行： 12345678910111213141516171819202122232425262728293031323334353637383940414243package test.staticProxy; // 接口public interface IUserDao &#123; void save(); void find();&#125; //目标对象class UserDao implements IUserDao&#123; @Override public void save() &#123; System.out.println("模拟：保存用户！"); &#125; @Override public void find() &#123; System.out.println("模拟：查询用户"); &#125;&#125; /** * 静态代理 * 特点： * 2. 目标对象必须要实现接口 * 2. 代理对象，要实现与目标对象一样的接口 */class UserDaoProxy implements IUserDao&#123; // 代理对象，需要维护一个目标对象 private IUserDao target = new UserDao(); @Override public void save() &#123; System.out.println("代理操作： 开启事务..."); target.save(); // 执行目标对象的方法 System.out.println("代理操作：提交事务..."); &#125; @Override public void find() &#123; target.find(); &#125;&#125; 测试结果： 静态代理虽然保证了业务类只需关注逻辑本身，代理对象的一个接口只服务于一种类型的对象。如果要代理的方法很多，势必要为每一种方法都进行代理。再者，如果增加一个方法，除了实现类需要实现这个方法外，所有的代理类也要实现此方法。增加了代码的维护成本。那么要如何解决呢？答案是使用动态代理。 动态代理原理及实践动态代理模式：动态代理类的源码是在程序运行期间，通过 JVM 反射等机制动态生成。代理类和委托类的关系是运行时才确定的。实例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package test.dynamicProxy; import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy; // 接口public interface IUserDao &#123; void save(); void find();&#125; //目标对象class UserDao implements IUserDao&#123; @Override public void save() &#123; System.out.println("模拟： 保存用户！"); &#125; @Override public void find() &#123; System.out.println("查询"); &#125;&#125; /** * 动态代理： * 代理工厂，给多个目标对象生成代理对象！ * */class ProxyFactory &#123; // 接收一个目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; // 返回对目标对象(target)代理后的对象(proxy) public Object getProxyInstance() &#123; Object proxy = Proxy.newProxyInstance( target.getClass().getClassLoader(), // 目标对象使用的类加载器 target.getClass().getInterfaces(), // 目标对象实现的所有接口 new InvocationHandler() &#123; // 执行代理对象方法时候触发 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; // 获取当前执行的方法的方法名 String methodName = method.getName(); // 方法返回值 Object result = null; if ("find".equals(methodName)) &#123; // 直接调用目标对象方法 result = method.invoke(target, args); &#125; else &#123; System.out.println("开启事务..."); // 执行目标对象方法 result = method.invoke(target, args); System.out.println("提交事务..."); &#125; return result; &#125; &#125; ); return proxy; &#125;&#125; 测试结果： 在运行测试类中创建测试类对象代码中 1IUserDao proxy = (IUserDao)new ProxyFactory(target).getProxyInstance(); 其实是JDK动态生成了一个类去实现接口,隐藏了这个过程: 1class $jdkProxy implements IUserDao&#123;&#125; 使用 jdk 生成的动态代理的前提是目标类必须有实现的接口。但这里又引入一个问题,如果某个类没有实现接口,就不能使用 JDK 动态代理,所以 Cglib 代理就是解决这个问题的。 Cglib 是以动态生成的子类继承目标的方式实现，在运行期动态的在内存中构建一个子类，如下: 123public class UserDao&#123;&#125;//Cglib是以动态生成的子类继承目标的方式实现,程序执行时,隐藏了下面的过程public class $Cglib_Proxy_class extends UserDao&#123;&#125; Cglib 使用的前提是目标类不能为 final 修饰。因为final修饰的类不能被继承。 现在，我们可以看看 AOP 的定义：面向切面编程，核心原理：使用动态代理模式在方法执行前后或出现异常时加入相关逻辑。 ​ 通过定义和前面代码我们可以发现3点： ​ 1.AOP 是基于动态代理模式。 ​ 2.AOP 是方法级别的。 ​ 3.AOP 可以分离业务代码和关注点代码（重复代码），在执行业务代码时，动态的注入关注点代码。切面就是关注点代码形成的类。 Spring AOP原理及实战前文提到 JDK 代理和 Cglib 代理两种动态代理，优秀的 Spring 框架把两种方式在底层都集成了进去,我们无需担心自己去实现动态生成代理。那么，Spring 是如何生成代理对象的？ 创建容器对象的时候，根据切入点表达式拦截的类，生成代理对象。 如果目标对象有实现接口，使用 jdk 代理。如果目标对象没有实现接口，则使用 Cglib 代理。然后从容器获取代理后的对象，在运行期植入”切面”类的方法。通过查看 Spring 源码，我们在 DefaultAopProxyFactory 类中，找到这样一段话。 简单的从字面意思看出,如果有接口,则使用 Jdk 代理,反之使用 Cglib，这刚好印证了前文所阐述的内容。Spring AOP 综合两种代理方式的使用前提有会如下结论：如果目标类没有实现接口，且 class 为 final 修饰的，则不能进行 Spring AOP 编程！ 知道了原理，现在我们将自己手动实现 Spring 的 AOP： 1234567891011121314151617181920212223242526272829303132333435363738394041package test.spring_aop_anno;import org.aspectj.lang.ProceedingJoinPoint;public interface IUserDao &#123; void save();&#125;//用于测试Cglib动态代理class OrderDao &#123; public void save() &#123; //int i =1/0;用于测试异常通知 System.out.println("保存订单..."); &#125;&#125;//用于测试jdk动态代理class UserDao implements IUserDao &#123; public void save() &#123; //int i =1/0;用于测试异常通知 System.out.println("保存用户..."); &#125;&#125;//切面类class TransactionAop &#123; public void beginTransaction() &#123; System.out.println("[前置通知] 开启事务.."); &#125; public void commit() &#123; System.out.println("[后置通知] 提交事务.."); &#125; public void afterReturing()&#123; System.out.println("[返回后通知]"); &#125; public void afterThrowing()&#123; System.out.println("[异常通知]"); &#125; public void arroud(ProceedingJoinPoint pjp) throws Throwable&#123; System.out.println("[环绕前：]"); pjp.proceed(); // 执行目标方法 System.out.println("[环绕后：]"); &#125;&#125; Spring 的 xml 配置文件: 12345678910111213141516171819202122232425262728293031323334353637383940&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd"&gt; &lt;!-- dao实例加入容器 --&gt; &lt;bean id="userDao" class="test.spring_aop_anno.UserDao"&gt;&lt;/bean&gt; &lt;!-- dao实例加入容器 --&gt; &lt;bean id="orderDao" class="test.spring_aop_anno.OrderDao"&gt;&lt;/bean&gt; &lt;!-- 实例化切面类 --&gt; &lt;bean id="transactionAop" class="test.spring_aop_anno.TransactionAop"&gt;&lt;/bean&gt; &lt;!-- Aop相关配置 --&gt; &lt;aop:config&gt; &lt;!-- 切入点表达式定义 --&gt; &lt;aop:pointcut expression="execution(* test.spring_aop_anno.*Dao.*(..))" id="transactionPointcut"/&gt; &lt;!-- 切面配置 --&gt; &lt;aop:aspect ref="transactionAop"&gt; &lt;!-- 【环绕通知】 --&gt; &lt;aop:around method="arroud" pointcut-ref="transactionPointcut"/&gt; &lt;!-- 【前置通知】 在目标方法之前执行 --&gt; &lt;aop:before method="beginTransaction" pointcut-ref="transactionPointcut" /&gt; &lt;!-- 【后置通知】 --&gt; &lt;aop:after method="commit" pointcut-ref="transactionPointcut"/&gt; &lt;!-- 【返回后通知】 --&gt; &lt;aop:after-returning method="afterReturing" pointcut-ref="transactionPointcut"/&gt; &lt;!-- 异常通知 --&gt; &lt;aop:after-throwing method="afterThrowing" pointcut-ref="transactionPointcut"/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 代码的测试结果如下: 到这里,我们已经全部介绍完Spring AOP，我们拿它做什么？ Spring声明式事务管理配置 Controller层的参数校验 使用Spring AOP实现MySQL数据库读写分离案例分析 在执行方法前,判断是否具有权限 对部分函数的调用进行日志记录。监控部分重要函数，若抛出指定的异常，可以以短信或邮件方式通知相关人员 信息过滤，页面转发等等功能]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊聊HashMap]]></title>
    <url>%2F2019%2F04%2F14%2F%E8%81%8A%E8%81%8AHashMap%2F</url>
    <content type="text"><![CDATA[HashMap是什么？ HashMap 是一个散列桶（1.7是数组+链表，1.8是数组+链表+红黑树），它存储的内容是键值对 key-value 映射。 HashMap 采用了数组和链表的数据结构，能在查询和修改方便继承了数组的线性查找和链表的寻址修改。 HashMap 是非 synchronized，所以 HashMap 很快。 HashMap 可以接受 null 键和值，而 Hashtable 则不能（原因就是 equlas() 方法需要对象，因为 HashMap 是后出的 API 经过处理才可以）。如果键为null，那么调用hash()方法得到的都将是0，所以键为 null 的元素都始终位于哈希表 table[0] 中。 HashMap在JDK1.7和1.8中的区别 底层数据结构不一样，1.7是数组+链表，1.8则是数组+链表+红黑树结构（当链表长度大于8，转为红黑树）。 JDK1.7 用的是头插法，而JDK1.8及之后使用的都是尾插法，那么他们为什么要这样做呢？ 因为 JDK1.7 是用单链表进行的纵向延伸，当采用头插法时会容易出现逆序且环形链表死循环问题。但是在 JDK1.8 之后是因为加入了红黑树使用尾插法，能够避免出现逆序且链表死循环的问题。 1.8 rehash 时保证原链表的顺序，而1.7中 rehash 时有可能改变链表的顺序（头插法导致）。 在扩容的时候：1.7在插入数据之前扩容，而1.8插入数据成功之后扩容。 1.7计算 hash 运算多，1.8计算 hash 运算少 。 1.7受 rehash 影响，1.8调整后是 (原位置) or (原位置+旧容量)。 HashMap工作原理HashMap 是基于 hashing 的原理，通过 put(key, value) 存储对象到 HashMap 中，使用 get(key) 从 HashMap 中获取对象。当我们给 put() 方法传递键和值时，我们先对键调用 hashCode() 方法，计算并返回的 hashCode 是用于找到 Map 数组的 bucket 位置来储存 Node 对象。这里关键点在于，HashMap 是在 bucket 中储存键对象和值对象，作为Map.Node 。 具体 put 过程（JDK1.8） 对 Key 求 Hash 值，然后再计算下标 如果没有碰撞，直接放入桶中（碰撞的意思是计算得到的 Hash 值相同，需要放到同一个 bucket 中） 如果碰撞了，以链表的方式链接到后面 如果链表长度超过阀值（TREEIFY THRESHOLD==8），就把链表转成红黑树，链表长度低于6，就把红黑树转回链表 如果节点已经存在就替换旧值 如果桶满了（容量16*加载因子0.75），就需要 resize（扩容2倍后重排） 具体 get 过程考虑特殊情况：如果两个键的 hashcode 相同，你如何获取值对象？ 当我们调用 get() 方法，HashMap 会使用键对象的 hashcode 找到 bucket 位置，找到 bucket 位置之后，会调用 keys.equals() 方法去找到链表中正确的节点，最终找到要找的值对象。 有什么方法可以减少碰撞？扰动函数可以减少碰撞原理是如果两个不相等的对象返回不同的 hashcode 的话，那么碰撞的几率就会小些。这就意味着链表结构减小，这样取值的话就不会频繁调用 equal 方法，从而提高 HashMap 的性能（扰动即 Hash 方法内部的算法实现，目的是让不同对象返回不同hashcode）。 使用不可变的、声明作 final 对象，并且采用合适的 equals() 和 hashCode() 方法，将会减少碰撞的发生不可变性使得能够缓存不同键的 hashcode，这将提高整个获取对象的速度，使用 String、Integer 这样的 wrapper 类作为键是非常好的选择。 为什么 String、Integer 这样的 wrapper 类适合作为键？因为 String 是 final，而且已经重写了 equals() 和 hashCode() 方法了。不可变性是必要的，因为为了要计算 hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的 hashcode 的话，那么就不能从 HashMap 中找到你想要的对象。 HashMap 中 hash 函数怎么是实现的?在 hashmap 中要找到某个元素，需要根据 key 的 hash 值来求得对应数组中的位置。如何计算这个位置就是 hash 算法。我们一般对哈希表的散列很自然地会想到用 hash 值对 length 取模（即除法散列法），Hashtable 中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap 中则通过 h&amp;(length-1) 的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是 HashMap 对 Hashtable 的一个改进。这也是为什么哈希表的容量一定要是2的整数次幂，length 为2的整数次幂的话，h&amp;(length-1) 就相当于对 length 取模，这样便保证了散列的均匀，同时也提升了效率。 拉链法导致的链表过深，为什么不用二叉查找树代替而选择红黑树？为什么不一直使用红黑树？引入红黑树就是为了查找数据快，解决链表查询深度的问题。我们知道红黑树属于平衡二叉树，为了保持“平衡”是需要付出代价的，但是该代价所损耗的资源要比遍历线性链表要少。所以当长度大于8的时候，会使用红黑树；如果链表长度很短的话，根本不需要引入红黑树，引入反而会慢。 红黑树 每个节点非红即黑 根节点总是黑色的 如果节点是红色的，则它的子节点必须是黑色的（反之不一定） 每个叶子节点都是黑色的空节点（NIL节点） 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度） HashMap 与 HashTable 区别 默认容量不同，扩容不同(HashMap 默认16，扩容 2 倍；HashTable 默认11，扩容 2 倍 + 1) 线程安全性：HashTable 安全 效率不同：HashTable 要慢，因为加锁 HashMap 可以接受为 null 的键值 (key) 和值 (value)，而 Hashtable 则不行 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的 Hashtable 计算 hash 值，直接用 key 的 hashCode()，而 HashMap 重新计算了 key 的 hash 值(hash 函数) 迭代器：HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器，而 Hashtable 的 enumerator 迭代器不是 fail-fast 的。所以当有其它线程改变了 HashMap 的结构（增加或者移除元素），将会抛出 ConcurrentModificationException，但迭代器本身的 remove() 方法移除元素则不会抛出 ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看 JVM。这条同样也是 Enumeration 和 Iterator 的区别。 CocurrentHashMapJDK1.7 CocurrentHashMap 是由 Segment 数组和 HashEntry 数组和链表组成 Segment 是基于重入锁（ReentrantLock）：一个数据段竞争锁。每个 HashEntry 一个链表结构的元素，利用 Hash 算法得到索引确定归属的数据段，也就是对应到在修改时需要竞争获取的锁。ConcurrentHashMap 支持 CurrencyLevel（Segment 数组数量）的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment 核心数据如 value，以及链表都是 volatile 修饰的，保证了获取时的可见性 首先是通过 key 定位到 Segment，之后在对应的 Segment 中进行具体的 put 操作如下： 将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。 遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value 不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容 最后会解除在 1 中所获取当前 Segment 的锁。 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理 首先第一步的时候会尝试获取锁，如果获取失败肯定就有其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。 尝试自旋获取锁 如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。最后解除当前 Segment 的锁 JDK1.8CocurrentHashMap 抛弃了原有的 Segment 分段锁，采用了 CAS + synchronized 来保证并发安全性。其中的 val next 都用了 volatile 修饰，保证了可见性。 最大特点是引入了 CAS 借助 Unsafe 来实现 native code。CAS有3个操作数，内存值 V、旧的预期值 A、要修改的新值 B。当且仅当预期值 A 和内存值 V 相同时，将内存值V修改为 B，否则什么都不做。Unsafe 借助 CPU 指令 cmpxchg 来实现。 CAS 使用实例 对 sizeCtl 的控制都是用 CAS 来实现的： -1 代表 table 正在初始化 N 表示有 -N-1 个线程正在进行扩容操作 如果 table 未初始化，表示table需要初始化的大小 如果 table 初始化完成，表示table的容量，默认是table大小的0.75倍，用这个公式算 0.75（n – (n &gt;&gt;&gt; 2)） CAS 会出现的问题：ABA 解决：对变量增加一个版本号，每次修改，版本号加 1，比较的时候比较版本号。 put 过程 根据 key 计算出 hashcode 判断是否需要进行初始化 通过 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功 如果当前位置的 hashcode == MOVED == -1,则需要进行扩容 如果都不满足，则利用 synchronized 锁写入数据 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树 get 过程 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值 如果是红黑树那就按照树的方式获取值 就不满足那就按照链表的方式遍历获取值 Addition ConcurrentHashMap 在 Java 8 中存在一个 bug 会进入死循环，原因是递归创建 ConcurrentHashMap 对象，但是在 JDK 1.9 已经修复了。 12345678910111213141516171819public class ConcurrentHashMapDemo&#123; private Map&lt;Integer,Integer&gt; cache =new ConcurrentHashMap&lt;&gt;(15); public static void main(String[]args)&#123; ConcurrentHashMapDemo ch = new ConcurrentHashMapDemo(); System.out.println(ch.fibonaacci(80)); &#125; public int fibonaacci(Integer i)&#123; if(i==0||i ==1) &#123; return i; &#125; return cache.computeIfAbsent(i,(key) -&gt; &#123; System.out.println("fibonaacci : "+key); return fibonaacci(key -1)+fibonaacci(key - 2); &#125;); &#125;&#125; TreeMap 和 HashMap 的比较 TreeMap 是根据 key 进行排序的，它的排序和定位需要依赖比较器或覆写 Comparable 接口，也因此不需要 key 覆写 hashCode 方法和 equals 方法，就可以排除掉重复的 key，而 HashMap 的 key 则需要通过覆写 hashCode 方法和 equals 方法来确保没有重复的 key。 TreeMap 的查询、插入、删除效率均没有 HashMap 高，一般只有要对 key 排序时才使用 TreeMap。 TreeMap 的 key 不能为 null，而 HashMap 的 key 可以为 null。 LinkedHashMap 和 HashMap 的比较LinkedHashMap 是 HashMap 的一个子类，保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用设计模式简单总结]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Singleton 单例模式 Simple Factory 简单工厂模式 Proxy 代理模式 Decorator 装饰模式 Adapter 适配器模式 Strategy 策略模式 Observer 观察者模式 创建型模式（五种）：单例模式、工厂方法模式、抽象工厂模式、建造者模式、原型模式结构型模式（七种）：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式行为型模式（十一种）：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 Singleton 单例模式一个类在 Java 虚拟机中只有一个对象，并提供一个全局访问点。 生活中例子：太阳、月亮。 项目里面怎么用：数据库连接对象，属性配置文件的读取对象。 懒汉式-线程不安全以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。 这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。 1234567891011121314public class Singleton &#123; private static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; 饿汉式-线程安全线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。 但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。 1234567891011121314//饿汉式单例类.在类初始化时，已经自行实例化 public class EagerSingleton &#123; private static EagerSingleton instance = new EagerSingleton(); /** * 构造方法私有化 */ private EagerSingleton()&#123;&#125; /** * 静态工厂方法 */ public static EagerSingleton getInstance()&#123; return instance; &#125;&#125; Simple Factory 简单工厂模式用一个方法来代替new关键字。 在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。 项目里面怎么用：对于经常生成的对象，或者父子类替换的对象。spring的核心就是工厂模式。 123456789public class UserFactory &#123; public static User createUser(int i)&#123; //如果输入的是1，就创建它的子类，否则就创建父类 if(i==1)&#123; return new Alices(); &#125; return new User(); &#125; &#125; Proxy 代理模式为其他对象提供一个代理，以控制对当前对象的访问。 项目里面怎么用：权限，或者大对象的访问权限。 框架里面使用：Spring里面的AOP实现。 实现：设置一个顶级接口，被代理类和代理类都实现该接口。代理类里实例化被代理类，并且控制外界对被代理类对象的访问。 Decorator 装饰模式为对象动态添加功能。 应用场景：IO 流包装、 数据源包装、 简历包装 Adapter 适配器模式把一个类接口转换成另一个用户需要的接口。 生活中的例子：转换插头 框架里面使用：Spring AOP 模块对 BeforeAdvice、 AfterAdvice、 ThrowsAdvice 三种通知类型的支持实际上是借助适配器模式来实现的 java.util.Collections#list() Strategy 策略模式定义一系列算法并可以互相替换。 生活中的例子：图片的格式，压缩文件的格式。 项目里面怎么用：购物车里面的付款方式。 实现：设置一个顶级接口，各子类分别在实现接口方法时重写该方法。 Observer 观察者模式定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。 应用场景：监听器、日志收集、短信通知、邮件通知 javax.servlet.http.HttpSessionBindingListener]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机]]></title>
    <url>%2F2019%2F04%2F06%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[整理自 CyC2018 大神的笔记，以及周志明老师的《深入理解 Java 虚拟机》 一、Java 内存区域与内存溢出异常 运行时数据区域 程序计数器 Java 虚拟机栈 Java 堆 方法区 运行时常量池 直接内存 HotSpot 虚拟机对象探秘 创建对象 对象的内存布局 对象的访问定位 OutOfMemoryError 内存溢出异常(OOM)) Java 堆溢出 虚拟机栈和本地方法栈溢出 方法区和运行时常量池溢出 本机内存直接溢出 PS：StackOverflowError异常 二、垃圾收集器 判断一个对象是否可被回收 1. 引用计数算法 2. 可达性分析算法 3. 方法区的回收 4. finalize() 引用类型 垃圾收集算法 1.标记 - 清除算法(Mark - Sweep) 2.标记 - 整理算法(Mark - Compact) 3.复制算法(Copying) 4.分代收集算法(Generational Collection) 垃圾收集器 1. Serial 收集器 2. ParNew 收集器 3. Parallel Scavenge 收集器 4. Serial Old 收集器 5. Parallel Old 收集器 6. CMS 收集器 7. G1 收集器 三、内存分配和回收策略 Minor GC 和 Full GC 内存分配策略 1. 对象优先在 Eden 分配 2. 大对象直接进入老年代 3. 长期存活的对象进入老年代 4. 动态对象年龄判定 5. 空间分配担保 Full GC 的触发条件 1. 调用 System.gc() 2. 老年代空间不足 3. 空间分配担保失败 4. JDK 1.7 及以前的永久代空间不足 5. 空间分配担保 四、类加载机制 类的生命周期 类加载过程 1. 加载 2. 验证 3. 准备 4. 解析 5. 初始化 类初始化时机 1. 主动引用 2. 被动引用 类与类加载器 类加载器分类 双亲委派模型 1. 工作过程 2. 好处 3. 实现 自定义类加载器实现 五、Java 内存模型与线程 硬件与效率的一致性 Java 内存模型 主内存与工作内存 内存间交互操作 内存模型三大特性 先行发生原则 Java 与线程 线程的实现 Java 线程调度 状态转换 六、线程安全 五类各种操作共享的数据 不可变 绝对线程安全 相对线程安全 线程兼容 线程对立 线程安全的实现 1. 不可变 2. 互斥同步 3. 非阻塞同步 4. 无同步方案 七、锁优化 自旋锁 锁消除 锁粗化 轻量级锁 偏向锁 一、Java 内存区域与内存溢出异常运行时数据区域 程序计数器程序计数器 (Program Counter Register) 是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。记录正在执行的虚拟机字节码指令的地址（如果正在执行的是本地方法则为空）。 Java 虚拟机栈虚拟机栈描述的是 Java 方法执行的内存模型：每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。可以通过 -Xss 这个虚拟机参数来指定每个线程的 Java 虚拟机栈内存大小： 1java -Xss512M HackTheJava 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。 本地方法栈本地方法栈 (Native Method Stack) 与虚拟机栈作用类似，只不过虚拟机栈为虚拟机提供执行 Java 方法(也就是字节码)的服务，而本地方法栈为虚拟机使用到的 Native 方法提供服务。 本地方法一般是用其它语言（C、C++ 或汇编语言等）编写的，并且被编译为基于本机硬件和操作系统的程序，对待这些方法需要特别处理。下图中 JNI 为 Java Native Interface。 该区域可能抛出以下异常： StackOverflowError 异常和 OutOfMemoryError 异常。 Java 堆Java 堆 (Java Heap) 是 Java 虚拟机所管理的内存中最大的一块，是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称为 “GC堆” (Garbage Collected Heap)。 现代的垃圾收集器基本都是采用分代收集算法，其主要的思想是针对不同类型的对象采取不同的垃圾回收算法。可以将堆分成两块： 新生代（Young Generation） 老年代（Old Generation） 堆不需要连续内存空间，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 可以通过 -Xms 和 -Xmx 这两个虚拟机参数来指定一个程序的堆内存大小，第一个参数设置初始值，第二个参数设置最大值。 1java -Xms1M -Xmx2M HackTheJava 方法区方法区 (Method Area) 与 Java 堆一样，是被各个线程所共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 和堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。 对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。 HotSpot 虚拟机把它当成永久代来进行垃圾回收。但很难确定永久代的大小，因为它受到很多因素影响，并且每次 Full GC 之后永久代的大小都会改变，所以经常会抛出 OutOfMemoryError 异常。为了更容易管理方法区，从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。 运行时常量池运行时常量池 (Runtime Constant Pool) 是方法区的一部分。 Class 文件中的常量池（编译器生成的字面量和符号引用）会在类加载后被放入这个区域。 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。 受限于方法区，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 直接内存直接内存 (Direct Memory) 并非虚拟机运行时数据区的一部分，在 JDK 1.4 中新引入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在堆内存和堆外内存来回拷贝数据。 HotSpot 虚拟机对象探秘创建对象 虚拟机遇到一条 new 指令时，首先检查这个指令参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化，如果没有将先进行该类的加载。 为新生对象分配内存，对象所占内存的大小其实在类加载完成后就可以确定的。这里相当于把一块确定大小的内存从 Java 堆划分出来，分配方式大致可分为：指针碰撞 (Bump the Pointer) 和空闲列表 (Free List) 两种。 接下来虚拟机要对对象进行必要的设置，如该对象是哪个类的实例、如何才能找到该类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息，这些信息全部存在于对象头中（Object Header）。 执行 init 方法，即对象初始化。经过上面几步之后一个新的对象已经产生了，但所有字段还都为零，执行 init 方法是为了将对象按照程序员的意愿进行初始化，这样一个真正的可用对象才算完全生产出来。 对象的内存布局在 HotSpot 虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）和对齐信息（Padding）。 对象头对象头分为两个部分 第一部分：Mark Word 用于存储对象自身的运行时数据，如哈希码、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。 Mark Word数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为 32bit 和 64bit。 GC分代年龄HotSpot使用分代垃圾回收机制，被分为三个代：年轻代（Young Generation）、年老代（Old Generation）和永久代（Permanent Generation）。(注意：Java8 去除了持久代) 锁状态标志Mark Word 的最后 2bit 是锁状态标志位，用来标记当前对象的状态。对象的所处的状态决定了 Mark Word 存储的内容，如下表所示: 存储内容 标志位 状态 对象哈希码、对象分代年龄 01 未锁定 指向锁记录的指针 00 轻量级锁定 执行重量级锁定的指针 10 膨胀(重量级锁定) 空(不需要记录信息) 11 GC标记 偏向线程ID、偏向时间戳、对象分代年龄 01 可偏向 第二部分：类型指针 对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据实例数据是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。 对齐补充不必然存在，也没有特别的含义，仅仅起了占位符的作用。HotSpot VM 规定对象的大小必须是8字节的整数倍，当对象实例数据没有对齐时，需要对齐填充补全。 对象的访问定位目前主流的访问方式有使用句柄和直接指针两种。 通过句柄访问对象 通过直接指针访问对象 OutOfMemoryError 内存溢出异常(OOM)Java 堆溢出原因：Java堆用于储存对象实例，只要不断的创建对象，并且保证 GC Roots 到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象占用空间到最大堆的容量限制后就会产生 OOM。 溢出判断：异常栈信息 “java.lang.OutOfMemoryError” 后面会提示 “Java heap space” 。 处理方法： 通过参数 -XX:+HeapDumpOnOutOfMemoryError，可以让虚拟机在出现内存溢出时 Dump 出当前的内存堆转储快照。 用内存映像分析工具（如：Eclipse Memory Analyzer）对Dump出来的堆转储快照进行分析。找出原因是内存泄漏（Memory Leak）还是内存溢出（Memory Overflow）。 如为内存泄漏（Memory Leak），则通过工具查看对象到 GC Roots 的引用链，找出泄漏对象是通过怎样的路径与 GC Roots 相关联并导致垃圾收集器无法自动回收。掌握了泄漏对象的类型信息及 GC Roots 引用链的信息后，就可以比较准确的定位出泄漏代码的位置，进行修改。 如为内存溢出（Memory Overflow），则说明内存中的对象都是有用的对象，必须存活着。改进方法： 根据机器物理内存调整虚拟机堆参数(-Xmx(堆最大值) 和 -Xms(最小值))，来扩大堆内存。 检查代码中是否存在某些对象生命周期过长、持有状态时间过长等情况，减少程序运行期的内存消耗。 虚拟机栈和本地方法栈溢出原因：建立过多线程导致内存溢出 OOM。操作系统分配给每个进程的内存空间是有限的，每个线程分配到的栈容量越大，可以建立的线程数就越少，建立线程时容易把空间耗尽，出现 OOM。分配给虚拟机栈和本地方法栈的内存空间是从剩余空间中瓜分的。剩余空间 = 机器总内存 - Java最大容量堆(Xmx) - 最大方法区容量(MaxPermSize) - 程序计数器消耗(很小) - 虚拟机进程本身消耗。 PS：虚拟机只提供了用户对 Java 堆和方法区这两块内存区域的管理方法（提供相应参数调整内存大小），其他的内存区域由操作系统管理。 溢出判断：异常栈信息 “java.lang.OutOfMemoryError” 后面跟着会提供具体原因，比如：unable to create new native thread。 处理方法：在不能减少线程数和更换64位虚拟机的情况下，只能通过减小 Java 堆的最大堆容量和方法区的容量来换取更多的线程可利用的空间-虚拟机栈。这种方式称之为“减少内存”。 方法区和运行时常量池溢出原因： 运行时常量池溢出，比如使用 List 保持对常量池引用，避免 FullGC 回收常量池，导致常量池 OOM。 方法区用于存放 Class 的相关信息，运行时当大量的类需要加载，把方法区填满后，就会发生 OOM。 溢出判断：抛 OutOfMemoryError 异常，后面提示 PermGen space。 方法区填满溢出场景： a、主流框架 Spring、Hibernate (经常动态生成大量 Class 的应用)在对类进行增强时，会使用 GCLib 这类字节码增强技术，增强的类越多，就需要越大的方法区来保证动态生成的 Class 可以加载入内存。PS：CGLib 对类增强：运行中以该类作为父类生成该类的动态代理子类，动态代理子类中可以添加方法达到增强类的效果。 b、运行在 JVM 上的动态语言(如 Groovy)、大量 JSP 或者动态产生 JSP 文件的应用(JSP 第一次运行时需要编译为 Java 类)、基于 OSGi 的应用(即使是同一类文件，被不同的加载器加载也会视为不同的类)。 处理方法：经常动态生成大量 Class 的应用中，应该特别注意类的回收，由于一个类是否要被垃圾回收器回收掉，判定条件比较苛刻，所以方法区溢出是一种常见的内存溢出异常。 本机内存直接溢出DirectMemory 容量可通过 -XX:MaxDirectMemorySize 指定，如果不指定，则默认与 Java 堆的最大值 (-Xmx) 一样。由 DirectMemory 导致的内存溢出不会在 Heap Dump 中看到明显异常。如果发现 OOM 之后 Dump 文件很小，而程序中又直接或间接使用了 NIO，则可以考虑一下这个溢出情况。 PS：StackOverflowError异常原因：在单线程的情况下，当栈帧太大或者虚拟机栈容量太小，持续调用方法，向栈中放入栈帧导致线程所请求的栈深度大于虚拟机允许的最大栈深度时，虚拟机抛出 StackOverflowError。 解决方案：出现 StackOverflowError 异常时有错误堆栈可以阅读，直接查看抛出的异常信息，能够比较容易找到问题所在。 PS：虚拟机的默认设置下，栈深度在大多数情况下达到1000-2000万完全没有问题，对于正常的方法调用（包括递归），这个深度是够用的。 二、垃圾收集器垃圾收集主要是针对堆和方法区进行。程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后就会消失，因此不需要对这三个区域进行垃圾回收。 判断一个对象是否可被回收1. 引用计数算法为对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。 在两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。正是因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。 2. 可达性分析算法 以 GC Roots 为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收。 Java 虚拟机使用该算法来判断对象是否可被回收，GC Roots 一般包含以下内容： 虚拟机栈(栈帧中的本地变量表)中引用的对象 本地方法栈中 JNI (即一般说的 Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 3. 方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代低很多，所以在方法区上进行回收性价比不高。 主要是对常量池的回收和对类的卸载，回收废弃常量和无用的类。 为了避免内存溢出，在大量使用反射和动态代理的场景都需要虚拟机具备类卸载功能。 类的卸载条件很多，需要满足以下三个条件，并且满足了条件也不一定会被卸载： 该类所有的实例都已经被回收，此时堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 4. finalize()类似 C++ 的析构函数，用于关闭外部资源。但是 try-finally 等方式可以做得更好，并且该方法运行代价很高，不确定性大，无法保证各个对象的调用顺序，因此最好不要使用。 当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能在该方法中让对象重新被引用，从而实现自救。自救只能进行一次，如果回收的对象之前调用了 finalize() 方法自救，后面回收时不会再调用该方法。 引用类型无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。 Java 提供了四种强度不同的引用类型。 强引用只要强引用还存在，被强引用关联的对象就不会被回收，使用 new 一个新对象的方式来创建强引用。 1Object obj = new Object(); 软引用被软引用关联的对象只有在内存不够的情况下才会被回收。使用 SoftReference 类来创建软引用。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。使用 WeakReference 类来创建弱引用。 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 虚引用又称为幽灵引用或者幻影引用，一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象。为一个对象设置虚引用的唯一目的是能在这个对象被回收时收到一个系统通知。 使用 PhantomReference 来创建虚引用。 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj, null);obj = null; 垃圾收集算法1.标记 - 清除算法(Mark - Sweep) 标记要回收的对象，然后清除。 不足： 效率问题：标记和清除过程效率都不高； 空间问题：会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2.标记 - 整理算法(Mark - Compact) 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 优点：不会产生内存碎片 不足：需要移动大量对象，处理效率比较低。 3.复制算法(Copying) 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 现在的商业虚拟机都采用这种收集算法回收新生代，但是并不是划分为大小相等的两块，而是一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象全部复制到另一块 Survivor 上，最后清理 Eden 和使用过的那一块 Survivor。 HotSpot 虚拟机的 Eden 和 Survivor 大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 就不够用了，此时需要依赖于老年代进行空间分配担保，也就是借用老年代的空间存储放不下的对象。 4.分代收集算法(Generational Collection)现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 算法 垃圾收集器 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 单线程与多线程：单线程指的是垃圾收集器只使用一个线程，而多线程使用多个线程； 串行与并行：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并行指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 1. Serial 收集器 Serial 翻译为串行，也就是说它以串行的方式执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，在单个 CPU 环境下，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 场景下的默认新生代收集器，因为在该场景下内存一般来说不会很大。它收集一两百兆垃圾的停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿时间是可以接受的。 2. ParNew 收集器 它是 Serial 收集器的多线程版本。 它是 Server 场景下默认的新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合使用。 3. Parallel Scavenge 收集器与 ParNew 一样是多线程收集器。 其它收集器目标是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，因此它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户程序的时间占总时间的比值，即吞吐量 = 运行用户代码时间 / (运行用户代码时间 + 垃圾回收时间)。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。 缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 4. Serial Old 收集器 是 Serial 收集器的老年代版本，使用单线程和”标记 - 整理”算法，也是给 Client 场景下的虚拟机使用。如果用在 Server 场景下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本，使用多线程和”标记 - 整理”算法。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器 CMS（Concurrent Mark Sweep）是一种以获取最短回收停顿时间为目标的收集器，Mark Sweep 指的是标记 - 清除算法。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 并行与并发：G1 能充分利用多 CPU、多核环境下的硬件优势，使用多个 CPU 或 CPU 核心来缩短停顿时间，部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发地方式让 Java 程序继续执行。 分代收集：分代概念在 G1 中得以保留。 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 三、内存分配和回收策略Minor GC 和 Full GC Minor GC：回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC：回收老年代和新生代，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。 内存分配策略1. 对象优先在 Eden 分配大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。 2. 大对象直接进入老年代大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。 3. 长期存活的对象进入老年代为对象定义年龄计数器，对象在 Eden 出生并经过一次 Minor GC 后依然存活，并且能被 Survivor 容纳的话，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄(默认15岁)则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 4. 动态对象年龄判定虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 5. 空间分配担保在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。 Full GC 的触发条件对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： 1. 调用 System.gc()只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 2. 老年代空间不足老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 3. 空间分配担保失败使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第 5 小节。 4. JDK 1.7 及以前的永久代空间不足在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 5. Concurrent Mode Failure执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。 四、类加载机制类是在运行期间第一次使用时动态加载的，而不是一次性加载所有类。因为如果一次性加载，那么会占用很多的内存。 类的生命周期 包括以下 7 个阶段： 加载 (Loading) 验证 (Verification) 准备 (Preparation) 解析 (Resolution) 初始化 (Initialization) 使用 (Using) 卸载 (Unloading) 其中，验证、准备、解析3个部分统称为连接 (Linking) 类加载过程包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载加载是类加载的一个阶段，注意不要混淆。 加载过程完成以下三件事： 通过类的完全限定名称获取定义该类的二进制字节流。 将该字节流表示的静态存储结构转换为方法区的运行时存储结构。 在内存中生成一个代表该类的 Class 对象，作为方法区中该类各种数据的访问入口。 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础。 从网络中获取，最典型的应用是 Applet。 运行时计算生成，例如动态代理技术，在 java.lang.reflect.Proxy 使用 ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成，例如由 JSP 文件生成对应的 Class 类。 2. 验证确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 3. 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。 类变量是被 static 修饰的变量，不包括实例变量。实例变量不会在这阶段分配内存，它会在对象实例化时随着对象一起被分配在堆中。应该注意到，实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次。 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。 1public static int value = 123; 如果类变量是常量，那么它将初始化为表达式所定义的值而不是 0。例如下面的常量 value 被初始化为 123 而不是 0。 1public static final int value = 123; 4. 解析将常量池的符号引用替换为直接引用的过程。 其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。 5. 初始化初始化阶段才真正开始执行类中定义的 Java 程序代码。初始化阶段是虚拟机执行类构造器 &lt;clinit&gt;() 方法的过程。在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 &lt;clinit&gt;() 是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 由于父类的 &lt;clinit&gt;() 方法先执行，也就意味着父类中定义的静态语句块的执行要优先于子类。例如以下代码： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 2&#125; 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 &lt;clinit&gt;() 方法。但接口与类不同的是，执行接口的 &lt;clinit&gt;() 方法不需要先执行父接口的 &lt;clinit&gt;() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 &lt;clinit&gt;() 方法。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 &lt;clinit&gt;() 方法，其它线程都会阻塞等待，直到活动线程执行 &lt;clinit&gt;() 方法完毕。如果在一个类的 &lt;clinit&gt;() 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 类初始化时机1. 主动引用虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）： 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 2. 被动引用以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括： 通过子类引用父类的静态字段，不会导致子类初始化。 1System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。 1SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 1System.out.println(ConstClass.HELLOWORLD); 类与类加载器两个类相等，需要类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。 类加载器分类从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），使用 C++ 实现，是虚拟机自身的一部分； 所有其它类的加载器，使用 Java 实现，独立于虚拟机，继承自抽象类 java.lang.ClassLoader。 从 Java 开发人员的角度看，类加载器可以划分得更细致一些： 启动类加载器 (Bootstrap ClassLoader) 此类加载器负责将存放在 &lt;JRE_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。 扩展类加载器 (Extension ClassLoader) 这个类加载器是由 ExtClassLoader (sun.misc.Launcher$ExtClassLoader) 实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。 应用程序类加载器 (Application ClassLoader) 这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 双亲委派模型应用程序是由三种类加载器互相配合从而实现类加载，除此之外还可以加入自己定义的类加载器。 下图展示了类加载器之间的层次关系，称为双亲委派模型（Parents Delegation Model）。该模型要求除了顶层的启动类加载器外，其它的类加载器都要有自己的父类加载器。这里的父子关系一般通过组合关系（Composition）来实现，而不是继承关系（Inheritance）。 1. 工作过程一个类加载器首先将类加载请求转发到父类加载器，只有当父类加载器无法完成时才尝试自己加载。 2. 好处使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一。 例如 java.lang.Object 存放在 rt.jar 中，如果编写另外一个 java.lang.Object 并放到 ClassPath 中，程序可以编译通过。由于双亲委派模型的存在，所以在 rt.jar 中的 Object 比在 ClassPath 中的 Object 优先级更高，这是因为 rt.jar 中的 Object 使用的是启动类加载器，而 ClassPath 中的 Object 使用的是应用程序类加载器。rt.jar 中的 Object 优先级更高，那么程序中所有的 Object 都是这个 Object。 3. 实现以下是抽象类 java.lang.ClassLoader 的代码片段，其中的 loadClass() 方法运行过程如下：先检查类是否已经加载过，如果没有则让父类加载器去加载。当父类加载器加载失败时抛出 ClassNotFoundException，此时尝试自己去加载。 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class ClassLoader &#123; // The parent class loader for delegation private final ClassLoader parent; public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name); &#125;&#125; 自定义类加载器实现以下代码中的 FileSystemClassLoader 是自定义类加载器，继承自 java.lang.ClassLoader，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass() 方法来把这些字节代码转换成 java.lang.Class 类的实例。 java.lang.ClassLoader 的 loadClass() 实现了双亲委派模型的逻辑，自定义类加载器一般不去重写它，但是需要重写 findClass() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940public class FileSystemClassLoader extends ClassLoader &#123; private String rootDir; public FileSystemClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead; while ((bytesNumRead = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + ".class"; &#125;&#125; 五、Java 内存模型与线程硬件与效率的一致性由于处理器上的寄存器的读写的速度比内存快几个数量级，所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存 (Cache) 来作为内存。 加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。 Java 内存模型Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从外存中取出变量这样的底层细节。 主内存与工作内存所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。 线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。 内存间交互操作Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。 lock(锁定)：作用于主内存的变量，把一个变量标识为一条线程独占的状态 unlock(解锁)：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read(读取)：作用于主内存的变量，把一个变量的值从主内存传输到线程的工作内存中，以便随后的 load 动作使用 load(载入)：作用于工作内存的变量，在 read 之后执行，把 read 从主内存中得到的变量值放入工作内存的变量副本中 use(使用)：作用于工作内存的变量，把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作 assign(赋值)：作用于工作内存的变量，把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作 store(存储)：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便后面的 write 操作使用 write(写入)：作用于主内存的变量，在 store 之后执行，把 store 从工作内存中得到的变量的值放入主内存的变量中 内存模型三大特性1. 原子性Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。 有一个错误认识就是，int 等原子性的类型在多线程环境中不会出现线程安全问题。前面的线程不安全示例代码中，cnt 属于 int 类型变量，1000 个线程对它进行自增操作之后，得到的值为 997 而不是 1000。 为了方便讨论，将内存间的交互操作简化为 3 个：load、assign、store。 下图演示了两个线程同时对 cnt 进行操作，load、assign、store 这一系列操作整体上看不具备原子性，那么在 T1 修改 cnt 并且还没有将修改后的值写入主内存，T2 依然可以读入旧值。可以看出，这两个线程虽然执行了两次自增运算，但是主内存中 cnt 的值最后为 1 而不是 2。因此对 int 类型读写操作满足原子性只是说明 load、assign、store 这些单个操作具备原子性。 AtomicInteger 能保证多个线程修改的原子性。 使用 AtomicInteger 重写之前线程不安全的代码之后得到以下线程安全实现： 1234567891011public class AtomicExample &#123; private AtomicInteger cnt = new AtomicInteger(); public void add() &#123; cnt.incrementAndGet(); &#125; public int get() &#123; return cnt.get(); &#125;&#125; 123456789101112131415public static void main(String[] args) throws InterruptedException &#123; final int threadSize = 1000; AtomicExample example = new AtomicExample(); // 只修改这条语句 final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) &#123; executorService.execute(() -&gt; &#123; example.add(); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());&#125; 11000 除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的原子性。它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。 1234567891011public class AtomicSynchronizedExample &#123; private int cnt = 0; public synchronized void add() &#123; cnt++; &#125; public synchronized int get() &#123; return cnt; &#125;&#125; 123456789101112131415public static void main(String[] args) throws InterruptedException &#123; final int threadSize = 1000; AtomicSynchronizedExample example = new AtomicSynchronizedExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; threadSize; i++) &#123; executorService.execute(() -&gt; &#123; example.add(); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await(); executorService.shutdown(); System.out.println(example.get());&#125; 11000 2. 可见性可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。 主要有三种实现可见性的方式： volatile synchronized，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。 final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。 对前面的线程不安全示例中的 cnt 变量使用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。 3. 有序性有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。 先行发生原则上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。 1. 单一线程原则 Single Thread rule 在一个线程内，在程序前面的操作先行发生于后面的操作。 2. 管程锁定规则 Monitor Lock Rule 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 3. volatile 变量规则 Volatile Variable Rule 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 4. 线程启动规则 Thread Start Rule Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 5. 线程加入规则 Thread Join Rule Thread 对象的结束先行发生于 join() 方法返回。 6. 线程中断规则 Thread Interruption Rule 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。 7. 对象终结规则 Finalizer Rule 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。 8. 传递性 Transitivity 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 Java 与线程线程是比进程更轻量级的调度执行单位，线程的引入，可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源(内存地址，文件 IO 等)，又可以独立调度（线程是 CPU 调度的基本单位）。 Thread类的所有关键方法都声明了native的，意味着这个方法没有使用或无法使用平台无关的手段来实现，也有可能是为了执行效率。 实现线程主要有三种方式：使用内核线程实现，使用用户线程实现和使用用户线程加轻量级进程混合实现。 线程的实现1. 使用内核线程实现内核线程（KLT，Kernel-Level Thread），直接由操作系统内核（Kernel，即内核）支持的线程。 程序一般不会去直接使用内核线程，而是去使用内核线程的一种高级接口——轻量级进程（LWP），即通常意义上的线程。 2. 使用用户线程实现广义上，内核线程以外，就是用户线程。轻量级也算用户线程，但轻量级进程的实现始终是建立在内核上的，许多操作都要进行系统调度，效率会受到限制。 狭义上，用户线程指完全建立在用户空间的线程库上。这种线程不需要切换内核态，效率非常高且低消耗，也可以支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。这种进程与用户线程之间1：N的关系称为一对多的线程模型。 用户线程优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都是需要用户程序自己处理。阻塞处理等问题的解决十分困难，甚至不可能完成。所以使用用户线程会非常复杂。 3. 用户线程加轻量级进程混合实现内核线程与用户线程混合使用。可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级线程来完成，大大降低整个进程被完全阻塞的风险。用户线程与轻量级进程比例是N:M。 4. Java线程的实现JDK1.2 之前，绿色线程——用户线程。JDK1.2——基于操作系统原生线程模型来实现。 Sun JDK,它的Windows版本和 Linux 版本都使用一对一的线程模型实现，一条Java线程就映射到一条轻量级进程之中。 Solaris 同时支持一对一和多对多。 Java 线程调度线程调度是指系统为线程分配处理器使用权的过程，主要调度方式分两种，分别是协同式线程调度和抢占式线程调度。 协同式线程调度线程执行时间由线程本身来控制，线程把自己的工作执行完之后，要主动通知系统切换到另外一个线程上。 优点：实现简单，且切换操作对线程自己是可知的，没啥线程同步问题。 缺点：线程执行时间不可控制，如果一个线程有问题，可能一直阻塞在那里。 抢占式调度每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定（Java 中，Thread.yield() 可以让出执行时间，但无法获取执行时间）。线程执行时间系统可控，也不会有一个线程导致整个进程阻塞。 Java线程调度就是抢占式调度希望系统能给某些线程多分配一些时间，给一些线程少分配一些时间，可以通过设置线程优先级来完成。Java语言一共10个级别的线程优先级（Thread.MIN_PRIORITY至Thread.MAX_PRIORITY），在两线程同时处于ready状态时，优先级越高的线程越容易被系统选择执行。但优先级并不是很靠谱，因为Java线程是通过映射到系统的原生线程上来实现的，所以线程调度最终还是取决于操作系统。 状态转换 六、线程安全当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要考虑进行额外的同步，或者在调用方进行任何其他的写作操作，调用这个对象的行为都可以获得正确的结果，那这个对象时线程安全的。 五类各种操作共享的数据不可变一定是线程安全的，如用 final 关键字修饰的变量，String 对象，枚举类型，部分 Number 子类，如 Double, Long 等数值包装类，BigInteger, BigDecimal 等大数据类型 绝对线程安全对于 Java API 中标注自己是线程安全的类，大多不是绝对的线程安全，如 Vector，虽然他的 add()，get()，remove() 方法都是被 synchronized 修饰的，但是在多线程的环境中，如果不在方法中做额外的操作，仍然是不安全的。 相对线程安全通常意义上的线程安全，它需要保证对这个对象单独的操作是线程安全的，我们在调用的时候不需要做额外的保障措施，但对于一些特定顺序连续调用，可能需要在调用端使用额外的同步手段来保证调用的顺序性。如 Vector， HashTable。 线程兼容指对象本身并不是线程安全的，但是可以通过在调用端正确地使用同步手段来保证对象在并发环境中可以安全滴使用，如 HashMap，ArrayList。 线程对立指无论调用段是否采用了同步措施，都无法在多线程环境中并发使用的代码，如 resume(), suspend()，如果两个线程同时持有一个线程对象，一个尝试恢复线程，一个尝试中断线程，如果并发进行的话，无论调用时是否进行了同步，目标线程都是存在死锁风险。 线程安全的实现1. 不可变不可变（Immutable）的对象一定是线程安全的，不需要再采取任何的线程安全保障措施。只要一个不可变的对象被正确地构建出来，永远也不会看到它在多个线程之中处于不一致的状态。多线程环境下，应当尽量使对象成为不可变，来满足线程安全。 不可变的类型： final 关键字修饰的基本数据类型 String 枚举类型 Number 部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型。但同为 Number 的原子类 AtomicInteger 和 AtomicLong 则是可变的。 对于集合类型，可以使用 Collections.unmodifiableXXX() 方法来获取一个不可变的集合。 1234567public class ImmutableExample &#123; public static void main(String[] args) &#123; Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); Map&lt;String, Integer&gt; unmodifiableMap = Collections.unmodifiableMap(map); unmodifiableMap.put("a", 1); &#125;&#125; 123Exception in thread "main" java.lang.UnsupportedOperationException at java.util.Collections$UnmodifiableMap.put(Collections.java:1457) at ImmutableExample.main(ImmutableExample.java:9) Collections.unmodifiableXXX() 先对原始的集合进行拷贝，需要对集合进行修改的方法都直接抛出异常。 123public V put(K key, V value) &#123; throw new UnsupportedOperationException();&#125; 2. 互斥同步在多线程访问的时候，保证同一时间只有一条线程使用。而互斥是实现同步的一种手段，临界区(Critical Section)，互斥量(Mutex)，信号量(Semaphore)都是主要的互斥实现方式。 Java中实现互斥同步的两种方法 使用 synchronized 实现同步，编译之后会形成 monitorenter 和 monitorexit 这两个字节码指令，这两个字节码都需要一个 reference 类型的参数来指明要锁定和解锁的对象。如果 Java 程序中的 synchronized 明确制定了对象参数，那就是这个对象的 reference；如果没有明确指定，那就根据 synchronized 修饰的是实例方法还是类方法，去取对应的对象实例或 Class 对象来作为锁对象。还有个锁的计数器，来记录拥有锁的次数，跟 AQS 里面的 state 一样。 还可以使用 java.util.concurrent(J.U.C) 包中的重入锁（ReentrantLock）来实现同步，在基本用法上，与 synchronized 很相似，他们都具备一样的线程重入特性，只是代码写法上有点区别，一个表现为 API 层面的互斥锁（lock() 和 unlock() 方法配合 try/finally 语句块来完成），另一个表现为原生语法层面的互斥锁。不过，相比 synchronized,ReentrantLock 增加了一些高级功能，主要有以下3项：等待可中断、可实现公平锁，以及锁可以绑定多个条件。 等待可中断是指当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情，可中断特性对处理执行时间非常长的同步块很有帮助。 公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁；而非公平锁则不保证这一点，在锁被释放时，任何一个等待锁的线程都有机会获得锁。synchronized 中的锁是非公平的， ReentrantLock 默认情况下也是非公平的，但可以通过带布尔值的构造函数要求使用公平锁。 锁绑定多个条件是指一个 ReentrantLock 对象可以同时绑定多个 Condition 对象，而在 synchronized 中，锁对象的 wait() 和 notify() 或 notifyAll() 方法可以实现一个隐含的条件，如果要和多于一个的条件关联的时候，就不得不额外地添加一个锁，而 ReentrantLock 则无须这样做，只需要多次调用 newCondition() 方法即可。 缺点 互斥同步最主要的问题就是线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。 互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。 ###3. 非阻塞同步 基于冲突检测的乐观并发策略，通俗地说，就是先进行操作，如果没有其他线程争用共享数据，那操作就成功了；如果共享数据有争用，产生了冲突，那就再采取其他的补偿措施（最常见的补偿措施就是不断地重试，直到成功为止），这种乐观的并发策略的许多实现都不需要把线程挂起，因此这种操作称为非阻塞同步（Non-Blocking Synchronization）。 乐观并发策略需要“硬件指令集”，这类指令常用的有： 测试并设置（Test-and-Set） 获取并增加（Fetch-and-Increment） 交换（Swap） 比较并交换（Compare-and-Swap，下文称CAS） 加载链接/条件存储（Load-Linked/Store-Conditional，下文称LL/SC） CAS随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要将线程阻塞，因此这种同步操作称为非阻塞同步。 乐观锁需要操作和冲突检测这两个步骤具备原子性，这里就不能再使用互斥同步来保证了，只能靠硬件来完成。硬件支持的原子性操作最典型的是：比较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B。 AtomicIntegerJ.U.C 包里面的整数原子类 AtomicInteger 的方法调用了 Unsafe 类的 CAS 操作。 以下代码使用了 AtomicInteger 执行了自增的操作。 12345private AtomicInteger cnt = new AtomicInteger();public void add() &#123; cnt.incrementAndGet();&#125; 以下代码是 incrementAndGet() 的源码，它调用了 Unsafe 的 getAndAddInt() 。 123public final int incrementAndGet() &#123; return unsafe.getAndAddInt(this, valueOffset, 1) + 1;&#125; 以下代码是 getAndAddInt() 源码，var1 指示对象内存地址，var2 指示该字段相对对象内存地址的偏移，var4 指示操作需要加的数值，这里为 1。通过 getIntVolatile(var1, var2) 得到旧的预期值，通过调用 compareAndSwapInt() 来进行 CAS 比较，如果该字段内存地址中的值等于 var5，那么就更新内存地址为 var1+var2 的变量为 var5+var4。 可以看到 getAndAddInt() 在一个循环中进行，发生冲突的做法是不断的进行重试。 12345678public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; ABA如果一个变量初次读取的时候是 A 值，它的值被改成了 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。 J.U.C 包提供了一个带有标记的原子引用类 AtomicStampedReference 来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。大部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改用传统的互斥同步可能会比原子类更高效。 4. 无同步方案要保证线程安全，并不是一定就要进行同步。如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性。 栈封闭多个线程访问同一个方法的局部变量时，不会出现线程安全问题，因为局部变量存储在虚拟机栈中，属于线程私有的。 123456789public class StackClosedExample &#123; public void add100() &#123; int cnt = 0; for (int i = 0; i &lt; 100; i++) &#123; cnt++; &#125; System.out.println(cnt); &#125;&#125; 1234567public static void main(String[] args) &#123; StackClosedExample example = new StackClosedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; example.add100()); executorService.execute(() -&gt; example.add100()); executorService.shutdown();&#125; 12100100 线程本地存储（Thread Local Storage）如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。 符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完。其中最重要的一个应用实例就是经典 Web 交互模型中的“一个请求对应一个服务器线程”（Thread-per-Request）的处理方式，这种处理方式的广泛应用使得很多 Web 服务端应用都可以使用线程本地存储来解决线程安全问题。 可以使用 java.lang.ThreadLocal 类来实现线程本地存储功能。 对于以下代码，thread1 中设置 threadLocal 为 1，而 thread2 设置 threadLocal 为 2。过了一段时间之后，thread1 读取 threadLocal 依然是 1，不受 thread2 的影响。 123456789101112131415161718192021public class ThreadLocalExample &#123; public static void main(String[] args) &#123; ThreadLocal threadLocal = new ThreadLocal(); Thread thread1 = new Thread(() -&gt; &#123; threadLocal.set(1); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(threadLocal.get()); threadLocal.remove(); &#125;); Thread thread2 = new Thread(() -&gt; &#123; threadLocal.set(2); threadLocal.remove(); &#125;); thread1.start(); thread2.start(); &#125;&#125; 11 为了理解 ThreadLocal，先看以下代码： 12345678910111213141516public class ThreadLocalExample1 &#123; public static void main(String[] args) &#123; ThreadLocal threadLocal1 = new ThreadLocal(); ThreadLocal threadLocal2 = new ThreadLocal(); Thread thread1 = new Thread(() -&gt; &#123; threadLocal1.set(1); threadLocal2.set(1); &#125;); Thread thread2 = new Thread(() -&gt; &#123; threadLocal1.set(2); threadLocal2.set(2); &#125;); thread1.start(); thread2.start(); &#125;&#125; 它所对应的底层结构图为： 每个 Thread 都有一个 ThreadLocal.ThreadLocalMap 对象。 123/* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ThreadLocal.ThreadLocalMap threadLocals = null; 当调用一个 ThreadLocal 的 set(T value) 方法时，先得到当前线程的 ThreadLocalMap 对象，然后将 ThreadLocal-&gt;value 键值对插入到该 Map 中。 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; get() 方法类似。 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; ThreadLocal 从理论上讲并不是用来解决多线程并发问题的，因为根本不存在多线程竞争。 在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，应该尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险。 可重入代码（Reentrant Code）这种代码也叫做纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误。 可重入代码有一些共同的特征，例如不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法等。 七、锁优化这里的锁优化主要是指 JVM 对 synchronized 的优化。 自旋锁互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。 自旋锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。 在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。 锁消除锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。 锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。 对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁： 123public static String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作： 1234567public static String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。 锁粗化如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。 上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。 轻量级锁JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。 以下是 HotSpot 虚拟机对象头的内存布局，这些数据被称为 Mark Word。其中 tag bits 对应了五个状态，这些状态在右侧的 state 表格中给出。除了 marked for gc 状态，其它四个状态已经在前面介绍过了。 下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。 轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。 当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程的虚拟机栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。 如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。 偏向锁偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。 当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。 当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2019%2F04%2F06%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前提 待排序的元素需要实现 Java 的 Comparable 接口，该接口有 compareTo() 方法，可以用它来判断两个元素的大小关系。 研究排序算法的成本模型时，统计的是比较和交换的次数。 使用辅助函数 less() 和 swap() 来进行比较和交换的操作，使得代码的可读性和可移植性更好。 1234567891011121314public abstract class Sort&lt;T extends Comparable&lt;T&gt;&gt; &#123; public abstract void sort(T[] nums); protected boolean less(T v, T w) &#123; return v.compareTo(w) &lt; 0; &#125; protected void swap(T[] a, int i, int j) &#123; T t = a[i]; a[i] = a[j]; a[j] = t; &#125;&#125; 冒泡排序从左到右不断交换相邻逆序的元素，在一轮的循环之后，可以让未排序的最大元素上浮到右侧。 在一轮循环中，如果没有发生交换，就说明数组已经是有序的，此时可以直接退出。 1234567891011121314151617public class Bubble&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; boolean hasSorted = false; for (int i = N - 1; i &gt; 0 &amp;&amp; !hasSorted; i--) &#123; hasSorted = true; for (int j = 0; j &lt; i; j++) &#123; if (less(nums[j + 1], nums[j])) &#123; hasSorted = false; swap(nums, j, j + 1); &#125; &#125; &#125; &#125;&#125; 插入排序每次都将当前元素插入到左侧已经排序的数组中，使得插入之后左侧数组依然有序。 对于数组 {3, 5, 2, 4, 1}，它具有以下逆序：(3, 2), (3, 1), (5, 2), (5, 4), (5, 1), (2, 1), (4, 1)，插入排序每次只能交换相邻元素，令逆序数量减少 1，因此插入排序需要交换的次数为逆序数量。 插入排序的复杂度取决于数组的初始顺序，如果数组已经部分有序了，逆序较少，那么插入排序会很快。 平均情况下插入排序需要 ~N2/4 比较以及 ~N2/4 次交换； 最坏的情况下需要 ~N2/2 比较以及 ~N2/2 次交换，最坏的情况是数组是倒序的； 最好的情况下需要 N-1 次比较和 0 次交换，最好的情况就是数组已经有序了。 123456789101112public class Insertion&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; for (int i = 1; i &lt; N; i++) &#123; for (int j = i; j &gt; 0 &amp;&amp; less(nums[j], nums[j - 1]); j--) &#123; swap(nums, j, j - 1); &#125; &#125; &#125;&#125; 选择排序选择出数组中的最小元素，将它与数组的第一个元素交换位置。再从剩下的元素中选择出最小的元素，将它与数组的第二个元素交换位置。不断进行这样的操作，直到将整个数组排序。 选择排序需要 ~N2/2 次比较和 ~N 次交换，它的运行时间与输入无关，这个特点使得它对一个已经排序的数组也需要这么多的比较和交换操作。 12345678910111213141516public class Selection&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; for (int i = 0; i &lt; N - 1; i++) &#123; int min = i; for (int j = i + 1; j &lt; N; j++) &#123; if (less(nums[j], nums[min])) &#123; min = j; &#125; &#125; swap(nums, i, min); &#125; &#125;&#125; 希尔排序 对于大规模的数组，插入排序很慢，因为它只能交换相邻的元素，每次只能将逆序数量减少 1。 希尔排序的出现就是为了解决插入排序的这种局限性，它通过交换不相邻的元素，每次可以将逆序数量减少大于 1。 希尔排序使用插入排序对间隔 h 的序列进行排序。通过不断减小 h，最后令 h=1，就可以使得整个数组是有序的。 12345678910111213141516171819202122public class Shell&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; int N = nums.length; int h = 1; while (h &lt; N / 3) &#123; h = 3 * h + 1; // 1, 4, 13, 40, ... &#125; while (h &gt;= 1) &#123; for (int i = h; i &lt; N; i++) &#123; for (int j = i; j &gt;= h &amp;&amp; less(nums[j], nums[j - h]); j -= h) &#123; swap(nums, j, j - h); &#125; &#125; h = h / 3; &#125; &#125;&#125; 希尔排序的运行时间达不到平方级别，使用递增序列 1, 4, 13, 40, … 的希尔排序所需要的比较次数不会超过 N 的若干倍乘于递增序列的长度。后面介绍的高级排序算法只会比希尔排序快两倍左右。 归并排序归并排序的思想是将数组分成两部分，分别进行排序，然后归并起来。 归并方法归并方法将数组中两个已经排序的部分归并成一个。 1234567891011121314151617181920212223242526public abstract class MergeSort&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; protected void merge(T[] arr, int left, int mid, int right, T[] temp)&#123; int i = left; //左序列指针 int j = mid + 1; //右序列指针 int t = 0;//临时数组指针 while (i &lt;= mid &amp;&amp; j &lt;= right)&#123; if(less(arr[i], arr[j]))&#123; temp[t++] = arr[i++]; &#125;else &#123; temp[t++] = arr[j++]; &#125; &#125; while(i &lt;= mid)&#123;//将左边剩余元素填充进temp中 temp[t++] = arr[i++]; &#125; while(j &lt;= right)&#123;//将右序列剩余元素填充进temp中 temp[t++] = arr[j++]; &#125; t = 0; //将temp中的元素全部拷贝到原数组中 while(left &lt;= right)&#123; arr[left++] = temp[t++]; &#125; &#125;&#125; 自顶向下归并排序将一个大数组分成两个小数组去求解。 因为每次都将问题对半分成两个子问题，这种对半分的算法复杂度一般为 O(NlogN)。 123456789101112131415161718public class Up2DownMergeSort&lt;T extends Comparable&lt;T&gt;&gt; extends MergeSort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; T[] temp = (T[]) new Comparable[nums.length];//在排序前，先建好一个长度等于原数组长度的临时数组，避免递归中频繁开辟空间 sort(nums, 0, nums.length-1, temp); &#125; private void sort(T[] nums, int left, int right, T[] temp) &#123; if(left &lt; right)&#123; int mid = (left + right) / 2; sort(nums, left, mid, temp);//左边归并排序，使得左子序列有序 sort(nums, mid + 1, right, temp);//右边归并排序，使得右子序列有序 merge(nums, left, mid, right, temp);//将两个有序子数组合并操作 &#125; &#125;&#125; 快速排序 归并排序将数组分为两个子数组分别排序，并将有序的子数组归并使得整个数组排序； 快速排序通过一个切分元素将数组分为两个子数组，左子数组小于等于切分元素，右子数组大于等于切分元素，将这两个子数组排序也就将整个数组排序了。 取 a[l] 作为切分元素，然后从数组的左端向右扫描直到找到第一个大于等于它的元素，再从数组的右端向左扫描找到第一个小于它的元素，交换这两个元素。不断进行这个过程，就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，将切分元素 a[l] 和 a[j] 交换位置。 1234567891011121314151617181920212223242526272829public class QuickSort&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; @Override public void sort(T[] nums) &#123; sort(nums, 0, nums.length - 1); &#125; private void sort(T[] nums, int l, int h) &#123; if (h &lt;= l) return; int j = partition(nums, l, h); sort(nums, l, j - 1); sort(nums, j + 1, h); &#125; private int partition(T[] nums, int l, int h) &#123; int i = l, j = h + 1; T v = nums[l]; while (true) &#123; while (less(nums[++i], v) &amp;&amp; i != h) ; while (less(v, nums[--j]) &amp;&amp; j != l) ; if (i &gt;= j) break; swap(nums, i, j); &#125; swap(nums, l, j); return j; &#125;&#125; 性能分析 快速排序是原地排序，不需要辅助数组，但是递归调用需要辅助栈。 快速排序最好的情况下是每次都正好将数组对半分，这样递归调用次数才是最少的。这种情况下比较次数为 CN=2CN/2+N，复杂度为 O(NlogN)。 最坏的情况下，第一次从最小的元素切分，第二次从第二小的元素切分，如此这般。因此最坏的情况下需要比较 N2/2。为了防止数组最开始就是有序的，在进行快速排序时需要随机打乱数组。 算法改进 切换到插入排序 因为快速排序在小数组中也会递归调用自己，对于小数组，插入排序比快速排序的性能更好，因此在小数组中可以切换到插入排序。 三数取中 最好的情况下是每次都能取数组的中位数作为切分元素，但是计算中位数的代价很高。一种折中方法是取 3 个元素，并将大小居中的元素作为切分元素。 三向切分 对于有大量重复元素的数组，可以将数组切分为三部分，分别对应小于、等于和大于切分元素。 三向切分快速排序对于有大量重复元素的随机数组可以在线性时间内完成排序。 1234567891011121314151617181920212223public class ThreeWayQuickSort&lt;T extends Comparable&lt;T&gt;&gt; extends QuickSort&lt;T&gt; &#123; @Override protected void sort(T[] nums, int l, int h) &#123; if (h &lt;= l) &#123; return; &#125; int lt = l, i = l + 1, gt = h; T v = nums[l]; while (i &lt;= gt) &#123; int cmp = nums[i].compareTo(v); if (cmp &lt; 0) &#123; swap(nums, lt++, i++); &#125; else if (cmp &gt; 0) &#123; swap(nums, i, gt--); &#125; else &#123; i++; &#125; &#125; sort(nums, l, lt - 1); sort(nums, gt + 1, h); &#125;&#125; 堆排序堆堆中某个节点的值总是大于等于其子节点的值，并且堆是一颗完全二叉树。 堆可以用数组来表示，这是因为堆是完全二叉树，而完全二叉树很容易就存储在数组中。位置 k 的节点的父节点位置为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。这里不使用数组索引为 0 的位置，是为了更清晰地描述节点的位置关系。 123456789101112131415161718192021222324252627public class Heap&lt;T extends Comparable&lt;T&gt;&gt; &#123; private T[] heap; private int N = 0; public Heap(int maxN) &#123; this.heap = (T[]) new Comparable[maxN + 1]; &#125; public boolean isEmpty() &#123; return N == 0; &#125; public int size() &#123; return N; &#125; private boolean less(int i, int j) &#123; return heap[i].compareTo(heap[j]) &lt; 0; &#125; private void swap(int i, int j) &#123; T t = heap[i]; heap[i] = heap[j]; heap[j] = t; &#125;&#125; 上浮和下沉在堆中，当一个节点比父节点大，那么需要交换这个两个节点。交换后还可能比它新的父节点大，因此需要不断地进行比较和交换操作，把这种操作称为上浮。 123456private void swim(int k) &#123; while (k &gt; 1 &amp;&amp; less(k / 2, k)) &#123; swap(k / 2, k); k = k / 2; &#125;&#125; 类似地，当一个节点比子节点来得小，也需要不断地向下进行比较和交换操作，把这种操作称为下沉。一个节点如果有两个子节点，应当与两个子节点中最大那个节点进行交换。 1234567891011private void sink(int k) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(j, j + 1)) j++; if (!less(k, j)) break; swap(k, j); k = j; &#125;&#125; 插入元素将新元素放到数组末尾，然后上浮到合适的位置。 1234public void insert(Comparable v) &#123; heap[++N] = v; swim(N);&#125; 删除最大元素从数组顶端删除最大的元素，并将数组的最后一个元素放到顶端，并让这个元素下沉到合适的位置。 1234567public T delMax() &#123; T max = heap[1]; swap(1, N--); heap[N + 1] = null; sink(1); return max;&#125; 堆排序把最大元素和当前堆中数组的最后一个元素交换位置，并且不删除它，那么就可以得到一个从尾到头的递减序列，从正向来看就是一个递增序列，这就是堆排序。 构建堆 无序数组建立堆最直接的方法是从左到右遍历数组进行上浮操作。一个更高效的方法是从右至左进行下沉操作，如果一个节点的两个节点都已经是堆有序，那么进行下沉操作可以使得这个节点为根节点的堆有序。叶子节点不需要进行下沉操作，可以忽略叶子节点的元素，因此只需要遍历一半的元素即可。 交换堆顶元素与最后一个元素 交换之后需要进行下沉操作维持堆的有序状态。 1234567891011121314151617181920212223242526272829303132public class HeapSort&lt;T extends Comparable&lt;T&gt;&gt; extends Sort&lt;T&gt; &#123; /** * 数组第 0 个位置不能有元素 */ @Override public void sort(T[] nums) &#123; int N = nums.length - 1; for (int k = N / 2; k &gt;= 1; k--) sink(nums, k, N); while (N &gt; 1) &#123; swap(nums, 1, N--); sink(nums, 1, N); &#125; &#125; private void sink(T[] nums, int k, int N) &#123; while (2 * k &lt;= N) &#123; int j = 2 * k; if (j &lt; N &amp;&amp; less(nums, j, j + 1)) j++; if (!less(nums, k, j)) break; swap(nums, k, j); k = j; &#125; &#125; private boolean less(T[] nums, int i, int j) &#123; return nums[i].compareTo(nums[j]) &lt; 0; &#125;&#125; 分析一个堆的高度为 logN，因此在堆中插入元素和删除最大元素的复杂度都为 logN。 对于堆排序，由于要对 N 个节点进行下沉操作，因此复杂度为 NlogN。 堆排序是一种原地排序，没有利用额外的空间。 现代操作系统很少使用堆排序，因为它无法利用局部性原理进行缓存，也就是数组元素很少和相邻的元素进行比较和交换。 小结排序算法的比较 算法 稳定性 时间复杂度 空间复杂度 备注 选择排序 × N2 1 冒泡排序 √ N2 1 插入排序 √ N ~ N2 1 时间复杂度和初始顺序有关 希尔排序 × N 的若干倍乘于递增序列的长度 1 改进版插入排序 快速排序 × NlogN logN 三向切分快速排序 × N ~ NlogN logN 适用于有大量重复主键 归并排序 √ NlogN N 堆排序 × NlogN 1 无法利用局部性原理 快速排序是最快的通用排序算法，它的内循环的指令很少，而且它还能利用缓存，因为它总是顺序地访问数据。它的运行时间近似为 ~cNlogN，这里的 c 比其它线性对数级别的排序算法都要小。 使用三向切分快速排序，实际应用中可能出现的某些分布的输入能够达到线性级别，而其它排序算法仍然需要线性对数时间。 Java 的排序算法实现Java 主要排序方法为 java.util.Arrays.sort()，对于原始数据类型使用三向切分的快速排序，对于引用类型使用归并排序。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客迁移]]></title>
    <url>%2F2019%2F04%2F05%2F%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[把 hexo 博客从 win10 迁移到 Mac 上打理。 环境搭建 安装 Homebrew 在 Homebrew 上安装 nodejs 1brew install node 使用 npm 搭建 hexo 12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server 这时候在 localhost:4000 就能看到 hexo 的欢迎界面了。 注意：这里 npm 下载因为源的问题可能会很慢，可使用以下方法解决 1npm config set registry https://registry.npm.taobao.org 迁移 在 blog 目录下初始化 git 1git init 安装 git 依赖 1npm install hexo-deployer-git --save 设置配置 user.name 和 user.email 12git config user.name "xxx" //(""的账号是Github里面自己的账号) git config user.email "xxx@xxx.com" //(""的邮箱是你自己注册的邮箱) 设置 ssh key 到 GitHub 复制 .ssh 中的 id_rsa.pub 中的内容到 github 的 SSH keys 中 测试 ssh 1ssh -T git@github.com 如果是第一次测试的话终端会丢一个 warn 来问候你一下，敲 yes 然后回车就行了。如果收到 Hi xxx!(xxx是你的用户名)，证明 ssh 连接成功。 把博客文件复制到新的博客文件中覆盖 复制之前可以用 hexo clean 来清一下文件 复制不必全部复制覆盖，只需如下文件即可： _config.yml package.json scaffolds/ source/ themes/ 安装自定义主题所需要的依赖，到主题官网查看即可 执行 hexo d -g 查看效果，大功告成。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合常用方法]]></title>
    <url>%2F2019%2F04%2F03%2FJava%E9%9B%86%E5%90%88%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Collection 常用方法 方法 详解 boolean add(E e) 添加元素到 Collection 集合中 boolean addAll(Collection&lt;? extends E&gt; c) 将指定 c 中的所有元素都添加到此 Collection 集合中 void clear() 移除此 Collection 集合中的所有元素 boolean contains(Object o) 检查 Collection 集合中是否包含 o 对象，如果包含则返回 true，否则返回 false boolean containsAll(Collection&lt;?&gt; c) 检查 Collection 集合中是否包含c的全部对象，全部包含则返回 true，否则返回 false boolean equals(Object o) 比较此 Collection 集合与指定对象是否相等，是比较的是里面元素是否相等，而不是比较地址是否相等 int hashCode() 返回此 Collection 集合的哈希码值 boolean isEmpty() 检查 Collection 集合是否包含有元素，如果没有包含元素，则返回 true，否则返回 false Iterator&lt;E&gt; iterator() 返回在此 Collection 集合的元素上进行迭代的迭代器 boolean remove(Object o) 从 Collection 集合中删除指定的元素，如果集合中有这个元素，并且删除成功，那么就返回 true，否则返回 false boolean removeAll(Collection&lt;?&gt; c) 从集合中删除c集合中所有的元素 boolean retainAll(Collection&lt;?&gt; c) 集合中仅保留c集合中的所有元素 int size() 返回集合中元素个数 Object[] toArray() 返回包含此 Collection 集合中所有元素的数组 List 集合常用方法继承 Collection 常用方法 方法 详解 void add(int index, E element) 在指定位置插入元素，后面的元素都往后移一个元素 boolean addAll(int index, Collection&lt;? extends E&gt; c) 在指定的位置中插入 c 集合全部的元素，如果集合发生改变，则返回 true，否则返回 false E get(int index) 返回 list 集合中指定索引位置的元素 int indexOf(Object o) 返回 list 集合中第一次出现 o 对象的索引位置，如果 list 集合中没有 o 对象，那么就返回 -1 ListIterator&lt;E&gt; listIterator() 返回此列表元素的列表迭代器（按适当顺序） ListIterator&lt;E&gt; listIterator(int index) 从指定位置开始，返回此列表元素的列表迭代器（按适当顺序） E remove(int index) 删除指定索引的对象 E set(int index, E element) 在索引为 index 位置的元素更改为 element 元素 List&lt;E&gt; subList(int fromIndex, int toIndex) 返回从索引 fromIndex 到 toIndex 的元素集合，包左不包右 Set 集合使用方法继承 Collection 常用方法 Map 集合常用方法 方法 详解 V put(K key, V value) 向 map 集合中添加 Key 为 key，Value 为 value 的元素，当添加成功时返回 null，否则返回 value void putAll(Map&lt;? extends K,? extends V&gt; m) 向 map 集合中添加指定集合的所有元素 void clear() 把 map 集合中所有的键值删除 boolean containsKey(Object key) 检出 map 集合中有没有包含 Key 为 key 的元素，如果有则返回 true，否则返回 false boolean containsValue(Object value) 检出 map 集合中有没有包含 Value 为 value 的元素，如果有则返回 true，否则返回 false Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() 返回 map 到一个 Set 集合中，以 map 集合中的 Key=Value 的形式返回到 set 中 boolean equals(Object o) 判断两个 Set 集合的元素是否相同 V get(Object key) 根据 map 集合中元素的 Key 来获取相应元素的 Value int hashCode() 返回 map 集合的哈希码值 boolean isEmpty() 检出 map 集合中是否有元素，如果没有则返回 true，如果有元素则返回 false Set&lt;K&gt; keySet() 返回 map 集合中所有 Key V remove(Object key) 删除 Key 为 key 值的元素 int size() 返回 map 集合中元素个数 Collection&lt;V&gt; values() 返回 map 集合中所有的 Value 到一个 Collection 集合 HashMap其余的方法都是实现Map集合的 方法 详解 public Object clone() 返回 hashMap 集合的副本 Hashtable其他的方法都是实现 Map 集合的方法 方法 详解 public Object clone() 返回 Hashtable 的副本 public Enumeration&lt;V&gt; elements() 返回此哈希表中的值的枚举 TreeMap其他的方法都是实现 Map 集合的方法 方法 详解 public Map.Entry&lt;K,V&gt; ceilingEntry(K key) 返回指定的 Key 大于或等于的最小值的元素，如果没有，则返回 null public K ceilingKey(K key) 返回指定的 Key 大于或等于的最小值的 Key，如果没有，则返回 null public Object clone() 返回集合的副本 public Comparator&lt;? super K&gt; comparator() 如果使用默认的比较器，就返回 null，如果使用其他的比较器，则返回比较器的哈希码值 public NavigableSet&lt;K&gt; descendingKeySet() 返回集合的全部 Key，并且是逆序的 public NavigableMap&lt;K,V&gt; descendingMap() 把集合逆序返回 public Map.Entry&lt;K,V&gt; firstEntry() 返回集合中最小 Key 的元素 public K firstKey() 返回集合中最小 Key 的 key public Map.Entry&lt;K,V&gt; floorEntry(K key) 与 ceilingEntry() 方法相反，是返回小于等于 key 的最大 Key 的元素 public K floorKey(K key) 返回小于等于 key 的最大 Key 的 key public SortedMap&lt;K,V&gt; headMap(K toKey) 返回 Key 小于 toKey 的所有元素 public NavigableMap&lt;K,V&gt; headMap(K toKey, boolean inclusive) 当 inclusive 为 true 时，就是返回 Key 小于等于 toKey 的所有元素 public Map.Entry&lt;K,V&gt; higherEntry(K key) 返回 Key 大于 key 的所有元素 public K higherKey(K key) 返回 Key 大于 key 的所有 Key public Map.Entry&lt;K,V&gt; lastEntry() 返回 Key 最大的元素 public K lastKey() 返回 Key 最大的 Key public Map.Entry&lt;K,V&gt; lowerEntry(K key) 返回小于 key 的最大元素 public K lowerKey(K key) 返回小于 key 最大的 Key public Map.Entry&lt;K,V&gt; pollFirstEntry() 删除 key 最小的元素 public Map.Entry&lt;K,V&gt; pollLastEntry() 删除最大 Key 的元素 public NavigableMap&lt;K,V&gt; subMap(K fromKey, boolean fromInclusive, K toKey, boolean toInclusive) 截取集合中 Key 从 fromKey 到 toKey 的元素，否是截取他们本身，取决于 true 或者 false public SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey) 截取集合中 Key 从 fromKey 到 toKey 的元素，包括 fromKey，不包括 toKey public SortedMap&lt;K,V&gt; tailMap(K fromKey) 截取 Key 大于等于 fromKey 的所有元素 public NavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive) 当 inclusive 为 true 时，截取 Key 大于等于 fromKey 的所有元素，否则截取 Key 大于 fromKey 的所有元素 Vector 常用方法Vector 类是实现 List 接口，所以继承的方法就不在这里讲了 方法 详解 public void add(int index, E element) 从 index 索引的位置添加 element 元素，后面的元素都往后移一位 public boolean addAll(int index, Collection&lt;? extends E&gt; c) 从 index 索引位置开始添加 c 集合里所有的元素，后面的元素都往后移 c.size() 位 public void addElement(E obj) 在集合后面添加一个元素，无论该元素是什么类型的，都会把他的 toString() 的返回值添加进去 public int capacity() 返回此向量的当前容量，不是元素个数 public void copyInto(Object[] anArray) 把集合中的元素复制到 anArray 数组中去 public E elementAt(int index) 返回索引位置的元素 public Enumeration&lt;E&gt; elements() 返回集合的枚举 public void ensureCapacity(int minCapacity) 增加集合的容量，如果增大的容量小于10，那么无效，也就是增大容量要是10倍数 public void insertElementAt(E obj, int index) 在指定索引位置中插入 obj 元素 public void removeAllElements() 删除集合的所有元素，并且设置容量为0，和 clear() 方法一样， clear 底层也是用 removeAllElements() 方法的 public void setSize(int newSize) 设置集合的容量大小为 newSize，如果 newSize 大于集合元素个数，那么会在后面添加 null，如果 newSize 小于集合元素个数，那么直保留 newSize 个元素 public void trimToSize() 整理集合的容量大小，如果集合元素个数等于容量大小，那么没有变化，如果集合个数小于容量大小，那么容量会设置为元素个数大小 Arrays 工具类常用方法 Arrays.asList(T… data) 返回一个受指定数组支持的固定大小的列表。 Arrays.binarySearch() 注意：在调用该方法之前，必须先调用 sort() 方法进行排序，如果数组没有排序，那么结果是不确定的，此外如果数组中包含多个指定元素，则无法保证将找到哪个元素 Arrays.binarySearch(Object[] array, Object key) 使用二分法查找数组内指定元素的索引值 Arrays.binarySearch(Object[] array, int fromIndex, int toIndex, Object obj) 使用二分法查找数组内指定范围内的指定元素的索引值 Arrays.copyOf(T[] original, int newLength) 拷贝数组，其内部调用了 System.arraycopy() 方法，从下标0开始，如果超过原数组长度，会用null进行填充 Arrays.copyOfRange(T[] original, int from, int to) 拷贝数组，指定起始位置和结束位置，如果超过原数组长度，会用null进行填充 Arrays.equals(Object[] array1, Object[] array2) 判断两个数组是否相等，实际上比较的是两个数组的哈希值，即 Arrays.hashCode(data1) == Arrays.hashCode(data2) Arrays.deepEquals(Object[] array1, Object[] array2) 判断两个多维数组是否相等，实际上比较的是两个数组的哈希值，即 Arrays.hashCode(data1) == Arrays.hashCode(data2) Arrays.fill() Arrays.fill(Object[] array, Object obj) 用指定元素填充整个数组（会替换掉数组中原来的元素） Arrays.fill(Object[] array, int fromIndex, int toIndex, Object obj) 用指定元素填充数组，从起始位置到结束位置，取头不取尾（会替换掉数组中原来的元素） Arrays.sort() Arrays.sort(Object[] array) 对数组元素进行排序（串行排序） Arrays.sort(T[] array, Comparator&lt;? super T&gt; comparator) 使用自定义比较器，对数组元素进行排序（串行排序） Arrays.sort(Object[] array, int fromIndex, int toIndex) 对数组元素的指定范围进行排序（串行排序） Arrays.sort(T[] array, int fromIndex, int toIndex, Comparator&lt;? super T&gt; c) 使用自定义比较器，对数组元素的指定范围进行排序（串行排序） Arrays.parallelSort() Arrays.parallelSort(T[] array) 对数组元素进行排序（并行排序），当数据规模较大时，会有更好的性能 Arrays.hashCode(Object[] array) 返回数组的哈希值 Arrays.deepHashCode(Object[] array) 返回多维数组的哈希值 Arrays.toString(Object[] array) 返回数组元素的字符串形式 Arrays.deepToString(Object[] array) 返回多维数组元素的字符串形式 Arrays.setAll(T[] array, IntFunction) 把 array 里的每个元素执行 IntFunction 表达式(一个 lambda 表达式) Arrays.parallelSetAll(T[] array, IntFunction) 并行地把 array 里的每个元素执行 IntFunction 表达式(一个 lambda 表达式) Arrays.spliterator(T[] array) 返回数组的分片迭代器，用于并行遍历数组 Arrays.stream(T[] array) 返回数组的流 Stream，然后我们就可以使用 Stream 相关的许多方法了]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络通信]]></title>
    <url>%2F2019%2F04%2F02%2FJava%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[Java Socket 和网络模型 Java Socket 是 JVM 通过操作系统操控 CPU、网卡与外界通信的一个组件，包括 BIO、NIO、AIO 等网络 IO 组件的底层也是 Socket。在了解 Java Socket 之前最好先了解一下网络模型的相关概念： OSI 七层模型 应用层:主机中的各个进程，提供端口号供访问 表示层:将主机中的进程传递的数据通过不同的OS编译/转换/加密处理后交给硬件(网卡) 会话层:标识相互通讯的主机间正在进行的回话，负责建立、管理、终止会话 传输层:数据脱离网卡后即进入传输层，记录了即将访问的端口号(TCP/UDP)，将数据分段 网络层:通过网线寻找目标主机(TCP/IP) 链路层:将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正。 物理层:建立、维护、断开物理连接。 TCP/IP 四层模型 主机到网络层 实际上 TCP/IP 参考模型没有真正描述这一层的实现，只是要求能够提供给其上层-网络互连层一个访问接口，以便在其上传递 IP 分组。由于这一层次未被定义，所以其具体的实现方法将随着网络类型的不同而不同。 网络互连层 网络互连层是整个 TCP/IP 协议栈的核心。它的功能是把分组发往目标网络或主机。同时，为了尽快地发送分组，可能需要沿不同的路径同时进行分组传递。因此，分组到达的顺序和发送的顺序可能不同，这就需要上层必须对分组进行排序。 网络互连层定义了分组格式和协议，即 IP 协议（Internet Protocol）。 网络互连层除了需要完成路由的功能外，也可以完成将不同类型的网络（异构网）互连的任务。除此之外，网络互连层还需要完成拥塞控制的功能。 传输层 在 TCP/IP 模型中，传输层的功能是使源端主机和目标端主机上的对等实体可以进行会话。在传输层定义了两种服务质量不同的协议。即：传输控制协议 TCP（transmission control protocol）和用户数据报协议 UDP（user datagram protocol）。 TCP 协议是一个面向连接的、可靠的协议。它将一台主机发出的字节流无差错地发往互联网上的其他主机。在发送端，它负责把上层传送下来的字节流分成报文段并传递给下层。在接收端，它负责把收到的报文进行重组后递交给上层。 TCP 协议还要处理端到端的流量控制，以避免缓慢接收的接收方没有足够的缓冲区接收发送方发送的大量数据。 UDP协议是一个不可靠的、无连接协议，主要适用于不需要对报文进行排序和流量控制的场合。 应用层 TCP/IP 模型将 OSI 参考模型中的会话层和表示层的功能合并到应用层实现。 应用层面向不同的网络应用引入了不同的应用层协议。其中，有基于 TCP 协议的，如文件传输协议（File Transfer Protocol，FTP）、虚拟终端协议（TELNET）、超文本链接协议（Hyper Text Transfer Protocol，HTTP），也有基于 UDP 协议的。 OSI 和 TCP/IP 的区别OSI和TCP/IP都是概念，TCP/IP是OSI的合并，简化 Java 网络 IO 模型 一个输入操作通常包括两个阶段： 等待数据准备好 从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 Unix 有五种 I/O 模型： 阻塞式 I/O 非阻塞式 I/O I/O 复用（select 和 poll） 信号驱动式 I/O（SIGIO） 异步 I/O（AIO） 阻塞式 I/O 应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回。 应该注意到，在阻塞的过程中，其它应用进程还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其它应用进程还可以执行，所以不消耗 CPU 时间，这种模型的 CPU 利用率效率会比较高。 下图中，recvfrom() 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。 1ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); 非阻塞式 I/O 应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）。 由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低。 I/O 复用 使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。 它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。 如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。 信号驱动 I/O 应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。 相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。 异步 I/O 应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。 异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。 五大 I/O 模型比较 同步 I/O：将数据从内核缓冲区复制到应用进程缓冲区的阶段，应用进程会阻塞。 异步 I/O：不会阻塞。 阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O 都是同步 I/O，它们的主要区别在第一个阶段。 非阻塞式 I/O 、信号驱动 I/O 和异步 I/O 在第一阶段不会阻塞。 I/O 复用select/poll/epoll 都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 select1int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); 有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。 fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义。 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 123456789101112131415161718192021222324252627282930313233343536fd_set fd_in, fd_out;struct timeval tv;// Reset the setsFD_ZERO( &amp;fd_in );FD_ZERO( &amp;fd_out );// Monitor sock1 for input eventsFD_SET( sock1, &amp;fd_in );// Monitor sock2 for output eventsFD_SET( sock2, &amp;fd_out );// Find out which socket has the largest numeric value as select requires itint largest_sock = sock1 &gt; sock2 ? sock1 : sock2;// Wait up to 10 secondstv.tv_sec = 10;tv.tv_usec = 0;// Call the selectint ret = select( largest_sock + 1, &amp;fd_in, &amp;fd_out, NULL, &amp;tv );// Check if select actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; if ( FD_ISSET( sock1, &amp;fd_in ) ) // input event on sock1 if ( FD_ISSET( sock2, &amp;fd_out ) ) // output event on sock2&#125; poll1int poll(struct pollfd *fds, unsigned int nfds, int timeout); pollfd 使用链表实现。 1234567891011121314151617181920212223242526272829// The structure for two eventsstruct pollfd fds[2];// Monitor sock1 for inputfds[0].fd = sock1;fds[0].events = POLLIN;// Monitor sock2 for outputfds[1].fd = sock2;fds[1].events = POLLOUT;// Wait 10 secondsint ret = poll( &amp;fds, 2, 10000 );// Check if poll actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; // If we detect the event, zero it out so we can reuse the structure if ( fds[0].revents &amp; POLLIN ) fds[0].revents = 0; // input event on sock1 if ( fds[1].revents &amp; POLLOUT ) fds[1].revents = 0; // output event on sock2&#125; select 和 poll 比较 功能 select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。 select 会修改描述符，而 poll 不会； select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 的描述符类型使用链表实现，没有描述符数量的限制； poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。 速度 select 和 poll 速度都比较慢。 select 和 poll 每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。 select 和 poll 的返回结果中没有声明哪些描述符已经准备好，所以如果返回值大于 0 时，应用进程都需要使用轮询的方式来找到 I/O 完成的描述符。 可移植性 几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 epoll123int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// Create the epoll descriptor. Only one is needed per app, and is used to monitor all sockets.// The function argument is ignored (it was not before, but now it is), so put your favorite number hereint pollingfd = epoll_create( 0xCAFE );if ( pollingfd &lt; 0 ) // report error// Initialize the epoll structure in case more members are added in futurestruct epoll_event ev = &#123; 0 &#125;;// Associate the connection class instance with the event. You can associate anything// you want, epoll does not use this information. We store a connection class pointer, pConnection1ev.data.ptr = pConnection1;// Monitor for input, and do not automatically rearm the descriptor after the eventev.events = EPOLLIN | EPOLLONESHOT;// Add the descriptor into the monitoring list. We can do it even if another thread is// waiting in epoll_wait - the descriptor will be properly addedif ( epoll_ctl( epollfd, EPOLL_CTL_ADD, pConnection1-&gt;getSocket(), &amp;ev ) != 0 ) // report error// Wait for up to 20 events (assuming we have added maybe 200 sockets before that it may happen)struct epoll_event pevents[ 20 ];// Wait for 10 seconds, and retrieve less than 20 epoll_event and store them into epoll_event arrayint ready = epoll_wait( pollingfd, pevents, 20, 10000 );// Check if epoll actually succeedif ( ret == -1 ) // report error and abortelse if ( ret == 0 ) // timeout; no event detectedelse&#123; // Check if any events detected for ( int i = 0; i &lt; ret; i++ ) &#123; if ( pevents[i].events &amp; EPOLLIN ) &#123; // Get back our connection pointer Connection * c = (Connection*) pevents[i].data.ptr; c-&gt;handleReadEvent(); &#125; &#125;&#125; 工作模式epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。 LT 模式 当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 ET 模式 和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。 select 应用场景 select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。 select 可移植性更好，几乎被所有主流平台所支持。 poll 应用场景 poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 epoll 应用场景 只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。 需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合框架]]></title>
    <url>%2F2019%2F04%2F02%2FJava%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[本文整理自 兰亭风雨 的专栏。 Java 集合框架Java集合框架可以大致分为五个部分：List 列表、Set 列表、Map映射、迭代器(Iterator、Enumeration)、工具类(Arrays、Conllection)。 从上图中可以看出，集合类主要分为两大类：Collection 和 Map。 Java 集合框架的组成接口Collection接口Collection 是 List、Set 等集合高度抽象出来的接口，它包含了这些集合的基本操作，它主要又分为两大部分：List 和 Set。 List接口List 接口通常表示一个列表（数组、队列、链表、栈等），其中的元素可以重复，常用实现类为 ArrayList 和 LinkedList ，另外还有不常用的 Vector。另外，LinkedList 还是实现了 Queue 接口，因此也可以作为队列使用。 Set接口Set 接口通常表示一个集合，其中的元素不允许重复（通过 hashcode 和 equals 函数保证），常用实现类有 HashSet 和 TreeSet ，HashSet 是通过 Map 中的 HashMap 实现的，而 TreeSet 是通过 Map 中的 TreeMap 实现的。另外，TreeSet 还实现了 SortedSet 接口，因此是有序的集合（集合中的元素要实现 Comparable 接口，并覆写 Compartor 函数才行）。 Map接口Map 是一个映射接口，其中的每个元素都是一个 key-value 键值对，同样抽象类 AbstractMap 通过适配器模式实现了 Map 接口中的大部分函数，TreeMap、HashMap、WeakHashMap 等实现类都通过继承 AbstractMap 来实现，另外，不常用的 HashTable 直接实现了 Map 接口，它和 Vector 都是 JDK1.0 就引入的集合类。 Iterator接口Iterator 是遍历集合的迭代器（不能遍历 Map，只用来遍历 Collection），Collection 的实现类都实现了 iterator() 函数，它返回一个 Iterator 对象，用来遍历集合，ListIterator 则专门用来遍历 List。而 Enumeration 则是 JDK1.0 时引入的，作用与 Iterator 相同，但它的功能比 Iterator 要少，它只能再 Hashtable、Vector 和 Stack 中使用。 其他工具类Arrays 和 Collections 是用来操作数组、集合的两个工具类，例如在 ArrayList 和 Vector 中大量调用了 Arrays.Copyof() 方法，而 Collections 中有很多静态方法可以返回各集合类的 synchronized 版本，即线程安全的版本，当然了，如果要用线程安全的结合类，首选 Concurrent 并发包下的对应的集合类。 抽象类我们看到，抽象类 AbstractCollection、AbstractList 和 AbstractSet 分别实现了 Collection、List 和 Set 接口，这就是在 Java 集合框架中用的很多的适配器设计模式，用这些抽象类去实现接口，在抽象类中实现接口中的若干或全部方法，这样下面的一些类只需直接继承该抽象类，并实现自己需要的方法即可，而不用实现接口中的全部抽象方法。 实体类ArrayList简介 ArrayList 是基于数组实现的，是一个动态数组，其容量能自动增长，类似于 C 语言中的动态申请内存，动态增长内存。 ArrayList 不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用 Collections.synchronizedList(List l) 函数返回一个线程安全的 ArrayList 类，也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 ArrayList 实现了 Serializable 接口，因此它支持序列化，能够通过序列化传输，实现了 RandomAccess 接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了 Cloneable 接口，能被克隆。 源码特征 ArrayList 有三个构造器 无参构造方法构造的 ArrayList 的容量默认为10 带容量大小参数的构造方法 带有 Collection 参数的构造方法，将 Collection 转化为数组赋给 ArrayList 的实现数组 elementData 扩充容量的方法 ensureCapacity ：ArrayList在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就设置新的容量为旧的容量的1.5倍加1，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后用 Arrays.copyof() 方法将元素拷贝到新的数组（详见下面的第3点）。从中可以看出，当容量不够时，每次增加元素，都要将原来的元素拷贝到一个新的数组中，非常之耗时，也因此建议在事先能确定元素数量的情况下，才使用 ArrayList，否则建议使用 LinkedList。 ArrayList 的实现中大量地调用了 Arrays.copyof() 和 System.arraycopy() 方法。 Arrays.copyof() 方法实际上是在其内部又创建了一个长度为 newlength 的数组，调用 System.arraycopy() 方法，将原来数组中的元素复制到了新的数组中。 System.arraycopy() 方法。该方法被标记了 native，调用了系统的 C/C++ 代码，在 JDK 中是看不到的，但在 openJDK 中可以看到其源码。该函数实际上最终调用了 C 语言的 memmove() 函数，因此它可以保证同一个数组内元素的正确复制和移动，比一般的复制方法的实现效率要高很多，很适合用来批量处理数组。Java 强烈推荐在复制大量数组元素时用该方法，以取得更高的效率。 注意 ArrayList 的两个转化为静态数组的 toArray 方法。 Object[] toArray() 方法。该方法有可能会抛出 java.lang.ClassCastException 异常，如果直接用向下转型的方法，将整个 ArrayList 集合转变为指定类型的 Array 数组，便会抛出该异常，而如果转化为 Array 数组时不向下转型，而是将每个元素向下转型，则不会抛出该异常，显然对数组中的元素一个个进行向下转型，效率不高，且不太方便。 &lt;T&gt; T[] toArray(T[] a) 方法。该方法可以直接将 ArrayList 转换得到的 Array 进行整体向下转型（转型其实是在该方法的源码中实现的），且从该方法的源码中可以看出，参数 a 的大小不足时，内部会调用 Arrays.copyOf 方法，该方法内部创建一个新的数组返回，因此对该方法的常用形式如下： 1234public static Integer[] vectorToArray2(ArrayList&lt;Integer&gt; v) &#123; Integer[] newText = (Integer[])v.toArray(new Integer[0]); return newText; &#125; ArrayList 基于数组实现，可以通过下标索引直接查找到指定位置的元素，因此查找效率高，但每次插入或删除元素，就要大量地移动元素，插入删除元素的效率低。 在查找给定元素索引值等的方法中，源码都将该元素的值分为 null 和不为 null 两种情况处理，ArrayList 中允许元素为 null。 LinkedList简介 LinkedList 是基于双向循环链表（从源码中可以很容易看出）实现的，除了可以当做链表来操作外，它还可以当做栈、队列和双端队列来使用。 LinkedList 同样是非线程安全的，只在单线程下适合使用。 LinkedList 实现了 Serializable 接口，因此它支持序列化，能够通过序列化传输，实现了 Cloneable 接口，能被克隆。 源码特征 LinkedList 的实现是基于双向循环链表的，且头结点中不存放数据(即使用虚拟头结点) LinkedList 有两个不同的构造方法。无参构造方法直接建立一个仅包含 head 节点的空链表，包含 Collection 的构造方法，先调用无参构造方法建立一个空链表，而后将 Collection 中的数据加入到链表的尾部后面。 在查找和删除某元素时，源码中都划分为该元素为 null 和不为 null 两种情况来处理，LinkedList 中允许元素为 null。 LinkedList 是基于链表实现的，因此不存在容量不足的问题，所以这里没有扩容的方法。 注意源码中的 Entry&lt;E&gt; entry(int index) 方法。该方法返回双向链表中指定位置处的节点，而链表中是没有下标索引的，要指定位置出的元素，就要遍历该链表，从源码的实现中，我们看到这里有一个加速动作。源码中先将 index 与长度 size 的一半比较，如果 index&lt;size/2 ，就只从位置 0 往后遍历到位置 index 处，而如果 index&gt;size/2，就只从位置 size 往前遍历到位置 index 处。这样可以减少一部分不必要的遍历，从而提高一定的效率（实际上效率还是很低）。 1234567891011121314151617// 获取双向链表中指定位置的节点 private Entry&lt;E&gt; entry(int index) &#123; if (index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException("Index: " + index + ", Size: "+size); Entry&lt;E&gt; e = header; // 获取index处的节点。 // 若index &lt; 双向链表长度的1/2,则从前先后查找; // 否则，从后向前查找。 if (index &lt; (size &gt;&gt; 1)) &#123; for (int i = 0; i &lt;= index; i++) e = e.next; &#125; else &#123; for (int i = size; i &gt; index; i--) e = e.previous; &#125; return e; &#125; 注意链表类对应的数据结构 Entry。如下： 1234567891011121314151617181920212223// 双向链表的节点所对应的数据结构。 // 包含3部分：上一节点，下一节点，当前节点值。 private static class Entry&lt;E&gt; &#123; // 当前节点所包含的值 E element; // 下一个节点 Entry&lt;E&gt; next; // 上一个节点 Entry&lt;E&gt; previous; /** * 链表节点的构造函数。 * 参数说明： * element —— 节点所包含的数据 * next —— 下一个节点 * previous —— 上一个节点 */ Entry(E element, Entry&lt;E&gt; next, Entry&lt;E&gt; previous) &#123; this.element = element; this.next = next; this.previous = previous; &#125; &#125; LinkedList 是基于链表实现的，因此插入删除效率高，查找效率低（虽然有一个加速动作）。 要注意源码中还实现了栈和队列的操作方法，因此也可以作为栈、队列和双端队列来使用。 Vector简介 Vector 也是基于数组实现的，是一个动态数组，其容量能自动增长。 Vector 是 JDK1.0 引入了，它的很多实现方法都加入了同步语句，因此是线程安全的（其实也只是相对安全，有些时候还是要加入同步语句来保证线程的安全），可以用于多线程环境。 Vector 实现了 Serializable 接口，因此它支持序列化，实现了 Cloneable 接口，能被克隆，实现了 RandomAccess 接口，支持快速随机访问。 源码特征 Vector 有四个不同的构造方法 无参构造方法的容量为默认值10 仅包含容量的构造方法则将容量增长量（从源码中可以看出容量增长量的作用，第二点也会对容量增长量详细说）明置为0。 指定 Vector “容量大小”和”增长系数”的构造函数。 指定集合的 Vector 构造函数。 注意扩充容量的方法 ensureCapacityHelper：与 ArrayList 相同，Vector 在每次增加元素（可能是1个，也可能是一组）时，都要调用该方法来确保足够的容量。当容量不足以容纳当前的元素个数时，就先看构造方法中传入的容量增长量参数 CapacityIncrement 是否为0，如果不为0，就设置新的容量为就容量加上容量增长量，如果为0，就设置新的容量为旧的容量的2倍，如果设置后的新容量还不够，则直接新容量设置为传入的参数（也就是所需的容量），而后同样用 Arrays.copyof() 方法将元素拷贝到新的数组。 很多方法都加入了 synchronized 同步语句，来保证线程安全。 同样在查找给定元素索引值等的方法中，源码都将该元素的值分为 null 和不为 null 两种情况处理，Vector 中也允许元素为 null。 其他很多地方都与 ArrayList 实现大同小异，Vector 现在已经基本不再使用。 HashMap简介 HashMap 是基于哈希表实现的，每一个元素是一个 key-value 对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 HashMap 是非线程安全的，只是用于单线程环境下，多线程环境下可以采用 concurrent 并发包下的 concurrentHashMap。 HashMap 实现了 Serializable 接口，因此它支持序列化，实现了 Cloneable 接口，能被克隆。 源码特征 首先要清楚HashMap的存储结构，如下图所示： 图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 首先看链表中节点的数据结构： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括"哈希值(h)", "键(k)", "值(v)", "下一节点(n)" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; 它的结构元素除了 key、value、hash 外，还有 next ，next 指向下一个节点。另外，这里覆写了 equals 和 hashCode 方法来保证键值对的独一无二。 HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响 HashMap 性能的重要参数。 容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16）。 无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方。 加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。 如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 HashMap 中 key 和 value 都允许为 null。 要重点分析下 HashMap 中用的最多的两个方法 put 和 get。先从比较简单的 get 方法着手，源码如下： 12345678910111213141516171819202122232425262728 // 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125;//没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; 首先，如果 key 为 null，则直接从哈希表的第一个位置 table[0] 对应的链表上查找。记住，key 为 null 的键值对永远都放在以 table[0] 为头结点的链表中，当然不一定是存放在头结点 table[0] 中。 如果 key 不为 null，则先求的 key 的 hash 值，根据 hash 值找到在 table 中的索引，在该索引对应的单链表中查找是否有键值对的 key 与目标 key 相等，有就返回对应的 value，没有则返回 null。 put方法稍微复杂些，代码如下： 12345678910111213141516171819202122232425 // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++;//将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 如果key为null，则将其添加到table[0]对应的链表中，putForNullKey的源码如下： 123456789101112131415// putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; 如果 key 不为 null，则同样先求出 key 的 hash 值，根据 hash 值得出在 table 中的索引，而后遍历对应的单链表，如果单链表中存在与目标 key 相等的键值对，则将新的 value 覆盖旧的 value，比将旧的 value 返回，如果找不到与目标 key 相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有 addEntry 方法实现的，它的源码如下： 1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; 注意这里倒数第三行的构造方法，将 key-value 键值对赋给 table[bucketIndex]，并将其 next 指向元素 e，这便将 key-value 放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次 put 键值对的时候，总是将新的该键值对放在 table[bucketIndex] 处（即头结点处）。 另外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 扩容 resize 方法： 12345678910111213141516// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; ​ 很明显，是新建了一个 HashMap 的底层数组，而后调用 transfer 方法，将就 HashMap 的全部元素添加到新的 HashMap 中（要重新计算元素在新的数组中的索引位置）。transfer 方法的源码如下： 123456789101112131415161718// 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用 HashMap 的时，最好能提前预估下 HashMap 中元素的个数，这样有助于提高 HashMap 的性能。 注意 containsKey 方法和 containsValue 方法。前者直接可以通过 key 的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。 我们重点来分析下求 hash 值和索引值的方法，这两个方法便是 HashMap 设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。 计算哈希值的方法如下： 1234static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; 它只是一个数学公式，JDK 这样设计对 hash 值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使 hash 值的计算效率很高。 由 hash 值找到对应索引的方法如下： 123static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; 这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用 hash 值对 length 取模（即除法散列法），Hashtable 中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap 中则通过 h&amp;(length-1) 的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是 HashMap 对 Hashtable 的一个改进。 为什么哈希表的容量一定要是2的整数次幂? length 为2的整数次幂的话，h&amp;(length-1) 就相当于对 length 取模，这样便保证了散列的均匀，同时也提升了效率 length 为2的整数次幂的话，为偶数，这样 length-1 为奇数，奇数的最后一位是1，这样便保证了 h&amp;(length-1) 的最后一位可能为0，也可能为1（这取决于 h 的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果 length 为奇数的话，很明显 length-1 为偶数，它的最后一位是0，这样 h&amp;(length-1) 的最后一位肯定为0，即只能为偶数，这样任何 hash 值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间 因此，length 取2的整数次幂，是为了使不同 hash 值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。 Hashtable简介 Hashtable 同样是基于哈希表实现的，同样每个元素是一个 key-value 对，其内部也是通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 Hashtable 也是 JDK1.0 引入的类，是线程安全的，能用于多线程环境中。 Hashtable 同样实现了 Serializable 接口，它支持序列化，实现了 Cloneable 接口，能被克隆。 源码特征 针对 Hashtable，我们同样给出几点比较重要的总结，但要结合与 HashMap 的比较来总结。 二者的存储结构和解决冲突的方法都是相同的。 HashTable 在不指定容量的情况下的默认容量为11，而 HashMap 为16，Hashtable 不要求底层数组的容量一定要为2的整数次幂，而 HashMap 则要求一定为2的整数次幂。 Hashtable 中 key 和 value 都不允许为 null，而 HashMap 中 key 和 value 都允许为 null（key 只能有一个为 null，而 value 则可以有多个为 null）。但是如果在 Hashtable 中有类似 put(null,null) 的操作，编译同样可以通过，因为 key 和 value 都是 Object 类型，但运行时会抛出 NullPointerException 异常，这是 JDK 的规范规定的。我们来看下 ContainsKey 方法和 ContainsValue 的源码： 12345678910111213141516171819202122232425262728293031323334353637383940 // 判断Hashtable是否包含“值(value)” public synchronized boolean contains(Object value) &#123; //注意，Hashtable中的value不能是null， // 若是null的话，抛出异常! if (value == null) &#123; throw new NullPointerException(); &#125; // 从后向前遍历table数组中的元素(Entry) // 对于每个Entry(单向链表)，逐个遍历，判断节点的值是否等于value Entry tab[] = table; for (int i = tab.length ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; e = tab[i] ; e != null ; e = e.next) &#123; if (e.value.equals(value)) &#123; return true; &#125; &#125; &#125; return false; &#125; public boolean containsValue(Object value) &#123; return contains(value); &#125; // 判断Hashtable是否包含key public synchronized boolean containsKey(Object key) &#123; Entry tab[] = table; //计算hash值，直接用key的hashCode代替 int hash = key.hashCode(); // 计算在数组中的索引值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return true; &#125; &#125; return false; &#125; 很明显，如果 value 为 null，会直接抛出 NullPointerException 异常，但源码中并没有对 key 是否为 null 判断，有点小不解！不过 NullPointerException 属于 RuntimeException 异常，是可以由 JVM 自动抛出的，也许对 key 的值在 JVM 中有所限制吧。 Hashtable 扩容时，将容量变为原来的2倍加1，而 HashMap 扩容时，将容量变为原来的2倍。 Hashtable 计算 hash 值，直接用 key 的 hashCode()，而 HashMap 重新计算了 key 的 hash 值，Hashtable 在求 hash 值对应的位置索引时，用取模运算，而 HashMap 在求位置索引时，则用与运算，且这里一般先用hash&amp;0x7FFFFFFF 后，再对 length 取模，&amp;0x7FFFFFFF 的目的是为了将负的hash值转化为正值，因为 hash值有可能为负数，而 &amp;0x7FFFFFFF 后，只有符号外改变，而后面的位都不变。 TreeMap简介 TreeMap是基于红黑树实现的，这里只对红黑树做个简单的介绍，红黑树是一种特殊的二叉排序树，红黑树通过一些限制，使其不会出现二叉树排序树中极端的一边倒的情况，相对二叉排序树而言，这自然提高了查询的效率。 源码特征 存储结构：TreeMap的排序是基于对key的排序实现的，它的每一个Entry代表红黑树的一个节点，Entry的数据结构如下： 1234567891011121314151617181920212223 static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; // 键 K key; // 值 V value; // 左孩子 Entry&lt;K,V&gt; left = null; // 右孩子 Entry&lt;K,V&gt; right = null; // 父节点 Entry&lt;K,V&gt; parent; // 当前节点颜色 boolean color = BLACK; // 构造函数 Entry(K key, V value, Entry&lt;K,V&gt; parent) &#123; this.key = key; this.value = value; this.parent = parent; &#125; 。。。。。。 &#125; TreeMap一共有4个构造方法： 无参构造方法 123public TreeMap() &#123; comparator = null; &#125; 采用无参构造方法，不指定比较器，这时候，排序的实现要依赖 key.compareTo() 方法，因此 key 必须实现 Comparable 接口，并覆写其中的 compareTo 方法。 带有比较器的构造方法 123public TreeMap(Comparator&lt;? super K&gt; comparator) &#123; this.comparator = comparator; &#125; 采用带比较器的构造方法，这时候，排序依赖该比较器，key 可以不用实现 Comparable 接口。 带 Map 的构造方法 1234public TreeMap(Map&lt;? extends K, ? extends V&gt; m) &#123; comparator = null; putAll(m); &#125; 该构造方法同样不指定比较器，调用 putAll 方法将 Map 中的所有元素加入到 TreeMap 中。putAll 的源码如下： 123456789101112131415161718192021222324// 将map中的全部节点添加到TreeMap中 public void putAll(Map&lt;? extends K, ? extends V&gt; map) &#123; // 获取map的大小 int mapSize = map.size(); // 如果TreeMap的大小是0,且map的大小不是0,且map是已排序的“key-value对” if (size==0 &amp;&amp; mapSize!=0 &amp;&amp; map instanceof SortedMap) &#123; Comparator c = ((SortedMap)map).comparator(); // 如果TreeMap和map的比较器相等； // 则将map的元素全部拷贝到TreeMap中，然后返回！ if (c == comparator || (c != null &amp;&amp; c.equals(comparator))) &#123; ++modCount; try &#123; buildFromSorted(mapSize, map.entrySet().iterator(), null, null); &#125; catch (java.io.IOException cannotHappen) &#123; &#125; catch (ClassNotFoundException cannotHappen) &#123; &#125; return; &#125; &#125; // 调用AbstractMap中的putAll(); // AbstractMap中的putAll()又会调用到TreeMap的put() super.putAll(map); &#125; 显然，如果 Map 里的元素是排好序的，就调用 buildFromSorted 方法来拷贝 Map 中的元素，这在下一个构造方法中会重点提及，而如果 Map 中的元素不是排好序的，就调用 AbstractMap 的 putAll(map) 方法，该方法源码如下： 1234public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) put(e.getKey(), e.getValue()); &#125; 很明显它是将 Map 中的元素一个个 put（插入）到 TreeMap 中的，主要因为 Map 中的元素是无序存放的，因此要一个个插入到红黑树中，使其有序存放，并满足红黑树的性质。 带有 SortedMap 的构造方法 12345678public TreeMap(SortedMap&lt;K, ? extends V&gt; m) &#123; comparator = m.comparator(); try &#123; buildFromSorted(m.size(), m.entrySet().iterator(), null, null); &#125; catch (java.io.IOException cannotHappen) &#123; &#125; catch (ClassNotFoundException cannotHappen) &#123; &#125; &#125; 首先将比较器指定为 m 的比较器，这取决于生成 m 时调用构造方法是否传入了指定的构造器，而后调用 buildFromSorted 方法，将 SortedMap 中的元素插入到 TreeMap 中，由于 SortedMap 中的元素师有序的，实际上它是根据 SortedMap 创建的 TreeMap，将 SortedMap 中对应的元素添加到 TreeMap 中。 插入删除 插入 put 插入操作即对应 TreeMap 的 put 方法，put 操作实际上只需按照二叉排序树的插入步骤来操作即可，插入到指定位置后，再做调整，使其保持红黑树的特性。put 源码的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 public V put(K key, V value) &#123; Entry&lt;K,V&gt; t = root; // 若红黑树为空，则插入根节点 if (t == null) &#123; // TBD: // 5045147: (coll) Adding null to an empty TreeSet should // throw NullPointerException // // compare(key, key); // type check root = new Entry&lt;K,V&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; // 找出(key, value)在二叉排序树中的插入位置。 // 红黑树是以key来进行排序的，所以这里以key来进行查找。 if (cpr != null) &#123; do &#123; parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; else &#123; if (key == null) throw new NullPointerException(); Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do &#123; parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; // 为（key-value）新建节点 Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; // 插入新的节点后，调用fixAfterInsertion调整红黑树。 fixAfterInsertion(e); size++; modCount++; return null; &#125; 删除 deleteEntry 删除操作及对应 TreeMap 的 deleteEntry 方法，deleteEntry 方法同样也只需按照二叉排序树的操作步骤实现即可，删除指定节点后，再对树进行调整即可。deleteEntry 方法的实现源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142// 删除“红黑树的节点p” private void deleteEntry(Entry&lt;K,V&gt; p) &#123; modCount++; size--; if (p.left != null &amp;&amp; p.right != null) &#123; Entry&lt;K,V&gt; s = successor (p); p.key = s.key; p.value = s.value; p = s; &#125; Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); if (replacement != null) &#123; replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; p.left = p.right = p.parent = null; if (p.color == BLACK) fixAfterDeletion(replacement); &#125; else if (p.parent == null) &#123; root = null; &#125; else &#123; if (p.color == BLACK) fixAfterDeletion(p); if (p.parent != null) &#123; if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; &#125; &#125; &#125; TreeMap 和 HashMap 的比较 TreeMap 是根据 key 进行排序的，它的排序和定位需要依赖比较器或覆写 Comparable 接口，也因此不需要 key 覆写 hashCode 方法和 equals 方法，就可以排除掉重复的 key，而 HashMap 的 key 则需要通过覆写 hashCode 方法和 equals 方法来确保没有重复的 key。 TreeMap 的查询、插入、删除效率均没有 HashMap 高，一般只有要对 key 排序时才使用 TreeMap。 TreeMap 的 key 不能为 null，而 HashMap 的 key 可以为 null。 注：对 TreeSet 和 HashSet 的源码不再进行剖析，二者分别是基于 TreeMap 和 HashMap 实现的，只是对应的节点中只有 key，而没有 value，因此对 TreeMap 和 HashMap 比较了解的话，对 TreeSet 和 HashSet 的理解就会非常容易。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的Object类]]></title>
    <url>%2F2019%2F03%2F25%2FJava%E7%9A%84Object%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[参考 JDK1.8 的 API 文档 Object 类Object 类在 JDK1.8 的 API 中是这么定义的： public class Object Class Object is the root of the class hierarchy. Every class has Object as a superclass. All objects, including arrays, implement the methods of this class. Object 类的方法 Method Description protected Object clone() 创造并返回这个对象的复制 boolean equals(Object obj) 指示某个其他对象是否“等于”此对象 protected void finalize() 当垃圾回收确定没有对该对象的引用时，由对象上的垃圾收集器调用 Class&lt;?&gt; getClass() 返回这个对象的运行时类 int hashCode() 返回这个对象的哈希码 void notify() 唤醒在该对象监视器上的一个单线程 void notifyAll() 唤醒在该对象监视器上的所有线程 String toString() 返回该对象的字符串表示 void wait() 导致当前线程进入等待，直到另一个线程调用这个对象的 notify() 或者 notifyAll() 方法 void wait(long timeout) 导致当前线程进入等待，直到另一个线程调用这个对象的 notify() 或者 notifyAll() 方法，或者超过了指定的时间 void wait(long timeout, int nanos) 导致当前线程进入等待，直到另一个线程调用这个对象的 notify() 或者 notifyAll() 方法，或者某个其他线程中断当前线程，或者超过了指定的时间 方法详解getClass1public final Class&lt;?&gt; getClass() 返回此 Object 运行时类的 Class 对象。返回的 Class 对象是由所表示类的 static synchronized 方法锁定的对象。 hashCode1public int hashCode() 返回该对象的哈希码值。支持此方法是为了提高哈希表（例如 java.util.Hashtable 提供的哈希表）的性能。 hashCode 的常规协定是： 在 Java 应用程序执行期间，在对同一对象多次调用 hashCode 方法时，必须一致地返回相同的整数，前提是将对象进行 equals 比较时所用的信息没有被修改。从某一应用程序的一次执行到同一应用程序的另一次执行，该整数无需保持一致。 如果根据 equals(Object) 方法，两个对象是相等的，那么对这两个对象中的每个对象调用 hashCode 方法都必须生成相同的整数结果。 如果根据 equals(java.lang.Object) 方法，两个对象不相等，那么对这两个对象中的任一对象上调用 hashCode 方法不 要求一定生成不同的整数结果。但是，程序员应该意识到，为不相等的对象生成不同整数结果可以提高哈希表的性能。 equals1public boolean equals(Object obj) 指示其他某个对象是否与此对象“相等”。如果此对象与 obj 参数相同，则返回 true；否则返回 false。 equals 方法在非空对象引用上实现相等关系： 自反性：对于任何非空引用值 x，x.equals(x) 都应返回 true。 对称性：对于任何非空引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 才应返回 true。 传递性：对于任何非空引用值 x、y 和 z，如果 x.equals(y) 返回 true，并且 y.equals(z) 返回 true，那么 x.equals(z) 应返回 true。 一致性：对于任何非空引用值 x 和 y，多次调用 x.equals(y) 始终返回 true 或始终返回 false，前提是对象上 equals 比较中所用的信息没有被修改。 对于任何非空引用值 x，x.equals(null) 都应返回 false。 该方法比较的是引用，对于任何非空引用值 x 和 y，当且仅当 x 和 y 引用同一个对象时，此方法才返回 true（x == y 具有值 true）。 很多类对该方法进行了重写，以实现比较对象实际内容的功能，这里需要注意的是：当此方法被重写时，通常有必要重写 hashCode 方法，以维护 hashCode 方法的常规协定，该协定声明相等对象必须具有相等的哈希码。 clone1protected Object clone() throws CloneNotSupportedException 创建并返回此对象的一个复制。“复制”的准确含义可能取决于对象的类。 如果对象的类不支持 Cloneable 接口，则重写 clone 方法的子类也会抛出此异常，以指示无法复制某个实例。 该方法执行的是对象的”浅拷贝”，即对象本身没有被复制。 Object 类本身不实现接口 Cloneable，所以在类为 Object 的对象上调用 clone 方法将会导致在运行时抛出异常。但所有的数组都被视为实现接口 Cloneable。 toString1public String toString() 返回该对象的字符串表示。通常， toString 方法会返回一个“以文本方式表示”此对象的字符串。结果应是一个简明但易于读懂的信息表达式。建议所有子类都重写此方法。 Object 类的 toString 方法返回一个字符串，该字符串由类名（对象是该类的一个实例）、at 标记符“@”和此对象哈希码的无符号十六进制表示组成。换句话说，该方法返回一个字符串，它的值等于： 1getClass().getName() + '@' + Integer.toHexString(hashCode()) notify1public final void notify() 唤醒在此对象监视器上等待的单个线程。如果所有线程都在此对象上等待，则会选择唤醒其中一个线程。选择是任意性的，并在对实现做出决定时发生。 此方法只应由作为此对象监视器的所有者的线程来调用。 如果当前线程不是此对象监视器的所有者，抛出 IllegalMonitorStateException 异常。 notifyAll1public final void notifyAll() 唤醒在此对象监视器上等待的所有线程。 此方法只应由作为此对象监视器的所有者的线程来调用。 如果当前线程不是此对象监视器的所有者，抛出 IllegalMonitorStateException 异常。 wait1public final void wait(long timeout) throws InterruptedException 在其他线程调用此对象的 notify() 方法或 notifyAll() 方法前，导致当前线程等待。 此方法只应由作为此对象监视器的所有者的线程来调用。 抛出： IllegalMonitorStateException - 如果当前线程不是此对象监视器的所有者。 InterruptedException - 如果在当前线程等待通知之前或者正在等待通知时，任何线程中断了当前线程。在抛出此异常时，当前线程的中断状态被清除。 1public final void wait(long timeout) throws InterruptedException 在其他线程调用此对象的 notify() 方法或 notifyAll() 方法前，或者超过指定的时间量前，导致当前线程等待。 此方法只应由作为此对象监视器的所有者的线程来调用。 参数： timeout - 要等待的最长时间（以毫秒为单位）。 抛出： IllegalArgumentException - 如果超时值为负。 IllegalMonitorStateException - 如果当前线程不是此对象监视器的所有者。 InterruptedException - 如果在当前线程等待通知之前或者正在等待通知时，任何线程中断了当前线程。在抛出此异常时，当前线程的中断状态被清除。 1public final void wait(long timeout, int nanos) throws InterruptedException 在其他线程调用此对象的 notify() 方法或 notifyAll() 方法前，或者其他某个线程中断当前线程，或者超过指定的时间量前，导致当前线程等待。 此方法只应由作为此对象监视器的所有者的线程来调用。 参数： timeout - 要等待的最长时间（以毫秒为单位）。 nanos - 额外时间（以毫微秒为单位，范围是 0-999999）。 抛出： IllegalArgumentException - 如果超时值是负数，或者毫微秒值不在 0-999999 范围内。 IllegalMonitorStateException - 如果当前线程不是此对象监视器的所有者。 InterruptedException - 如果在当前线程等待通知之前或者正在等待通知时，任何线程中断了当前线程。在抛出此异常时，当前线程的中断状态被清除。 finalize1protected void finalize() throws Throwable 当垃圾回收器确定不存在对该对象的引用时，由对象的垃圾回收器调用此方法。子类重写 finalize 方法，以配置系统资源或执行其他清除。 对于任何给定对象，Java 虚拟机最多只调用一次 finalize 方法。 finalize 方法抛出的任何异常都会导致此对象的终结操作停止，但此外的都会被忽略。 Throwable - 此方法抛出的 Exception]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（七）]]></title>
    <url>%2F2019%2F03%2F20%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%83%EF%BC%89%2F</url>
    <content type="text"><![CDATA[文件系统文件系统和文件 文件系统是操作系统中管理持久性数据的子系统，提供数据存储和访问功能 组织、检索、读写访问数据 大多数计算机系统都有文件系统 Google 也是一个文件系统 文件是具有符号名，由字节序列构成的数据项集合 文件系统的基本数据单位 文件名是文件的标识符号 文件系统的功能 分配文件磁盘空间 管理文件块(位置和顺序) 管理空闲空间(位置) 分配算法(策略) 管理文件集合 定位：文件及其内容 命名：通过名字找到文件 文件系统结构：文件组织方式 数据可靠和安全 安全：多层次保护数据安全 可靠 持久保存文件 避免系统崩溃、媒体错误、攻击等 文件属性 名称、类型、位置、大小、保护、创建者、创建时间、最近修改时间，… 文件头：文件系统元数据中的文件信息 文件属性 文件存储位置和顺序 打开文件 文件访问模式 进程访问文件数据前必须先”打开”文件 内核跟踪进程打开的所有文件 操作系统为每个进程维护一个打开文件表 文件描述符是打开文件的标识 文件描述符 操作系统在打开文件表中维护的打开文件状态和信息 文件指针 最近一次读写位置 每个进程分别维护自己的打开文件指针 文件打开计数 当前打开文件的次数 最后一个进程关闭文件时，将其从打开文件表中移除 文件的磁盘位置 缓存数据访问信息 访问权限 每个进程的文件访问模式信息 文件的用户视图和系统视图 文件的用户视图 持久的数据结构 系统访问接口 字节序列的集合( UNIX ) 系统不关心存储在磁盘上的数据结构 操作系统的文件视图 数据块的集合 数据块是逻辑存储单元，而扇区是物理存储单元 块大小 &lt; &gt; 扇区大小 用户视图到系统视图的转换 进程读文件 获取字节所在的数据块 返回数据块内对应部分 进程写文件 获取数据块 修改数据块中对应部分 写回数据块 文件系统中的基本操作单位是数据块 例如，getc() 和 putc() 即使每次只访问1字节的数据，也需要缓存目标数据4096字节 访问模式 操作系统需要了解进程如何访问文件 顺序访问：按字节依次读取 大多数的文件访问都是顺序访问 随机访问：从中间读写 不常用，但仍然重要 例如：虚拟内存中把内存页存储在文件 索引访问：依据数据特征索引 通常操作系统不完整提供索引访问 数据库是建立在索引内容的磁盘访问上 文件内部结构 无结构：单词、字节序列 简单记录结构 分列 固定长度 可变长度 复杂结构 格式化的文档(如，MS Word，PDF) 可执行文件 … 文件共享和访问控制 多用户系统中的文件共享是很必要的 访问控制 每个用户能够获得哪些文件的哪些访问权限 访问模式：读、写、执行、删除、列表等 文件访问控制列表( ACL )：&lt;文件实体，权限&gt; Unix 模式 &lt;用户|组|所有人，读|写|可执行&gt; 用户标识 ID ：标识用户，表明每个用户所允许的权限以及保护模式 组标识 ID ：允许用户组成组，并指定了组访问权限 语义一致性 规定多进程如何同时访问共享文件 与同步算法类似 因磁盘 I/O 和网络延迟而设计简单 Unix 文件系统( UFS )语义 对打开文件的写入内容立即对其他打开同一文件的其他用户可见 共享文件指针允许多用户同时读取和写入文件 会话语义：写入内容只有当文件关闭时可见 读写锁：一些操作系统和文件系统提供该功能 分层文件系统 文件以目录的方式组织起来 目录是一类特殊的文件 目录的内容是文件索引表&lt;文件名，指向文件的指针&gt; 目录和文件的树形结构 早起的文件系统是扁平的(只有一层目录) 目录 目录操作 典型目录操作 搜索文件 创建文件 删除文件 列目录 重命名文件 遍历路径 操作系统应该只允许内核修改目录 确保映射的完整性 应用程序通过系统调用访问目录 目录实现 文件名的线性列表，包涵了指向数据块的指针 编程简单 执行耗时 哈希表：哈希数据结构的线性表 减少目录搜索时间 冲突 - 两个文件名的哈希值相同 固定大小 文件别名 两个或多个文件名关联同一个文件 硬链接：多个文件项指向一个文件 软链接：以”快捷方式”指向其他文件 通过存储真实文件的逻辑名称来实现 文件目录中的循环 如何保证没有循环? 只允许到文件的链接，不允许在子目录的链接 增加链接时，用循环检测算法确定是否合理 更多实践 限制路径可遍历文件目录的数量 名字解析(路径遍历) 名字解析：把逻辑名字转换成物理资源(如文件) 依据路径名，在文件系统中找到实际文件位置 遍历文件目录直到找到目标文件 举例：解析”/bin/ls” 读取根目录的文件头(在磁盘固定位置) 读取根目录的数据块，搜索 “bin” 项 读取 bin 的文件头 读取 bin 的数据块；搜索 “ls” 项 读取 ls 大的文件头 当前工作目录( PWD ) 每个进程都会指向一个文件目录用于解析文件名 允许用户指定相对路径来代替绝对路径 如，用 PWD = “/bin” 能够解析 “ls” 文件系统挂载 文件系统需要先挂载才能被访问 未挂载的文件系统被挂载在挂载点上 文件系统种类 磁盘文件系统 文件存储在数据存储设备上，如磁盘 例如：FAT，NTFS，ext2/3，ISO9660等 数据库文件系统 文件特征是可被寻址(辨识)的 例如：WinFS 日志文件系统 记录文件系统的修改/事件 网络/分布式文件系统 例如：NFS，SMB，AFS，GFS 文件可以通过网络被共享 文件位于远程服务器 客户端远程挂载服务器文件系统 标准系统文件访问被转换成远程访问 标准文件共享协议：NFS for Unix，CIFS for Windows 分布式文件系统的挑战 客户端和客户端上的用户辨别起来很复杂 例如：NFS 是不安全的 一致性问题 错误处理模式 特殊/虚拟文件系统 虚拟文件系统 虚拟文件系统的实现 分层结构 虚拟(逻辑)文件系统( VFS，Virtual File System ) 特定文件系统模块 虚拟文件系统( VFS ) 目的：对所有不同文件系统的抽象 功能 提供相同的文件和文件系统接口 管理所有文件和文件系统关联的数据结构 高效查询例程，遍历文件系统 与特定文件系统模块的交互 文件系统基本数据结构 文件卷控制块( Unix：”superblock“ ) 每个文件系统一个 文件系统详细信息 块、块大小、空余块、计数/指针等 文件控制块( Unix：”vnode” or “inode“ ) 每个文件一个 文件详细信息 访问权限、拥有者、大小、数据块位置等 目录项( Linux：”dentry” ) 每个目录项一个(目录和文件) 将目录项数据结构及树形布局编码成树形数据结构 指向文件控制块、父目录、子目录等 文件系统的存储结构 文件系统数据结构 卷控制块(每个文件系统一个) 文件控制块(每个文件一个) 目录节点(每个目录项一个) 持久存储在外存中：存储设备的数据块中 当需要时加载进内存 卷控制模块：当文件系统挂载时进入内存 文件控制块：当文件被访问时进入内存 目录节点：在遍历一个文件路径时进入内存 文件系统的存储视图 文件缓存 多种磁盘缓存位置 数据块缓存 数据块按需读入内存 提供 read() 操作 预读：预先读取后面的数据块 数据块使用后被缓存 假设数据将会再次用到 写操作可能被缓存和延迟写入 两种数据块缓存方式 数据块缓存 页缓存：同一缓存数据块和内存页 页缓存 虚拟页式存储：在虚拟地址空间中虚拟页面可映射到本地外存文件中 文件数据块的页缓存 在虚拟内存中文件数据块被映射成页 文件的读/写操作被转换成对内存的访问 可能导致缺页和/或设置为脏页 问题：页置换算法需要协调虚拟存储和页缓存间的页面数 文件系统中打开文件的数据结构 文件描述符 每个被打开的文件都有一个文件描述符 文件状态信息：目录项、当前文件指针、文件操作设置等 打开文件表 每个进程有一个进程打开文件表 一个系统级的打开文件表 有文件被打开时，文件卷就不能被卸载 打开文件锁 一些文件系统提供文件锁，用于协调多线程的文件访问 强制：根据锁保持情况和访问需求确定是否拒绝访问 劝告：进程可以查找锁的状态来决定怎么做 文件分配 文件大小 大多数文件都很小 需要对小文件提供很好的支持 块空间不能太大 一些文件非常大 必须支持大文件(64位文件偏移) 大文件访问需要高效 文件分配 如何表示分配给一个文件数据块的位置和顺序 分配方式 连续分配 链式分配 索引分配 指标 存储效率：外部碎片等 读写性能：访问速度 连续分配 文件头指定起始块和长度 分配策略：最先匹配，最佳匹配，… 优点 文件读取表现好 高效的顺序和随机访问 缺点 碎片！ 文件增长问题(预分配？按需分配？) 链式分配 文件以数据块链表方式存储 文件头包含了到第一块和最后一块的指针 优点 创建、增大、缩小很容易 没有碎片 缺点 无法实现真正的随机访问 可靠性差：破坏一个链，后面的数据块就丢了 索引分配 为每个文件创建一个索引数据块：指向文件数据块的指针列表 文件头包含了索引数据块指针 优点 创建、增大、缩小很容易 没有碎片 支持直接访问 缺点 当文件很小时，存储索引的开销 如何处理大文件？ 大文件的索引分配 链式索引块( IB + IB + … ) 多级索引块( IB IB … ) UFS 多级索引分配 文件头包含13个指针 10个指针指向数据块 第11个指针指向索引快 第12个指针指向二级索引块 第13个指针指向三级索引块 效果 提高了文件大小限制阈值 动态分配数据块，文件扩展很容易 小文件开销小 只为大文件分配间接数据块，大文件在访问数据块时需要大量查询 空闲空间管理 跟踪记录文件卷中未分配的数据块 采用什么数据结构表示空闲空间列表？ 空闲空间组织：位图 用位图代表空闲数据块列表 11111111111111111111100111010101110011111….. Di = 0 表明数据块 i 是空闲，否则，表示已分配 使用简单但是可能会是一个很大的向量表 160 GB 磁盘 -&gt; 40 M 数据块 -&gt; 5 MB 位图 假定空闲空间在磁盘中均匀分布，则找到”0”之前要扫面 n/r n = 磁盘上数据块的总数 r = 空闲块的数目 其余空闲空间组织方式 冗余磁盘阵列 RAID 磁盘分区 通常磁盘通过分区来最大限度减小寻道时间 分区是一组柱面的集合 每个分区都可视为逻辑上独立的磁盘 一个典型的磁盘文件系统组织 文件卷：一个拥有完整文件系统实例的外存空间，通常常驻在磁盘的单个分区上 多磁盘管理 使用多磁盘可改善 吞吐量(通过并行) 可靠性和可用性(通过冗余) 冗余磁盘阵列( RAID，Redundant Array of Inexpensive Disks ) 多种磁盘管理技术 RAID 分类，如：RAID - 0，RAID - 1，RAID - 5 冗余磁盘阵列的实现 软件：操作系统内核的文件卷管理 硬件：RAID 硬件控制器( I/O ) RAID - 0：磁盘条带化 把数据块分成多个子块，存储在独立的磁盘中 通过独立磁盘上并行数据块访问提供更大的磁盘带宽 RAID - 1：磁盘镜像 向两个磁盘写入，从任何一个读取 可靠性成倍增长 读取性能线性增长 RAID - 4：带校验的磁盘条带化 数据块级的磁盘条带化加专用奇偶校验磁盘 允许从任意一个故障磁盘中恢复 RAID - 5：带分布式校验的磁盘条带化 基于位和基于块的磁盘条带化 条带化和奇偶校验按”字节“或者”位“ RAID - 0/4/5：基于数据块 RAID - 3：基于位 可纠正多个磁盘错误的冗余磁盘阵列 RAID - 5：每组条带块有一个奇偶校验块 允许一个磁盘错误 RAID - 6：每组条带块有两个冗余块 允许两个磁盘错误 RAID 嵌套 RAID 0 + 1 RAID 1 + 0 I/O 子系统三种常见设备接口类型 字符设备：如，键盘/鼠标，串口等 访问特征：以字节为单位顺序访问 I/O 命令 get()、put()等 通常使用文件访问接口和语义 块设备：如，磁盘驱动器，磁带驱动器，光驱等 访问特征：均匀的数据块访问 I/O 命令 原始 I/O 或文件系统接口 内存映射文件访问 网络设备：如，以太网，无线，蓝牙等 访问特征：格式化报文交换 I/O 命令 send/receive 网络报文 通过网络接口支持多种网络协议 同步和异步 I/O 阻塞 I/O ：”Wait“ 读数据( read )时，进程将进入等待状态，直到完成数据读出 写数据( write )时，进程将进入等待状态，直到设备完成数据写入处理 非阻塞 I/O ：”Don’t Wait“ 立即从 read 或 write 系统调用返回，返回值为成功传输字节数 read 或 write 的传输字节数可能为零 异步 I/O ：”Tell Me Later“ 读数据时，使用指针标记好用户缓冲区，立即返回；稍后内核将填充缓冲区并通知用户 写数据时，使用指针标记好用户缓冲区，立即返回；稍后内核将处理数据并通知用户 I/O 结构 一个实际例子 CPU 与设备的连接 补充：设备控制器 CPU 和 I/O 设备间的接口 向 CPU 提供特殊指令和寄存器 I/O 指令和内存映射 I/O I/O 指令 通过 I/O 端口号访问设备寄存器 特殊的 CPU 指令 out 0x21,AL 内存映射 I/O 设备的寄存器/存储被映射到内存物理地址空间中 通过内存 load/store 指令完成 I/O 操作 MMU 设置映射，硬件跳线或程序在启动时设置地址 内核 I/O 结构 I/O 请求生存周期 I/O 数据传输 CPU 与设备控制器的数据传输 程序控制 I/O (PIO，Programmed I/O ) 通过 CPU 的 in/out 或者 load/store 传输所有数据 特点 硬件简单，编程容易 消耗的 CPU 时间和数据量成正比 适用于简单的、小型的设备 I/O 直接内存访问( DMA ) 设备控制器可直接访问系统总线 控制器直接与内存互相传输数据 特点 设备传输数据不影响 CPU 需要 CPU 参与设置 适用于高吞吐量 I/O 通过直接 I/O 寻址读取磁盘数据的步骤 I/O 设备通知操作系统的机制 操作系统需要了解设备状态 I/O 操作完成时间 I/O 操作遇到错误 两种方式 CPU 主动轮询 设备中断 轮询 I/O 设备在特定的状态寄存器中放置状态和错误信息 操作系统定期检测状态寄存器 特点 简单 I/O 操作频繁或不可预测时，开销大和延时长 设备中断 设备中断处理流程 CPU 在 I/O 之前设置任务参数 CPU 发出 I/O 请求后，继续执行其他任务 I/O 设备处理 I/O 请求 I/O 设备处理完成时，触发 CPU 中断请求 CPU 接收中断，分发到相应中断处理例程 特点 处理不可预测事件效果好 开销相对较高 一些设备可能结合了轮询和设备中断 如：高宽带网络设备 第一个传入数据包到达前采用中断 轮询后面的数据包直到硬件缓存为空 设备中断 I/O 处理流程 第6步CPU 恢复被中断进程的执行后可能又会调用另一个 I/O 然后又回到第1步 磁盘调度 磁盘工作机制和性能参数 读取或写入时，磁头必须被定位在期望的磁道，并从所期望的柱面和扇区开始 寻道时间：定位到期望的磁道所花费的时间 旋转延迟：从零扇区开始处到达目的地花费的时间 平均旋转延迟时间 = 磁盘旋转一周时间的一半 磁盘 I/O 传输时间 Ta ：访问时间 Ts ：寻道时间 1/2r ：旋转延迟，1/r = 旋转一周的时间 b/rN：传输时间，b = 传输的比特数；N = 磁道上的比特数；r = 磁盘转速 磁盘调度算法 通过优化磁盘访问请求顺序来提高磁盘访问性能 寻道时间是磁盘访问最耗时的部分 同时会有多个在同一磁盘上的 I/O 请求 随机处理磁盘访问请求的性能表现很差 先进先出( FIFO )算法 按顺序处理请求 公平对待所有进程 在有很多进程的情况下，接近随机调度的性能 示例 最短服务时间优先( SSTF ) 选择从磁臂当前位置需要移动最少的 I/O 请求 总是选择最短寻道时间 示例 扫描算法( SCAN ) 磁臂在一个方向上移动，访问所有未完成的请求，直到磁臂到达该方向上最后的磁道 调换方向 也称为电梯算法( elevator algorithm ) 示例 循环扫描算法( C - SCAN ) 限制了仅在一个方向上扫描 当最后一个磁道也被访问过了后，磁臂返回到磁盘的另外一端再次进行 C - LOOK 算法 磁臂先到达该方向上最后一个请求处，然后立即反转，而不是先到最后点路径上的所有请求 N 步扫描( N - step - SCAN )算法 磁头粘着( Arm Stickiness )现象 SSTF，SCAN 及 CSCAN 等算法中，可能出现磁头停留在某处不动的情况 如：进程反复请求对某一磁道的 I/O 操作 N 步扫描算法 将磁盘请求队列分成长度为 N 的子队列 按 FIFO 算法依次处理所有子队列 扫描算法处理每个队列 双队列扫描( FSCAN )算法 FSCAN 算法是 N 步扫描算法的简化 FSCAN 只将磁盘请求队列分成两个子队列 FSCAN 算法 把磁盘 I/O 请求分成两个队列 交替使用扫描算法处理一个队列 新生成的磁盘 I/O 请求放入另一队列中，所有的新请求都将被推迟到下一次扫描的处理 磁盘缓存 缓存：数据传输双方访问速度差异较大时，引入的速度匹配中间层 磁盘缓存是磁盘扇区在内存中的缓存区 磁盘缓存的调度算法很类似虚拟存储调度算法 磁盘的访问频率远低于虚拟存储中的内存访问频率 通常磁盘缓存调度算法会比虚拟存储复杂 单缓存与双缓存 访问频率置换算法( Frequency-based Replacement ) 问题：在一段密集磁盘访问后，LFU 算法的引用计数变化无法反映当前的引用情况 算法思路 考虑磁盘访问的密集特征，对密集引用不计数 在短周期中使用 LRU 算法，而在长周期中使用 LFU 算法 具体做法 把 LRU 算法中的特殊栈分成三部分，并在每个缓存块增加一个引用计数 新区域( New Section ) 中间区域( Middle Section ) 旧区域( Old Section ) 栈中缓存块被访问时移到栈顶；如果该块在新区域，引用计数不变；否则，引用计数加1 在新区域中引用计数不变的目的是避免密集访问对引用计数不利影响 在中间区域和旧区域中引用计数加1是为了使用 LFU 算法 未缓存数据块读入后放在栈顶，引用计数为1 在旧区域中引用计数最小的缓存块被置换 中间区域的定义是为了避免新读入的缓存块在第一次出新区域时马上被置换掉，有一个过渡期]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（六）]]></title>
    <url>%2F2019%2F03%2F20%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%85%AD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[信号量与管程信号量( semaphore ) 信号量是操作系统提供的一种协调共享资源访问的方法 软件同步是平等线程间的一种同步协商机制 信号量是和锁同级的一种实现同步的高级抽象方法 OS 是管理者，地位高于进程 用信号量表示系统资源的数量 早期的操作系统的主要同步机制 现在很少用(但还是非常重要在计算机科学研究) 信号量是一种抽象数据类型 由一个整形( sem )变量和两个原子操作组成 P() ( Prolaag (荷兰语：尝试减少)) sem 减1 如 sem &lt; 0，进入等待，否则继续 V() ( Verhoog (荷兰语：增加)) sem 加1 如 sem &lt;= 0，唤醒一个等待进程 信号量的特征 信号量是被保护的整数变量 初始化完成后，只能通过 P() 和 V() 操作修改 由操作系统保证，PV 操作是原子操作 P() 可能阻塞，V() 不会阻塞 通常假定信号量是”公平的” 线程不会被无限期阻塞在 P() 操作 假定信号量等待按先进先出排队 信号量的实现1234567891011121314151617181920classSemaphore &#123; int sem; WaitQueue q;&#125;Semaphore::P() &#123; sem --; if (sem &lt; 0) &#123; Add this thread t to q; block(p); &#125;&#125;Semaphore::V() &#123; sem ++; if (sem &lt;= 0) &#123; Remove a thread t from q; wakeup(t); &#125;&#125; 信号量使用 信号量分类：可分为两种信号量 二进制信号量：资源数目为0或1 资源信号量：资源数目为任何非负值 两者等价：基于一个可以实现另一个 信号量的使用 互斥访问：临界区的互斥访问控制 条件同步：线程间的事件等待 用信号量实现临界区的互斥访问 每类资源设置一个信号量，其初值为1 1mutex = new Semaphore(1); 123mutex-&gt;P();Critical Section;mutex-&gt;V(); 必须成对使用 P() 操作和 V() 操作 P() 操作保证互斥访问临界资源 V() 操作在使用后释放临界资源 PV 操作不能次序错误、重复或遗漏 用信号量实现条件同步 每个条件同步设置一个信号量，其初值为0 1condition = new Semaphore(0); 生产者-消费者问题 有界缓冲区的生产者-消费者问题描述 一个或多个生产者在生成数据后放在一个缓冲区里 单个消费者从缓冲区取出数据处理 任何时刻只能有一个生产者或消费者可访问缓冲区 用信号量解决生产者-消费者问题 问题分析 任何时刻只能有一个线程操作缓存区(互斥访问) 缓冲区空时，消费者必须等待生产者(条件同步) 缓冲区满时，生产和必须等待消费者(条件同步) 用信号量描述每个约束 二进制信号量 mutex 资源信号量 fullBuffers 资源信号量 emptyBuffers 实现 12345678910111213141516171819202122// 初始化Class BoundedBuffer &#123; mutex = new Semaphore(1); fullBuffers = new Semaphore(0); emptyBuffers = new Semaphore(n);&#125;// 生产者BoundedBuffer::Deposit(c) &#123; emptyBuffers-&gt;P(); mutex-&gt;P(); Add c to the buffer; mutex-&gt;V(); fullBuffers-&gt;V();&#125;// 消费者BoundedBuffer::Remove(c) &#123; fullBuffers-&gt;P(); mutex-&gt;P(); Remove c from the buffer; mutex-&gt;V(); emptyBuffers-&gt;V();&#125; 如果 P、V 操作顺序有误，会造成死锁 使用信号量的困难 读/开发代码比较困难 程序员需要能运用信号量机制 容易出错 使用的信号量已经被另一个线程占用 忘记释放信号量 不能够处理死锁问题 管程( Moniter ) 管程是一种用于多线程互斥访问共享资源的程序结构 采用面向对象方法，简化了线程间的同步控制 任一时刻最多只有一个线程执行管程代码 正在管程中的线程可临时放弃管程的互斥访问，等待事件出现时恢复 管程的使用 在对象/模块中，收集相关共享数据 定义访问共享数据的方法 管程的组成 一个锁 控制管程代码的互斥访问 0 或者多个条件变量 管理共享数据的并发访问 条件变量( Condition Variable ) 条件变量是管程内的等待机制 进入管程的线程因资源被占用而进入等待状态 每个条件变量表示一种等待原因，对应一个等待队列 Wait() 操作 将自己阻塞在等待队列中 唤醒一个等待者或释放管程的互斥访问 Signal() 操作 将等待队列中的一个线程唤醒 如果等待队列为空，则等同空操作 条件变量的实现1234567891011121314151617181920Class Condition &#123; int numWaiting = 0; WaitQueue q;&#125;Condition::Wait(lock) &#123; numWaiting++; Add this thread t to q; release(lock)； schedule(); // need mutex require(lock);&#125;Condition::Signal() &#123; if (numWaiting &gt; 0) &#123; Remove a thread t from q; wakeup(t); //need mutex numWaiting--; &#125;&#125; 用管程解决生产者-消费者问题 123456789101112131415161718192021222324252627// 初始化classBoundedBuffer &#123; ... Lock lock; int count = 0; Condition notFull, notEmpty;&#125;// 生产者BoundedBuffer::Deposit(c) &#123; lock-&gt;Acquire(); while (count == n) notFull.Wait(&amp;lock); Add c to the buffer; count++; notEmpty.Signal(); lock-&gt;Release();&#125;// 消费者BoundedBuffer::Remove(c) &#123; lock-&gt;Acquire(); while (count == 0) notEmpty.Wait(&amp;lock); Remove c from the buffer; count--; notFull.Signal(); lock-&gt;Release();&#125; 管程条件变量的释放处理方式 Hansen 的连续执行效率更高，如图少了一次切换 Hansen 管程：主要用于真实 OS 和 Java 中 Hoare 管程：主要见于教材中 Hansen 管程与 Hoare 管程 以生产者-消费者问题的生产者代码为例 Hansen 管程 条件变量释放仅是一个提示 需要重新检查条件 特点：高效 Hoare 管程 条件变量释放同时表示放弃管程访问 释放后条件变量的状态可用 特点：低效 经典同步问题之哲学家就餐问题 问题描述 5个哲学家围绕一张圆桌而坐 桌子上放着5支叉子 每两个哲学家之间放一支 哲学家的动作包括思考和进餐 进餐时需同时拿到左右两边的叉子 思考时将两支叉子放回原处 如何保证哲学家们的动作有序进行？ 如：不出现有人永远拿不到叉子 方案一 不正确，可能导致死锁 方案二 互斥访问正确，但每次只允许一人进餐 方案三 没有死锁，可有多人同时进餐 经典同步问题之读者-写者问题 问题描述 共享数据的两类使用者 读者：只读取数据，不修改 写者：读取和修改数据 读者-写者问题描述：对共享数据的读写 “读 - 读”允许 同一时刻，允许有多个读者同时读 “读 - 写”互斥 没有写者时读者才能读 没有读者时写者才能写 “写 - 写”互斥 没有其他写者时写者才能写 用信号量解决读者-写者问题 用信号量描述每个约束 信号量 WriteMutex 控制读写操作的互斥 初始化为1 读者技术 Rcount 正在进行读操作的读者数目 初始化为0 信号量 CountMutex 控制对读者计数的互斥修改 初始化为1 此实现中，读者优先 读者-写者问题：优先策略 读者优先策略 只要有读者正在读状态，后来的读者都能直接进入 如读者持续不断进入，则写者就处于饥饿 写者优先策略 只要有写者就绪，写者应尽快执行写操作 如写者持续不断就绪，则读者就处于饥饿 用管程解决读者-写者问题 两个基本方法 1234567891011Database::Read() &#123; Wait until no writer; read database; check out - wake up waiting writers;&#125;Database::Write() &#123; Wait until no readers/writers; write database; check out - wake up waiting readers/writers;&#125; 管程的状态变量 解决方案详情：读者 这里 while() 里的条件设置的是有写者在写或有写者申请写就等待，体现的是写者优先 这里 if() 里的条件设置的是最后一个读者看有没有写者，如果有写者等着，就释放 解决方案详情：写者 这里 while() 里的条件设置的是有写者在写或有读者在读就等待，但是如果继续有读者等着就不管了，所以体现的是写者优先 这里 if() 里的条件设置的是如果有等待写的就优先唤醒，没有的话才唤醒读者 死锁和进程通信死锁 定义：由于竞争资源或者通信关系，两个或更多线程在执行中出现，永远相互等待只能由其他进程引发的事件 死锁示例：单向通行桥梁 桥梁只能单向通行 桥的每个部分可视为一个资源 可能出现死锁 对向行驶车辆在桥中相遇 解决办法：一个方向的车辆倒退(资源抢占和回退) 可能发生饥饿 由于一个方向的持续车流，另一个方向的车辆无法通过桥梁 进程访问资源的流程 资源类型 R1，R2，…，Rm CPU 执行时间、内存空间、I/O 设备等 每类资源 Ri 有 Wi 个实例 进程访问资源的流程 请求/获取：申请空闲资源 使用/占用：进程占用资源 释放：资源状态由占有变成空闲 资源 资源分类 可重用资源( Reusable Resource ) 资源不能被删除且在任何时刻只能有一个进程使用 进程释放资源后，其他进程可重用 可重用资源示例 硬件：处理器，I/O 通道，主和副存储器，设备等 软件：文件，数据库和信号量等数据结构 可能出现死锁 每个进程占用一部分资源并请求其他资源 消耗资源( Consumable Resource ) 资源创建和销毁 消耗资源示例 在 I/O 缓冲区的中断、信号、消息等 可能出现死锁 进程间相互等待接受对方的消息 资源分配图 描述资源和进程间的分配和占用关系的有向图 两类顶点 系统中的所有进程P = {P1，P2，…，Pn} 系统中的所有资源R = {R1，R2，…，Rm} 两类有向边 资源请求边 进程 Pi 请求资源 Rj ：Pi -&gt; Rj 资源分配边 资源 Rj 已分配给进程 Pi ：Rj -&gt; Pi 出现死锁的必要条件 互斥 任何时刻只能有一个进程使用一个资源实例 持有并等待 进程保持至少一个资源，并正在等待获取其他进程持有的资源 非抢占 资源只能在进程使用后自愿释放 循环等待 存在等待进程集合{P0，P1，…，PN}，P0 正在等待 P1 所占用的资源，P1 正在等待 P2 所占用的资源，…，PN-1 正在等待 PN 所占用的资源，PN 正在等待 P0 所占用的资源 现学现用 死锁处理方法 死锁预防( Deadlock Prevention )：确保系统永远不会进入死锁状态 死锁避免( Deadlock Avoidance )：在使用前进行判断，只允许不会出现死锁的进程请求资源 死锁检测和恢复( Deadlock Detection &amp; Recovery )：在检测到运行系统进入死锁状态后，进行恢复 由应用进程处理死锁 通常操作系统忽略死锁：大多数操作系统(包括 UNIX )的做法 死锁预防 预防是采用某种策略，限制并发进程对资源的请求，使系统在任何时刻都不满足死锁的必要条件 互斥：把互斥的共享资源封装成可同时访问 持有并等待 进程请求资源时，要求它不持有任何其他资源 仅允许进程在开始执行时，一次请求所有需要的资源 资源利用率低 非抢占 如进程请求不能立即分配的资源，则释放已占用资源 只在能够同时获得所有需要资源时，才执行分配操作 循环等待 对资源排序，要求进程按顺序请求资源 死锁避免 方法一：利用额外的先验信息，在分配资源时判断是否会出现死锁，只在不会死锁时分配资源 要求进程声明需要资源的最大数目 限定提供与分配的资源数量，确保满足进程的最大需求 动态检查资源分配状态，确保不会出现环形等待 方法二：当进程请求资源时，系统判断分配后是否处于安全状态 系统处于安全状态 针对所有已占用进程，存在安全序列 序列 &lt;P1，P2，…，PN&gt; 是安全的 Pi 要求的资源 &lt;= 当前可用资源 + 所有 Pj 持有资源其中 j &lt; i 如 Pi 的资源请求不能立即分配，则 Pi 等待所有 Pj ( j &lt; i )完成 Pi 完成后，Pi + 1可得到所需资源，执行并释放所分配的资源 最终整个序列的所有 Pi 都能获得所需资源 安全状态与死锁的关系 系统处于安全状态，一定没有死锁 系统处于不安全状态，可能出现死锁 避免死锁就是确保系统不会进入不安全状态 银行家算法( Banker’s Algorithm ) 银行家算法是一个避免死锁产生的算法。以银行借贷分配策略为基础，判断并保证系统处于安全状态 客户在第一次申请贷款时，声明所需最大资金量，在满足所有贷款要求并完成项目时，及时归还 在客户贷款数量不超过银行拥有的最大值时，银行家尽量满足客户需求 类比 银行家 ↔ 操作系统 资金 ↔ 资源 客户 ↔ 申请资源的线程 银行家算法：数据结构 n = 线程数量，m = 资源类型数量 Max (总需求量)：n * m 矩阵 线程 Ti 最多请求类型 Rj 的资源 Max[i, j] 个实例 Available (剩余空闲量)：长度为 m 的向量 当前有 Available[i] 个类型 Rj 的资源实例可用 Allocation (已分配量)：n * m 矩阵 线程 Ti 当前分配了Allocation[i, j] 个 Rj 的实例 Need (未来需要量)：n * m 矩阵 线程 Ti 未来需要 Need[i, j] 个 Rj 资源实例 Need[i, j] = Max[i, j] - Allocation[i, j] 银行家算法核心：安全状态判断 Work 和 Finish 分别是长度为 m 和 n 的向量初始化 12Work = Available //当前资源剩余空闲量Finish[i] = false for i : 1,2, …, n. //线程 i 没结束 寻找线程 Ti 满足 12Finish[i] = false //接下来找出 Need 比 Work 小的线程 iNeed[i] &lt;= Work 没有找到满足条件的 Ti ，转4 改成 true 表示线程结束 12Work = Work + Allocation[i] //线程 i 的资源需求量小于当前剩余空闲资源量，所以配置给它再回收Finish[i] = true 转2 如所有线程 Ti 满足 Finish[i] == true，则系统处于安全状态 银行家算法 初始化 Requesti 线程 Ti 的资源请求向量 Requesti[j] 线程 Ti 请求资源 Rj 的实例 循环 如果 Requesti &lt;= Need[i]，转到步骤2。否则，拒绝资源申请，因为线程已经超过了其最大要求 如果 Requesti &lt;= Available，转到步骤3。否则，Ti 必须等待，因为资源不可用 通过安全状态判断来确定是否分配资源给 Ti ： 生成一个需要判断状态是否安全的资源分配环境123Available = Available - Requesti;Allocation[i] = Allocation[i] + Requesti;Need[i] = Need[i] - Requesti; 调用安全状态判断 如果返回结果是安全，将资源分配给 Ti 如果返回结果是不安全，系统会拒绝 Ti 的资源请求 死锁检测和恢复 死锁检测简述 允许系统进入死锁状态 维护系统的资源分配图 定期调用死锁检测算法来搜索图中是否存在死锁 出现死锁时，用死锁恢复机制进行恢复 死锁检测算法：数据结构 Available：长度为 m 的向量 每种类型可用资源的数量 Allocation：一个 n * m 矩阵 当前分配给各个进程每种类型资源的数量 进程 Pi 拥有资源 Ri 的 Allocation[i, j] 个实例 死锁检测算法 Work 和 Finish 分别是长度为 m 和 n 的向量初始化 Work = Available //work 为当前空闲资源量 Allocation[i] &gt; 0 时，Finish[i] = false; 否则，Finish[i] = true; //finish 为线程是否结束 寻找线程 Ti 满足 Finish[i] = false //线程没有结束，且此线程将需要的资源量小于当前空闲资源量 Requesti &lt;= Work没有找到这样的 i ，转到4 Work = Work + Allocation[i] //把找到的线程拥有的资源释放到当前空闲资源中Finish[i] = true转到2 如果某个 Finish[i] == false，系统处于死锁状态算法需要O(m*n2)操作检测系统是否处于死锁状态 死锁检测算法的使用 死锁检测的时间和周期选择依据 死锁多久可能会发生 多少进程需要被回滚 资源图可能有多个循环 难以分辨”造成”死锁的关键进程 死锁恢复：进程终止 终止所有的死锁进程 一次只终止一个进程直到死锁清除 终止进程的顺序应该是 进程的优先级 进程已运行时间以及还需运行时间 进程已占用资源 进程完成需要的资源 终止进程数目 进程是交互还是批处理 死锁恢复：资源抢占 选择被抢占进程 最小成本目标 进程回退 返回到一些安全状态，重启进程到安全状态 可能出现饥饿 同一进程可能一直被选作被抢占者 进程通信( IPC，Inter-Process Communication ) 进程通信是进程进行通信和同步的机制 IPC 提供2个基本操作 发送操作：send(message) 接收操作：receive(message) 进程通信流程 在通信进程间建立通信链路 通过 send/receive 交换信息 进程链路特征 物理(如：共享内存，硬件总线) 逻辑(如：逻辑属性) 通信方式 直接通信 进程必须正确的命名对方 send(P, message)：发送信息到进程 P receive(Q, message)：从进程 Q 接受消息 通信链路的属性 自动建立链路 一条链路恰好对应一对通信进程 每对进程之间只有一个链接存在 链接可以是单向的，但通常为双向的 间接通信 通过操作系统维护的消息队列实现进程间的消息接收和发送 每个消息队列都有一个唯一的标识 只有共享了相同消息队列的进程，才能够通信 通信链路的属性 只有共享了相同消息队列的进程，才建立连接 连接可以是单向或双向 消息队列可以与多个进程相关联 每对进程可以共享多个消息队列 通信流程 创建一个新的消息队列 通过消息队列发送和接收消息 销毁消息队列 基本通信操作 send(A, message)：发送消息到队列 A receive(A, message)：从队列 A 接收消息 阻塞与非阻塞通信 进程通信可划分为阻塞(同步)或非阻塞(异步) 阻塞通信 阻塞发送：发送者在发送消息后进入等待，直到接受者成功收到 阻塞接收：接收者在请求接收消息后进入等待，直到成功收到一个消息 非阻塞通信 非阻塞发送：发送者在消息发送后，可立即进行其他操作 非阻塞接收：没有消息发送时，接受者在请求接收消息后，接收不到任何消息 通信链路缓冲 进程发送的消息在链路上可能有3种缓冲方式 0容量：发送方必须等待接收方 有限容量：通信链路缓冲队列满时，发送方必须等待 无限容量：发送方不需要等待 信号( Signal ) 信号 进程间的软件中断通知和处理机制 如：SIGKILL，SIGSTOP，SIGCONT等 信号的接收处理 捕获( Catch )：执行进程指定的信号处理函数被调用 忽略( Ignore )：执行操作系统指定的缺省操作 例如：进程终止、进程挂起等 屏蔽( Mask )：禁止进程接收和处理信号 可能是暂时的(当处理同样类型的信号) 不足 传送的信息量小，只有一个信号类型 信号的实现 信号使用示例 管道( pipe ) 进程间基于内存文件的通信机制 子进程从父进程继承文件描述符 缺省文件描述符：0 stdin，1 stdout，2 stderr 进程不知道(或不关心！)的另一端 可能从键盘、文件、程序读取 可能写入到终端、文件、程序 与管道相关的系统调用 读管道：read(fd, buffer, nbytes) scanf() 是基于它实现的 写管道：write(fd, buffer, nbytes) printf() 是基于它实现的 创建管道：pipe(rgfd) rgfd 是2个文件描述符组成的数组 rgfd[0] 是读文件描述符 rgfd[1] 是写文件描述符 管道示例 % ls | more shell 创建管道 为 ls 创建一个进程，设置 stdout 为管道写端 为 more 创建一个进程，设置 stdin 为管道读端 消息队列 消息队列是由操作系统维护的以字节序列为基本单位的间接通信机制 每个消息( Message )是一个字节序列 相同标识的消息组成按先进先出顺序组成一个消息队列( Message Queues ) 消息队列的系统调用 msgget ( key, flags )：获取消息队列标识 msgsnd ( QID, buf, size, flags )：发送消息 msgrcv ( QID, buf, size, type, flags )：接收消息 msgctl ( … )：消息队列控制 共享内存 共享内存是把同一个物理内存区域同时映射到多个进程的内存地址空间的通信机制 进程 每个进程都有私有内存地址空间 每个进程的内存地址空间需明确设置共享内存段 线程 同一进程中的线程总是共享相同的内存地址空间 优点：快捷、方便地共享数据 不足：必须用额外的同步机制来协调数据访问 共享内存的实现 最快的方法 一个进程写另外一个进程立即可见 没有系统调用干预 没有数据复制 不提供同步：由程序员提供同步 共享内存系统调用 shmget ( key, size, flags )：创建共享段 shmat ( shmid, *shmaddr, flags )：把共享段映射到进程地址空间 shmdt ( *shmaddr )：取消共享段到进程地址空间的映射 shmctl ( … )：共享段控制 需要信号量等机制协调共享内存的访问冲突]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（五）]]></title>
    <url>%2F2019%2F03%2F20%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%94%EF%BC%89%2F</url>
    <content type="text"><![CDATA[处理机调度CPU 资源的时分复用 进程切换：CPU 资源的当前占用者切换 保存当前进程在 PCB 中的执行上下文(CPU状态) 恢复下一个进程的执行上下文 处理及调度 从就绪队列中挑选下一个占用 CPU 运行的进程 从多个可用 CPU 中挑选就绪进程可使用的 CPU 资源 调度程序：挑选就绪进程的内核函数 调度策略：依据什么原则挑选进程/线程？ 调度时机：什么时候进行调度？ 调度时机在进程/线程的生命周期中的什么时候进行调度？ 内核运行调度程序的条件 进程从运行状态切换到等待状态 进程被终结了 非抢占系统 当前进程主动放弃 CPU 时 可抢占系统 中断请求被服务例程响应完成时 当前进程被抢占 进程时间片用完 进程从等待切换到就绪 调度策略 调度策略：确定如何从就绪队列中选择下一个执行进程 调度策略要解决的问题 挑选就绪队列中的哪一个进程？ 通过什么样的准则来选择？ 调度算法：在调度程序中实现的调度策略 比较调度算法的准则：哪一个策略/算法较好？ 处理机资源的使用模式 进程在 CPU 计算和 I/O 操作间交替 每次调度决定在下一个 CPU 计算时将哪个工作交给 CPU 在时间片机制下，进程可能在结束当前 CPU 计算前被迫放弃 CPU 比较调度算法的准则 CPU 使用率：CPU 处于忙状态的时间百分比 吞吐量：单位时间内完成的进程数量 周转时间：进程从初始化到结束(包括等待)的总时间 等待时间：进程在就绪队列中的总时间 响应时间：从提交请求到产生响应所花费的总时间 处理机调度策略的响应时间目标 减少响应时间 及时处理用户的输入请求，尽快将输出反馈给用户 减少平均响应时间的波动 在交互系统中，可预测性比高差异的平均更重要 低延迟调度改善了用户的交互体验 如果移动鼠标时，屏幕中的光标没动，用户可能会重启电脑 响应时间是操作系统的计算延迟 处理机调度策略的吞吐量目标 增加吞吐量 减少开销(操作系统开销，上下文切换) 系统资源的高效利用( CPU，I/O 设备 ) 减少等待时间 减少每个进程的等待时间 操作系统需要保证吞吐量不受用户交互的影响 操作系统必须不时进行调度，即使存在许多交互任务 吞吐量是操作系统的计算带宽 处理机调度的公平性目标 公平的定义 保证每个进程占用相同的 CPU 时间 保证每个进程的等到时间相同 公平通常会增加平均响应时间 调度算法 先来先服务算法 FCFS：First Come，First Served 短进程优先算法 SPN：Shortest Process Next SJF：Shortest Job First (短作业优先算法) SRT：Shortest Remaining Time(短剩余时间优先算法) 最高响应比优先算法 HRRN：Highest Response Ratio Next 时间片轮转算法 RR：Round Robin 多级反馈队列算法 MFQ：Multilevel Feedback Queues 公平共享调度算法 FSS：Fair Share Scheduling 先来先服务算法( First Come First Served，FCFS 依据进程进入就绪状态的先后顺序排列 进程进入等待或结束状态时，就绪队列中的下一个进程占用 CPU FCFS 算法的周转时间 示例：3个进程，计算时间分别为12，3，3 FCFS 算法的特征 优点：简单 缺点 平均等待时间波动较大 短进程可能排在长进程后面 I/O 资源和 CPU 资源的利用率较低 CPU 密集型进程会导致 I/O 设备闲置时 I/O 密集型进程也等待 短进程优先算法( SPN ) 选择就绪队列中执行时间最短进程占用 CPU 进入运行状态 就绪队列按预期的执行时间来排序 短剩余时间优先算法( SRT ) SPN 算法的可抢占改进 短进程优先算法具有最优平均周转时间 SPN 算法中一组进程的平均周转时间 修改顺序后平均等待时间更大 SPN 的特征 优点：具有最优平均周转时间 缺点 可能导致饥饿：连续的短进程流会使长进程无法获得 CPU 资源 需要预知未来 如何预估下一个 CPU 计算的持续时间？ 简单的解决办法：询问用户 用户欺骗就杀死相应进程 用户不知道怎么办？ SPN 的执行时间预估 用历史的执行时间来预估未来的执行时间 最高响应比优先算法( HRRN ) 选择就绪队列中响应比 R 值最高的进程 R = (w+s)/s w：等待时间(waiting time) s：执行时间(service time 特征 在短进程优先算法的基础上改进 不可抢占 关注进程的等待时间 防止无限期推迟 时间片轮转算法(RR，Round-Robin) 时间片：分配处理机资源的基本时间单元 算法思路 时间片结束后，按 FCFS 算法切换到下一个就绪进程 每隔 ( n - 1 ) 个时间片进程执行一个时间片 q 时间片为20的 RR 算法示例 4个进程的执行时间如下： P1：53；P2：8；P3：68；P4：24； 甘特图 等待时间 P1 = ( 68 - 20 ) + ( 112 - 88 ) = 72 P2 = ( 20 - 0 ) = 20 P3 = ( 28 - 0 ) + ( 88 - 48 ) + ( 125 - 108 ) = 85 P4 = ( 48 - 0 ) + ( 108 - 68 ) = 88 平均等待时间 = ( 72 + 20 + 85 + 88 ) / 4 = 66.25 RR 算法的时间片长度 RR 算法开销：额外的上下文切换 时间片太大 等待时间过长 极限情况退化成 FCFS 时间片太小 反应迅速，但产生大量上下文切换 大量上下文切换开销影响到系统吞吐量 时间片长度选择目标 选择一个合适的时间片长度 经验规则：维持上下文切换开销处于 1% 以内 比较 FCFS 和 RR 示例：4个进程的执行时间如下： P1：53；P2：8；P3：68；P4：24； 假设上下文切换时间为0，FCFS 和 RR 各自的平均等待时间是多少？ 这里两个极端中的 BestFCFS 即短进程优先算法，WorstFCFS 即长进程优先算法。表中可以看出，FCFS 抖动较大，而 RR 相对稳定 多级队列调度算法( MQ ) 就绪队列被划分成多个独立的子序列 如：前台(交互)、后台(批处理) 每个队列拥有自己的调度策略 如：前台-RR，后台-FCFS 队列间的调度 固定优先级 先处理前台，然后处理后台 可能导致饥饿 时间片轮转 每个队列都得到一个确定的能够调度其进程的 CPU 总时间 如：80% CPU 时间用于前台，20% CPU 时间用于后台 多级反馈队列算法( MLFQ ) 进程可在不同队列间移动的多级队列算法 时间片大小随优先级级别增加而增加 如进程在当前的时间片没有完成，则降到下一个优先级 MLFQ 算法的特征 CPU 密集型进程的优先级下降很快 I/O 密集型进程停留在高优先级 公平分享调度(FSS，Fair Share Scheduling) FSS 控制用户对系统资源的访问 一些用户组比其他用户组更重要 保证不重要的组无法垄断资源 未使用的资源按比例分配 没有达到资源使用率目标的组获得更高的优先级 传统调度算法总结 先来先服务算法 不公平，平均等待时间较差 短进程优先算法 不公平，平均周转时间最小 需要精准预测计算时间 可能导致饥饿 最高响应比优先算法 基于 SPN 调度 不可抢占 时间片轮转算法 公平，但是平均等待时间较差 多级反馈队列 多种算法的集成 公平共享调度 公平是第一要素 实时操作系统 实时操作系统的定义 正确性依赖于其时间和功能两方面的操作系统 实时操作系统的性能指标 时间约束的及时性( deadlines ) 速度和平均性能相对不重要 实时系统的特性 时间约束的可预测性 实时任务 任务(工作单元) 一次计算，一次文件读取，一次信息传递等等 任务属性 完成任务所需要的资源 定时参数 周期实时任务：一系列相似的任务 任务有规律地重复 周期 p = 任务请求时间间隔(0 &lt; p) 执行时间 e = 最大执行时间(0 &lt; e &lt; p) 使用率 U = e / p 软时限和硬时限 硬时限( Hard deadline ) 错过任务时限会导致灾难性或非常严重的后果 必须验证，在最坏情况下能够满足时限 软时限( Soft deadline ) 通常能满足任务实现 如有时不能满足，则降低要求 尽力保证满足任务时限 可调度性 可调度表示一个实时操作系统能够满足任务时限要求 需要确定实时任务的执行顺序 静态优先级调度 动态优先级调度 实时调度 速率单调调度算法( RM，Rate Monotonic) 通过周期安排优先级 周期越短优先级越高 执行周期最短的任务 最早截止时间优先算法( EDF，Earliest Deadline First ) 截止时间越早优先级越高 执行截止时间最早的任务 多处理器调度 多处理机调度的特征 多个处理机组成一个多处理机系统 处理机间可负载共享 对称多处理器( SMP，Symmetric multiprocessing )调度 截止时间越早优先级越高每个处理器运行自己的调度程序 调度程序对共享资源的访问需要进行同步 对称多处理器的进程分配 静态进程分配 进程从开始到结束都被分配到一个固定的处理机上执行 每个处理机有自己的就绪队列 调度开销小 各处理机可能忙闲不均 动态进程分配 进程在执行中可分配到任意空闲处理机执行 所有处理机共享一个公共的就绪队列 调度开销大 各处理机的负载时均衡的 优先级反置( Priority Inversion ) 操作系统中出现高优先级进程长时间等待低优先级进程所占用资源的现象 基于优先级的可抢占调度算法存在优先级反置 解决方法 优先级继承( Priority Inheritance ) 占用资源的低优先级进程继承申请资源的高优先级进程的优先级 只在占有资源的低优先级进程被阻塞时，才提高占有资源进程的优先级 优先级天花板协议( Priority Ceiling Protocol ) 占用资源进程的优先级和所有可能申请该资源的进程的最高优先级相同 不管是否发生等待，都提升占用资源进程的优先级 优先级高于系统中所有被锁定的资源的优先级上限，任务执行临界区时就不会被阻塞 同步互斥并发进程的正确性 独立进程 不和其他进程共享资源或状态 确定性：输入状态决定结果 可重现：能够重现起始条件 调度顺序不重要 并发进程 在多个进程间有资源共享 不确定性 不可重现 并发进程的正确性 执行过程是不确定性和不可重现的 程序错误可能是间歇性发生的 进程并发执行的好处 进程需要与计算机中的其他进程和设备进行协作 好处1：共享资源 多个用户使用同一台计算机 银行账号存款余额在多台 ATM 机操作 机器人上的嵌入式系统协调手臂和手的动作 好处2：加速 I/O 操作和 CPU 计算机可以重叠(并行) 程序可划分成多个模块放在多个处理器上并行执行 好处3：模块化 将大程序分解成小程序 以编译为例，gcc 会调用 cpp，cc1，cc2，as，Id 使系统易于复用和扩展 并发创建新进程时的标识分配 程序可以调用函数 fork() 来创建一个新的进程 操作系统需要分配一个新的并且唯一的进程 ID 在内核中，这个系统调用会运行 1new_pid = next_pid++ 翻译成机器指令 1234LOAD next_pid Reg1STORE Reg1 new_pidINC Reg1STORE Reg1 next_pid 两个进程并发执行时的预期结果(假定 next_pid = 100) 一个进程得到的 ID 应该是100 另一个进程的 ID 应该是101 next_pid 应该增加到102 原子操作( Atomic Operation ) 原子操作是指一次不存在任何中断或者失败的操作 要么操作成功完成 或者操作没有执行 不会出现部分执行的状态 操作系统需要利用同步机制在并发执行的同时，保证一些操作是原子操作 进程的交互关系：相互感知程度 相互感知的程度 交互关系 进程间的影响 相互不感知(完全不了解其他进程的存在) 独立 一个进程的操作对其他进程的结果无影响 间接感知(双方都与第三方交互，如共享资源) 通过共享进行协作 一个进程的结果依赖于共享资源的状态 直接感知(双方直接交互，如通信) 通过通信进行协作 一个进程的结果依赖于从其他进程获得的信息 互斥( mutual exclusion ) 一个进程占用资源，其他进程不会使用 死锁( deadlock ) 多个进程各占用部分资源，形成循环等待 饥饿( starvation ) 其他进程可能轮流占用资源，一个进程一直得不到资源 临界区( Critical Section )1234entry section critical sectionexit section remainder section 临界区 ( critical section ) 进程中访问临界资源的一段需要互斥执行的代码 进入区 ( entry section ) 检查可否进入临界区的一段代码 如可进入，设置相应”正在访问临界区”标志 退出区 ( exit section ) 清除”正在访问临界区”标志 剩余区 ( remainder section ) 代码中的其余部分 临界区的访问规则 空闲则入 没有进程在临界区时，任何进程可进入 忙则等待 有进程在临界区时，其他进程均不能进入临界区 有限等待 等待进入临界区的进程不能无限期等待 让权等待(可选) 不能进入临界区的进程，应释放 CPU (如转换到阻塞状态) 临界区的实现方法(同步方法) 禁用中断 没有中断，没有上下文切换，因此没有并发 硬件将中断处理延迟到中断被启动之后 现代计算机体系结构都提供指令来实现禁用中断 123local_irq_save(unsigned long flags);critical sectionlocal_irq_restore(unsigned long flags); 进入临界区：禁止所有中断，并保存标志 离开临界区：使能所有中断，并恢复标志 缺点 禁用中断后，进程无法被停止 整个系统都会为此停下来 可能导致其他进程处于饥饿状态 临界区可能很长 无法确定响应中断所需的时间(可能存在硬件影响) 要小心使用 软件方法 线程可通过共享一些共有变量来同步它们的行为 Peterson 算法 满足线程 Ti 和 Tj 之间互斥的经典的基于软件的解决方法(1981年) 共享变量 12int turn; //表示该谁进入临界区boolean flag[]； //表示进程是否准备好进入临界区 进入区代码 123flag[i] = true;true = j;while (flag[j] &amp;&amp; turn == j); 退出区代码 1flag[i] = false; 线程 Ti 的完整代码 12345678do &#123; flag[i] = true; true = j; while (flag[j] &amp;&amp; turn == j); CRITICAL SECTION flag[i] = false; REMAINDER SECTION&#125; while (true); Dekkers 算法 线程 Ti 的代码123456789101112131415flag[0] := false;flag[1] := false;turn := 0; //orldo &#123; flag[i] = true; while flag[j] == true &#123; if turn != i &#123; flag[i] := false while turn != i &#123; &#125; flag[i] := true &#125; &#125; CRITICAL SECTION turn := j flag[i] = false; EMAINDER SECTION&#125; while (true); N 线程的软件方法( Eisenbery 和 McGuire ) 缺点 复杂：需要两个进程间的共享数据项 需要忙等待：浪费 CPU 时间 更高级的抽象方法 硬件提供了一些同步原语 中断禁用，原子操作指令等 操作系统提供更高级的编程抽象来简化进程同步 例如：锁、信号量 用硬件原语来构建 不同的临界区实现机制的比较 性能：并发级别 锁( lock ) 锁是一个抽象的数据结构 一个二进制变量(锁定/解锁) Lock::Acquire() 锁被释放前一直等待，然后得到锁 Lock::Release() 释放锁，唤醒任何等待的进程 使用锁来控制临界区访问123lock_next_pid-&gt;Acquire();new_pid = next_pid++;lock_next_pid-&gt;Release(); 原子操作指令 现代 CPU 体系结构都提供一些特殊的原子操作指令 测试和置位( Test-and-Set )指令 从内存单元中读取值 测试该值是否为1(然后返回真或假) 内存单元值设置为112345boolean TestAndSet (boolean *target)&#123; boolean rv = *target; *target = true; return rv;&#125; 交换指令( exchange ) 交换内存中的两个值12345void Exchange (boolean *a, boolean *b)&#123; boolean temp = *a; *a = *b; *b = temp;&#125; 使用 TS 指令实现自旋锁( spinlock ) 线程在等待的时候需要消耗 CPU 时间 使用 TS 指令实现无忙等待锁 原子操作指令锁的特征 优点 适用于单处理器或者共享生存的多处理器中任意数量的进程同步 简单并且容易证明 支持多临界区 缺点 忙等待消耗处理器时间 可能导致饥饿：进程离开临界区时有多个等待进程的情况 死锁 拥有临界区的低优先级进程 请求访问临界区的高优先级进程获得处理器并等待临界区 同步方法总结 并发问题 多线程并发导致资源竞争 同步概念 协调多线程对共享数据的访问 任何时刻只能有一个线程执行临界区代码 确保同步正确的方法 底层硬件支持 高层次的编程抽象 锁是一种高级的同步抽象方法 互斥可以使用锁来实现 需要硬件支持 常用的三种同步实现方法 禁用中断(仅限于单处理器) 软件方法(复杂) 原子操作指令(单处理器或多处理器均可)]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（四）]]></title>
    <url>%2F2019%2F03%2F18%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[进程和线程进程 定义 进程是指一个具有一定独立功能的程序在一个数据集合上的一次动态执行过程 进程的组成 进程包含了正在运行的一个程序的所有状态信息 代码 数据 状态寄存器：CPU 状态 CR0 、指令指针 IP 通用寄存器：AX、BX、CX… 进程占用系统资源：打开文件、已分配内存… 进程的特点 动态性：可动态地创建，结束进程 并发性：进程可以被独立调度并占用处理机运行 独立性：不同进程的工作不相互影响 制约性：因访问共享数据/资源或进程间同步而产生制约 进程与程序的联系 进程是操作系统处于执行状态程序的抽象 程序 = 文件 (静态的可执行文件) 进程 = 执行中的程序 = 程序 + 执行状态 同一个程序的多次执行过程对应为不同进程 如命令 “ ls “ 的多次执行对应多个进程 进程执行需要的资源 内存：保存代码和数据 CPU：执行指令 进程与程序的区别 进程是动态的，程序是静态的 程序是有序代码的集合 进程是程序的执行，进程有核心态/用户态 进程是暂时的，程序的永久的 进程是一个状态变化的过程 程序可长久保存 进程与程序的组成不同 进程的组成包括程序、数据和进程控制块 进程控制块( PCB，Process Control Block ) 定义：操作系统管理控制进程运行所用的信息集合 操作系统用 PCB 来描述进程的基本情况以及运行变化的过程 PCB 是进程存在的唯一标志 每个进程都在操作系统中有一个对应的 PCB 进程控制块的使用 进程创建：生成该进程的 PCB 进程终止：回收它的 PCB 进程的组织管理：通过对 PCB 的组织管理来实现 进程控制块内容 进程标识信息 处理机现场保存 进程控制信息 调度和状态信息：进程和处理机制使用情况调度 进程间通信信息：进程间通信相关的各种标识 存储管理信息：指向进程映像存储空间数据结构 进程所用资源：进程使用的系统资源，如打开文件等 有关数据结构连接信息：与 PCB 相关的进程队列 进程控制块的组织 链表：同一状态的进程其 PCB 成一链表，多个状态对应多个不同的链表 各状态的进程形成不同的链表：就绪链表、阻塞链表 索引表：同一状态的进程归入一个索引表(由索引指向 PCB )，多个状态对应多个不同的索引表 各状态的进程形成不同的索引表：就绪索引表、阻塞索引表 进程状态 进程的生命周期划分 进程创建 引起进程创建的情况 系统初始化时 用户请求创建一个新进程 正在运行的进程执行了创建进程的系统调用 进程执行 内核选择一个就绪的进程，让它占用处理机并执行 进程等待 进入等待(阻塞)的情况 请求并等待系统服务，无法马上完成 启动某种操作，无法马上完成 需要的数据没有到达 只有进程自身才能知道何时需要等待某种事件的发生 进程抢占 进程会被抢占的情况 高优先级进程就绪 进程执行当前时间用完 进程唤醒 唤醒进程的情况 被阻塞进程需要的资源可被满足 被阻塞进程等待的事件到达 进程只能被别的进程或者操作系统唤醒 进程结束 进程结束的情况 正常退出(自愿的) 错误退出(自愿的) 致命错误(强制性的) 被其他进程扼杀(强制性的) 三状态进程模型 三个主要状态 运行状态( Running )：进程正在处理机上运行 就绪状态( Ready )：进程获得了除处理机之外的所需资源，得到处理机即可运行 等待状态( 又称阻塞状态 Blocked )：进程正在等待某一事件的出现而暂停运行 辅助状态 创建状态( New )：一个进程正在被创建，还没被转到就绪状态之前的状态 结束状态( Exit )：一个进程正在从系统中消失时的状态，这是因为进程结束或由于其他原因所导致 状态变迁 NULL &gt; 创建：一个新进程被产生出来执行一个程序 创建 &gt; 就绪：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态 就绪 &gt; 运行：处于就绪状态的进程被进程调度程序选中后，就分配到处理机上来运行 运行 &gt; 结束：当进程表示它已经完成或者因出错，当前运行进程会由操作系统作结束处理 运行 &gt; 就绪：处于运行状态的进程在其运行过程中，由于分配给它的处理机时间片用完而让出处理机 运行 &gt; 等待：当进程请求某资源且必须等待时 等待 &gt; 就绪：当进程要等待某事件到来时，它从阻塞状态变到就绪状态 挂起进程模型 进程挂起：处于挂起状态的进程映像在磁盘上，目的是减少进程占用内存 新增状态 等待挂起状态( Blocked-suspend )：进程在外存并等待某事件的出现 就绪挂起状态( Ready-suspend )：进程在外存，但只要进入内存即可运行 新增状态变迁 挂起( Suspend )：把一个进程从内存转到外存 等待到等待挂起：没有进程处于就绪状态或就绪进程要求更多内存资源 就绪到就绪挂起：当有高优先级等待(系统认为会很快就绪的)进程和低优先级就绪进程 运行到就绪挂起：对抢先式分时系统，当有高优先级等待挂起进程因事件出现而进入就绪挂起 在外存时的状态转换 等待挂起到就绪挂起：当有等待挂起进程因相关事件出现 激活( Activate )：把一个进程从外存转到内存 就绪挂起到就绪：没有就绪进程或挂起就绪进程优先级高于就绪进程 等待挂起到等待：当一个进程释放足够内存，并有高优先级等待挂起进程 状态队列 由操作系统来维护一组队列，表示系统中所有进程的当前状态 不同队列表示不同状态 就绪队列、各种等待队列 根据进程状态不同，进程 PCB 加入相应队列 进程状态变化时，它所在的 PCB 会从一个队列换到另一个 线程 定义：线程是进程的一部分，描述指令流执行状态。它是进程中的指令执行流的最小单元，是 CPU 调度的基本单位 进程的资源分配角色：进程由一组相关资源构成，包括地址空间(代码段、数据段)、打开的文件等各种资源 线程的处理机调度角色：线程描述在进程资源环境中的指令流执行状态 进程和线程的关系 线程 = 进程 - 共享资源 线程的优点 一个进程中可以同时存入多个线程 各个线程之间可以并发地执行 各个线程之间可以共享地址空间和文件等资源 线程的缺点 一个线程崩溃，会导致其所属进程的所有线程崩溃 线程和进程的比较 进程是资源分配单位，线程是 CPU 调度单位 进程拥有一个完整的资源平台，而线程只独享指令流执行的必要资源，如寄存器和栈 线程具有就绪、等待和运行三种基本状态和状态间的转换关系 线程能减少并发执行的时间和空间开销 线程的创建时间比进程短 线程的终止时间比进程短 同一进程内的线程切换时间比进程短 由于同一进程的各线程间共享内存和文件资源，可不通过内核进行直接通信 线程的三种实现方式 用户线程：在用户空间实现 POSIX Pthreads，Mach C-threads，Solaris threads 内核线程：在内核中实现 Windows，Solaris，Linux 轻量级进程：在内核中实现，支持用户线程 Solaris ( LightWeight Process ) 用户线程 定义：由一组用户级的线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等 特征 不依赖于操作系统的内核 内核不了解用户线程的存在 可用于不支持线程的多进程操作系统 在用户空间实现的线程机制 每个进程有私有的线程控制块( TCB )列表 TCB 有线程库函数维护 同一进程内的用户线程切换速度快 无需用户态/内核态切换 允许每个进程拥有自己的线程调度算法 不足 线程发起系统调用而阻塞时，则整个进程进入等待 不支持基于线程的处理机抢占 除非当前运行线程主动放弃，它所在进程的其他线程无法抢占 CPU 只能按进程分配 CPU 时间 多个线程进程中，每个线程的时间片较少 内核线程 定义：由内核通过系统调用实现的线程机制，由内核完成线程的创建、终止和管理 特征 由内核维护 PCB 和 TCB 线程执行系统调用而被阻塞不影响其他线程 线程的创建、终止和切换相对较大 通过系统调用/内核函数，在内核实现 以线程为单位进行 CPU 时间分配 多线程的进程可获得更多 CPU 时间 轻权进程( LightWeight Process )内核支持的用户线程。一个进程可有一个或多个轻量级进程，每个轻权进程由一个单独的内核线程来支持。(Solaris/Linux) 用户进程与内核线程的对应关系 实际实现中，一对一效果最好 #进程管理 进程切换 进程切换(上下文切换) 暂停当前运行进程，从运行状态变为其他状态 调度另一个进程从就绪状态变成运行状态 进程切换的要求 切换前，保存进程上下文 切换后，恢复进程上下文 快速切换 进程生命周期的信息 寄存器( PC，SP，…) CPU 状态 内存地址空间 上下文切换图示 进程控制块 PCB ：内核的进程状态记录 内核为每个进程维护了对应的进程控制块 ( PCB ) 内核将相同状态的进程的 PCB 放置在同一队列 就绪队列 I/O 等待队列 每个设备一个队列 僵尸队列 进程创建 创建新进程 Windows 进程创建 API：CreateProcess(filename) 创建时关闭所有在子进程里的文件描述符 CreateProcess(filename, CLOSE_FD) 创建时改变子进程的环境 CreateProcess(filename, CLOSE_FD, new_envp) 等等 Unix 进程创建系统调用：fork/exec fork() 把一个进程复制成两个进程 parent (old PID)，child (new PID) exec() 用新程序来重写当前进程 PID 没有改变 用 fork 和 exec 创建进程的示例 12345int pid = fork(); //创建子进程if(pid == 0)&#123; //Do anything (unmap memory, close net connections...) exec("program",argc,argv0,argv1,...)&#125; fork() 创建一个继承的子进程 复制父进程的所有变量和内存 复制父进程的所有 CPU 寄存器(有一个寄存器例外) fork() 的返回值 子进程的 fork() 返回0 父进程的 fork() 返回子进程标识符 fork() 返回值可方便后续使用，子进程可使用 getpid() 获取 PID fork() 的地址空间复制 fork() 执行过程对于子进程而言，是在调用时间对父进程地址空间的一次复制 对于父进程 fork() 返回 child PID，对于子进程返回值为0 程序加载和执行 系统调用 exec() 加载新程序取代当前运行进程 exec() 示例代码123456789101112main()...int pid = fork(); //创建子进程if(pid == 0)&#123; //子进程在这里继续 exec_status = exec("calc",argc,arg0,arg1,...); printf("Why would I execute?");&#125; else &#123; //父进程在这里继续 printf("Whose your daddy?"); ... child_status = wait(pid);&#125;if(pid &lt; 0)&#123;/*error occurred*/&#125; fork() 的开销 fork() 的实现开销 对子进程分配内存 复制父进程的内存和 CPU 寄存器到子进程里 开销昂贵！！ 在 99% 的情况里，我们在调用 fork() 之后调用 exec() 在 fork() 操作中内存复制是没有作用的 子进程将可能关闭打开的文件和连接 为什么不能结合它们在一个调用中？ vfork() 创建进程时，不再创建一个同样的内存映像 一些时候称为轻量级 fork() 子进程应该几乎立即调用 exec() 现在使用 Copy on Write (COW) 技术 进程加载 程序加载和执行系统调用 exec() 允许进程”加载”一个完全不同的程序，并从 main 开始执行(即_start) 允许进程加载时指定启动参数(argc，argv) exec 调用成功时 它是相同的进程… 但是运行了不同的程序 代码段、堆栈和堆(heap)等完全重写 进程等待与退出 父进程等待子进程 wait() 系统调用用于父进程等待子进程的结束 子进程结束时通过 exit() 向父进程返回一个值 父进程通过 wait() 接受并处理返回值 wait() 系统调用的功能 有子进程存活时，父进程进入等待状态，等待子进程的返回结果 当某子进程调用 exit() 时，唤醒父进程，将 exit() 返回值作为父进程中 wait 的返回值 有僵尸子进程等待时，wait()立即返回其中一个值 无子进程存活时，wait()立刻返回 进程的有序终止 exit() 进程结束执行时调用 exit()，完成进程资源回收 exit() 系统调用的功能 将调用参数作为进程的”结果” 关闭所有打开的文件等占用资源 释放内存 释放大部分进程相关的内核数据结构 检查是否父进程是存活着的 如存活，保留结果的值直到父进程需要它，进入僵尸( zombie / defunct )状态 如果没有，它释放所有的数据结构，进程结果 清理所有等待的僵尸进程 进程终止是最终的垃圾收集(资源回收) 其他进程控制系统调用 优先级控制 nice() 指定进程的初始优先级 Unix 系统中进程优先级会随执行时间而衰减 进程调试支持 ptrace() 允许一个进程控制另一个进程的执行 设置断点和查看寄存器等 定时 sleep() 可以让进程在定时器的等待队列中等待指定 进程控制 v.s. 进程状态 exec() 实际上是执行代码过程中的一种状态]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（三）]]></title>
    <url>%2F2019%2F03%2F18%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[虚拟存储概念虚拟存储 概念：基于非连续存储内存分配的基础上，可以把一部分内训放在外存里 需求 计算机系统时常出现内存空间不够用 覆盖( overlay )应用程序手动把需要的指令和数据保存在内存中 交换( swapping )操作系统自动把暂时不能执行的程序保存到外存中 虚拟存储在有限容量的内存中，以页为单位自动装入更多更大的程序 覆盖技术 目标：在较小的可用内存中运行较大的程序 方法：依据程序逻辑结构，将程序划分为若干功能相对独立的模块；将不会同时执行的模块共享同一块内存区域 必要部分(常用功能)的代码和数据常驻内存 可选部分(不常用功能)放在其他程序模块中，只在要用到时装入内存 不存在调用关系的模块可相互覆盖，共用同一内存区域 不足 增加编程困难 需程序员划分功能模块，并确定模块间的覆盖关系 增加了编程的复杂度 增加执行时间 从外存装入覆盖模块 时间换空间 交换技术 目标：增加正在运行或需要运行的程序的内存 实现方法 可将暂时不能运行的程序放到外存 换入换出的基本单位 整个进程的地址空间 换出( swap out ) 把一个进程的整个地址空间保存到外存 换入( swap in ) 将外存中某进程的地址空间读入到内存 交换技术面临的问题 交换时机：何时需要发生交换？ 只当内存空间不够或有不够的可能时换出 交换区大小 存放所有用户进程的所有内存映像的拷贝 程序换入时的重定向：换出后再换入时要放在原处吗？ 采用动态地址映射的方法 覆盖和交换的比较 覆盖 只能发生在没有调用关系的模块间 程序员须给出模块间的逻辑覆盖结构 发生在运行程序的内部模块间 交换 以进程为单位 不需要模块间的逻辑覆盖结构 发生在内存进程间 虚拟存储技术的目标 只把部分程序放到内存中，从而运行比物理内存大的程序 由操作系统自动完成，无需程序员的干涉 实现进程在内存与外存之间的交换，从而获得更多的空闲内存空间 在内存与外存之间只交换进程的部分内容 局部性原理( principle of locality ) 程序在执行过程中的一个较短时期，所执行的指令地址和指令的操作数地址，分别局限于一定区域。 时间局部性 一条指令的一次执行和下次执行，一个数据的一次访问和下次访问都集中在一个较短时期内 空间局部性 当前指令和邻近的几条指令，当前访问的数据和邻近的几个数据都集中在一个较小区域内 分支局部性 一条跳转指令的两次执行，很可能跳到相同的内存位置 局部性原理的意义 从理论上来说，虚拟存储技术是能够实现的，而且可取得满意的效果 虚拟存储的基本概念 思路：将不常用的部分内存块暂存到外存 原理 装载程序时 只将当前指令执行需要的部分页面或段装入内存 指令执行中需要的指令或数据不在内存(称为缺页或缺段)时 处理器通知操作系统将相应的页面或段调入内存 操作系统将内存中暂时不用的页面或段保存到外存 实现方式 虚拟页式存储 虚拟段式存储 基本特征 不连续性 物理内存分配非连续 虚拟地址空间使用非连续 大用户空间 提供给用户的虚拟内存可大于实际的物理内存 部分交换 虚拟存储只对部分虚拟地址空间进行调入和调出 支持技术 硬件：页式或段式存储中的地址转换机制 操作系统：管理内存和外存间页面或段的换入和换出 虚拟页式存储管理 在页式存储管理的基础上，增加请求调页和页面置换 思路 当用户程序要装载到内存运行时，只装入部分页面，就启动程序运行 进程在运行中发现有需要的代码或数据不在内存时，则向系统发出缺页异常请求 操作系统在处理缺页异常时，将外存中相应的页面调入内存，使得进程能继续运行 虚拟页式存储中的页表项结构 驻留位：表示该页是否在内存 1表示该页位于内存中，该页表项是有效的，可以使用 0表示该页当前在外存中，访问该页表项将导致缺页异常 修改位：表示在内存中的该页是否被修改过 回收该物理页面时，据此判断是否要把它的内容写回外存 访问位：表示该页面是否被访问过(读或写) 用于页面置换算法 保护位：表示该页的允许访问方式 只读，可写，可执行等 虚拟页式存储中的外存管理 在何处保存未被映射的页？ 应能方便地找到在外存中的页面内容 交换空间(磁盘或者文件) 采用特殊格式存储未被映射的页面 虚拟页式存储中的外存选择 代码段：可执行二进制文件 动态加载的共享库程序段：动态调用的库文件 其他段：交换空间 虚拟页式存储管理的性能 有效存储访问时间( effective memory access time EAT ) EAT = 访存时间 (1-p) + 缺页异常处理时间 缺页率 p 例子： 访存时间：10 ns；磁盘访问时间：5 ms；缺页率 p；页修改概率 q； EAT = 10(1-p) + 5,000,000p(1+q) 缺页异常(缺页中断)的处理流程 在内存中有空闲物理页面时，分配一物理页帧 f，转第 5 步 依据页面置换算法选择将被替换的物理页帧 f，对应逻辑页 q 如 q 被修改过，则把它写回外存 修改 q 的页表项中驻留位置为 0 将需要访问的页 p 装入到物理页面 f 重新执行产生缺页的指令 页面置换算法置换算法的功能和目标 功能 当出现缺页异常，需调入新页面而内存已满时，置换算法选择被置换的物理页面 设计目标 尽可能减少页面的调入调出次数 把未来不再访问或短期内不访问的页面调出 页面锁定( frame locking ) 描述必须常驻内存的逻辑页面 操作系统的关键部分 要求响应速度的代码和数据 页表中的锁定标志位( lock bit ) 置换算法的评价方法 记录进程访问内存的页面轨迹 举例：虚拟地址访问用(页号，位移)表示(3,0), (1,9), (4,1), (2,1), (5,3), (2,0), (1,9), (2,4), (3,1), (4,8) 对应的页面轨迹3，1，4，2，5，2，1，2，3，4替换c，a，d，b，e，b，a，b，c，d 评价方法 模拟页面置换行为，记录产生缺页的次数 更少的缺页，更好的性能 页面置换算法分类 局部页面置换算法 置换页面的选择范围仅限于当前进程占用的物理页面内 最优算法，先进先出算法，最近最久未使用算法 时钟算法，最不常用算法 全局页面置换算法 置换页面的选择范围是所有可换出的物理页面 工作集算法，缺页率算法 最优页面置换算法( OPT，optional ) 基本思路：置换在未来最长时间不访问的页面 算法实现 缺页时，计算内存中每个逻辑页面的下一次访问时间 选择未来最长时间不访问的页面 算法特征 缺页最少，是理想情况 实际系统中无法实现 无法预知每个页面在下次访问前的等待时间 作为置换算法的性能评价依据 在模拟器上运行某个程序，并记录每一次的页面访问情况 第二遍运行时使用最优算法 先进先出算法( First-In First-Out，FIFO ) 思路：选择在内存驻留时间最长的页面进行置换 实现 维护一个记录所有位于内存中的逻辑页面链表 链表元素按驻留内存的时间排序，链首最长，链尾最短 出现缺页时，选择链首页面进行置换，新页面加到链尾 特征 实现简单 性能较差，调出的页面可能是进场访问的 进程分配物理页面数增加时，缺页并不一定减少(Belady现象) 很少单独使用 最近最久未使用算法( Least Recently Used，LRU ) 思路 选择最长时间没有被引用的页面进行置换 如某些页面长时间未被访问，则它们在将来还可能会长时间不会访问 实现 缺页时，计算内存中每个逻辑页面的上一次访问时间 选择上一次使用到当前时间最长的页面 特征 最优置换算法的一种近似 LRU 算法的可能实现方法 页面链表 系统维护一个按最近一次访问时间排序的页面链表 链表首节点是最近刚刚使用过的页面 链表尾节点是最久未使用的页面 访问内存时，找到相应页面，并把它移到链表之首 缺页时，置换链表尾节点的页面 活动页面栈 访问页面时，将此页号压入栈顶，并栈内相同的页号抽出 缺页时，置换栈底的页面 LRU 算法特征：开销比较大 时钟置换算法( Clock ) 思路：仅对页面的访问情况进行大致统计 数据结构 在页表项中增加访问位，描述页面在过去一段时间的内访问情况 各页面组织成环型链表 指针指向最先调入的页面 算法 访问页面时，在页表项记录页面访问情况 缺页时，从指针处开始顺序查找未被访问的页面进行置换 特征：时钟算法是 LRU 和 FIFO 的折中 实现 页面装入内存时，访问位初始化为0 访问页面(读/写)时，访问位置1 缺页时，从指针当前位置顺序检查环形链表 访问位为0，则置换该页 访问位为1，则访问位置0，并指针移动到下一个页面，直到找到可置换的页面 改进的 Clock 算法 思路：减少修改页的缺页处理开销 算法 在页面中增加修改位，并在访问时进行相应修改 缺页时，修改页面标志位，以跳过有修改的页面 最不常用算法( Least Frequently Used，LFU ) 思路：缺页时，置换访问次数最少的页面 实现 每个页面设置一个访问计数 访问页面时，访问计数加1 缺页时，置换计数最小的页面 特征 算法开销大 开始时频繁使用，但以后不使用的页面很难置换 解决方法：计数定期右移 LRU 和 LFU 的区别 LRU 关注多久未访问，时间越短越好 LFU 关注访问次数，次数越多越好 Belady现象 现象：采用 FIFO 等算法时，可能出现分配的物理页面数增加，缺页次数反而升高的异常现象 原因 FIFO 算法的置换特征与进程访问内存的动态特征矛盾 被它置换出去的页面并不一定是进程近期不会访问的 FIFO 算法有 Belady 现象 LRU 算法没有 Belady 现象 LRU，FIFO 和 Clock 的比较 LRU 算法和 FIFO 本质上都是先进先出的思路 LRU 依据页面的最近访问时间排序 LRU 需要动态地调整顺序 FIFO 依据页面进入内存的时间排序 FIFO 的页面进入时间是固定不变的 LRU 可退化成 FIFO 如页面进入内存后没有被访问，最近访问时间与进入内存的时间相同 例如：给进程分配3个物理页面，逻辑页面的访问顺序为1，2，3，4，5，6，1，2，3… LRU 算法性能较好，但系统开销较大 FIFO 算法系统开销较小，会发生 Belady 现象 Clock 算法是它们的折中 页面访问时，不动态调整页面在链表中的顺序，仅做标记 缺页时，再把它移动到链表末尾 对于未被访问的页面，Clock 和 LRU 算法的表现一样好 对于被访问过的页面，Clock 算法不能记录准确访问顺序，而 LRU 算法可以 全局页面置换算法 背景：局部置换算法没有考虑进程访存差异 思路：全局置换算法为进程分配可变数目的物理页面 全局置换算法要解决的问题 进程在不同阶段的内存需求是变化的 分配给进程的内存也需要在不同阶段有所变化 全局置换算法需要确定分配给进程的物理页面数 CPU 利用率和并发进程数的关系 CPU 利用率与并发进程数存在相互促进和制约的关系 进程数少时，提高并发进程数，可提高 CPU 利用率 并发进程导致内存访问增加 并发进程的内存访问会降低了访存的局部性特征 局部性特征的下降会导致缺页率上升和 CPU 利用率下降 工作集 一个进程当前正在使用的逻辑页面集合，可表示为二元函数 W(t,△) t 是当前的执行时刻 △ 称为工作集窗口 ( working-set window )，即一个定长的页面访问时间窗口 W(t,△) 是指在当前时刻 t 前的 △ 时间窗口中的所有访问页面所组成的集合 |W(t,△)| 指工作集的大小，即页面数目 工作集的变化 进程开始执行后，随着访问新页面逐步建立较稳定的工作集 当内存访问的局部性区域的位置大致稳定时，工作集大小也大致稳定 局部性区域的位置改变时，工作集快速扩张和收缩过渡到下一个稳定值 常驻集 在当前时刻，进程实际驻留在内存当中的页面集合 工作集和常驻集的关系 工作集是进程在运行过程中固有的性质 常驻集取决于系统分配给进程的物理页面数目和页面置换算法 缺页率和常驻集的关系 常驻集 ⊇ 工作集时，缺页较少 工作集发生剧烈变动(过渡)时，缺页较多 进程常驻集大小达到一定数目后，缺页率也不会明显下降 工作集置换算法 思路：换出不在工作集中的页面 窗口大小 τ 当前时刻前 τ 个内存访问的页引用是工作集，τ 被称为窗口大小 实现方法 访存链表：维护窗口内的访存页面链表 访存时，换出不在工作集的页面；更新访存链表 缺页时，换入页面；更新访存链表 缺页率( page fault rate )缺页次数 / 内存访问次数 或 缺页平均时间间隔的倒数 影响缺页率的因素 页面置换算法 分配给进程的物理页面数目 页面大小 程序的编写方法 缺页率置换算法( PFF，Page-Fault-Frequency ) 原理 通过调节常驻集大小，使每个进程的缺页率保持在一个合理的范围内 若进程缺页率过高，则增加常驻集以分配更多的物理页面 若进程缺页率过低，则减少常驻集以减少它的物理页面 实现 访存时，设置引用位标志 缺页时，计算从上次缺页时间 tlast 到现在 tcurrent 的时间间隔 如果 tcurrent - tlast &gt; T，则置换所有在 [ tlast , tcurrent ] 时间内没有被引用的页 如果 tcurrent - tlast &lt;= T，则增加缺失页到常驻集中 抖动问题 抖动 进程物理页面太少，不能包含工作集 造成大量缺页，频繁置换 进程运行速度变慢 产生抖动的原因 随着驻留内存的进程数目增加，分配给每个进程的物理页面数不断减少，缺页率不断上升。 操作系统需在并发水平和缺页率之间达到一个平衡 选择一个适当的进程数目和进程需要的物理页面数 负载控制 通过调节并发进程数 ( MPL ) 来进行系统负载控制 ∑WSi = 内存大小 平均缺页间隔时间 ( MTBF ) = 缺页异常处理时间( PFST )]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（二）]]></title>
    <url>%2F2019%2F03%2F18%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[物理内存管理：连续内存分配地址空间定义 物理地址空间：硬件支持的地址空间 起始地址0，直到 MAXsys 逻辑地址空间：在 CPU 运行的进程看到的地址 起始地址0，直到 MAXprog 地址生成时机和限制 编译时 假设起始地址已知 如果起始地址改变，必须重新编译 加载时 如编译时起始位置未知，编译器需生成可重定位的代码(relocatable code) 加载时，生成绝对地址 执行时 执行时代码可移动 需地址转换(映射)硬件支持 地址生成过程 CPU ALU：需要逻辑地址的内存内容 MMU：进行逻辑地址和物理地址的转换 CPU 控制逻辑：给总线发送物理地址请求 内存 发送物理地址的内容给 CPU 或接受 CPU 数据到物理地址 操作系统 建立逻辑地址 LA 和物理地址 PA 的映射 连续内存分配和内存碎片 连续内存分配 给进程分配一块不小于指定大小的连续的物理内存区域 内存碎片 空闲内存不能被利用 外部碎片 分配单元之间的未被使用内存 内部碎片 分配单元内部的未被使用内存 取决于分配单元大小是否要取整 连续内存分配：动态分区分配 动态分区分配 当程序被加载执行时，分配一个进程指定大小可变的分区(块、内存块) 分区的地址是连续的 操作系统需要维护的数据结构 所有进程的已分配分区 空闲分区(Empty-blocks) 动态分区分配策略 最先匹配(First-fit) 最佳匹配(Best-fit) 最差匹配(Worst-fit) 最先匹配(First Fit Allocation)策略 思路：分配 n 个字节，使用功能第一个可用的空间比 n 大的空闲块 原理 &amp; 实现 空闲分区列表按地址顺序排序 分配过程中，搜索一个合适的分区 释放分区时，检查是都可与临近的空闲分区合并 优点 简单 在高地址空间有大块的空闲分区 缺点 外部碎片 分配大块时较慢 最佳匹配(Best Fit Allocation)策略 思路：分配 n 字节分区时，查找并使用不小于 n 的最小空闲分区 原理 &amp; 实现 空闲分区列表按照大小排序 分配时，查找一个合适的分区 释放时，查找并且合并临近的空闲分区(如果找到) 优点 大部分分配的尺寸较小时，效果很好 可避免大的空闲分区被拆分 可减少外部碎片的大小 相对简单 缺点 外部碎片 释放分区比较慢 容易产生很多无用的小碎片 最差匹配(Worst Fit Allocation)策略 思路：分配 n 字节，使用尺寸不小于 n 的最大空闲分区 原理 &amp; 实现 空闲分区列表按由大到小排序 分配时，选最大的分区 释放时，检查是否可与临近的空闲分区合并，进行可能的合并，并调整空闲分区列表顺序 优点 中等大小的分配比较多时，效果最好 避免出现太多的小碎片 缺点 释放分区较慢 外部碎片 容易破坏大的空闲分区，因此后续难以分配大的分区 碎片整理：紧凑( compaction ) 碎片整理 通过调整进程占用的分区位置来减少或避免分区碎片 碎片紧凑 通过移动分配给进程的内存分区，以合并外部碎片 碎片紧凑的条件：所有的应用程序可动态重定位 需要解决的问题： 什么时候移动？ 开销 碎片整理：分区兑换( Swapping in/out ) 通过抢占并回收处于等待状态进程的分区，以增大可用内存空间 需要解决的问题 交换哪个(些)程序？ 伙伴系统( Buddy System ) 整个可分配的分区大小 2U 需要的分区大小为 2U-1 &lt; s &lt;= 2U 时，把整个块分配给该进程 如 s &lt;= 2i-1 ，将大小为 2i 的当前空闲分区划分成两个大小为 2i-1 的空闲分区 重复划分过程，直到 2i-1 &lt; s &lt;= 2i ，并把一个空闲分区分配给该进程 伙伴系统的实现 数据结构 空闲块按大小和起始地址组织成二维数组 初始状态：只有一个大小为 2U 的空闲块 分配过程 由小到大在空闲块数组中找最小的可用空闲块 如空闲块过大，对可用空闲块进行二等分，知道得到合适的可用空闲块 释放过程 把释放的块放入空闲块数组 合并满足合并条件的空闲块 合并条件 大小相同 2i 地址相邻 起始地址较小的块的起始地址必须是 2^(i+1) 的倍数 物理内存管理：非连续内存分配非连续分配的设计目标 连续分配的缺点 分配给程序的物理内存必须连续 存在外碎片和内碎片 内存分配的动态修改困难 内存利用率较低 非连续分配的设计目标：提高内存利用效率和管理灵活性 允许一个程序的使用非连续的物理地址空间 允许共享代码与数据 支持动态加载和动态链接 非连续分配需要解决的问题 如何实现虚拟地址和物理地址的转换? 软件实现 (灵活，开销大) 硬件实现 (够用，开销小) 非连续分配的硬件辅助机制 如何选择非连续分配中的内存分块大小？ 段式存储管理( segmentation ) 页式存储管理( paging ) 段地址空间 进程的段地址空间由多个段组成 主代码段 子模块代码段 公用库代码段 堆栈段( stack ) 堆数据( heap ) 初始化数据段 符号表等 段式存储管理的目的 更细粒度和灵活的分离与共享 段访问机制 段的概念 段表示访问方式和存储数据等属性相同的一段地址空间 对应一个连续的内存”块” 若干个段组成进程逻辑地址空间 段访问：逻辑地址由二元组 (s,addr) 表示 s: 段号 addr: 段内偏移 页式存储管理 页帧(帧，物理页面，Frame，Page Frame) 页面(页，逻辑页面，Page) 页面到页帧 逻辑地址到物理地址的转换 页表 MMU/TLB 帧( Frame ) 物理内存被划分成大小相等的帧，内存物理地址的表示：二元组 (f,o)f: 帧号( F 位，共有 2F 个帧)o: 帧内偏移 ( S 位，每帧有 2S 字节)物理地址 = f * 2S + o 基于页帧的物理地址计算实例 假定 16-bit 的地址空间 9-bit (512 byte) 大小的页帧 物理地址计算 物理地址表示 = (3, 6) 物理地址 = f * 2S + o F = 7，S = 9，f = 3，o = 6 实际物理地址 = 2^9 * 3 + 6 = 1536 + 6 = 1542 页( Page ) 进程逻辑地址空间被划分为大小相等的页 页内偏移 = 帧内偏移 通常：页号大小 ≠ 帧号大小 进程逻辑地址的表示：二元组 (p, o)p: 页号 ( P 位，2P 个页)o: 页内偏移 ( S 位，每页有 2S 字节)虚拟地址 = p * 2S + o 页式存储中的地址映射 页到帧的映射 逻辑地址中的页号是连续的 物理地址中的帧号是不连续的 不是所有的页都有对应的帧 页表 页表概述 页表保存了逻辑地址与物理地址之间的映射关系 页表结构 每个进程都有一个页表 每个页面对应一个页表项 随进程运行状态而动态变化 页表基址寄存器( PTBR: Page Table Base Register ) 页表项的组成 帧号：f 页表项状态 存在位( register bit ) 修改位( dirty bit ) 引用位( clock/reference bit ) 页式存储管理机制的性能问题 内存访问性能问题 访问一个内存单元需要2次内存访问 第一次访问：获取页表项 第二次访问：访问数据 页表大小问题 页表可能非常大 64位机器如果每页1024字节，那么一个页表的大小会是多少 如何处理？ 缓存 ( Caching ) 间接 ( Indirection ) 访问 解决页表问题 快表 ( Translation Look-aside Buffer, TLB ) 缓存近期访问的页表项 TLB 使用关联存储 ( association memory ) 实现，具备快速访问性能 如果 TLB 命中，物理页号可以很快被获取 如果 TLB 未命中，对应的表项被更新到 TLB 中 多级页表 通过间接引用将页号分成 k 级 建立页表”树” 减少每级页表的长度 反置页表 大地址空间问题 对于大地址空间 ( 64-bits ) 系统，多级页表变得繁琐 比如：5级页表 逻辑(虚拟)地址空间增长速度快于物理地址空间 页寄存器和反置页面的思路 不让页表与逻辑地址空间的大小相对应 让页表与物理地址空间的大小相对应 页寄存器( Page Registers ) 每个帧与一个页寄存器( Page Register )关联，寄存器内容包括： 使用位( Register bit )：此帧是否被进程占用 占用页号( Occupier )：对应的页号 p 保护位 ( Protection bits ) 页寄存器示例 物理内存大小：4096 4096 = 4 K 4 KB = 16 MB 页面大小：4096 bytes = 4 KB 页帧数：4096 = 4 K 页寄存器使用的空间(假设每个页寄存器占8字节)：8 * 4096 = 32 Kbytes 页寄存器带来的额外开销：32K / 16M = 0.2%(大约) 虚拟内存的大小：任意 页寄存器方案特征 优点 页表大小相对于物理内存而言很小 页表大小与逻辑地址空间大小无关 缺点 页表信息对调后，需要依据帧号可找页号 在页寄存器中搜索逻辑地址中的页号 页寄存器中的地址转换 CPU 生成的逻辑地址如何找对应的物理地址？ 对逻辑地址进行 Hash 映射，以减少搜索范围 需要解决可能的冲突 用快表缓存页表项后的页寄存器搜索步骤 对逻辑地址进行 Hash 变换 在快表中查找对应页表项 有冲突时遍历冲突项链表 查找失败时，产生异常 快表的限制 快表的容量限制 快表的功耗限制( StrongARM 上快表功耗占 27% ) 反置页表 基于 Hash 映射值查找对应页表项中的帧号 进程标识与页号的 Hash 值可能有冲突 页表项中包括保护位、修改位、访问位和存在位等标识 反置页表的 Hash 冲突 段页式存储管理 需求 段式存储在内存保护方面有优势，页式存储在内存利用和优化转移到后备存储方面有优势。 段式存储，页式存储能否结合？ 做法 在段式存储管理基础上，给每个段加一级页表 段页式存储管理中的内存共享 通过指向相同的页表基址，实现进程间的段共享]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统（一）]]></title>
    <url>%2F2019%2F03%2F18%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[操作系统概述什么是操作系统？没有公认定义，可以理解为起协助作用的控制程序，或者是介于软硬件之间的资源管理器。 操作系统软件组成 Shell 命令行接口 通过键盘操纵 方便用户进行命令输入 GUI 图形用户接口 WIMP视窗（windows）、t图标（icon）、选单（menu）、指标（pointer） 直接操作、所见即所得 Kernel 操作系统内核 执行各种资源管理等功能 操作系统内核 并发：计算机系统中同时存在多个运行的程序，需要 OS 管理和调度 共享：“同时”访问（宏观），互斥共享（微观：对资源进行隔离保护） 虚拟：高频率交替（多道程序设计技术），使用户感觉在专用系统 异步：程序执行时间难于预测，运行环境相同时，OS 需要保证输出结果相同 系统类型 UNIX(开放)：UNIX BSD、Mac OS等 Linux(与 UNIX 同一类,API 兼容或类似，开放程度更完整)：Ubuntu、安卓等 Windows(专用和封闭，图形用户接口，易用) 操作系统主要功能：硬件抽象和协调管理操作系统的演变 单用户系统 批处理系统 多程序系统 分时 个人计算机：每个用户一个系统 分布式计算：每个用户多个系统 操作系统结构 简单结构：无模块划分，主要汇编，不可移植 分层结构： 将操作系统分为多层（levels） 每层建立在底层之上 最底层（layer 0）是硬件 最高层（layer N）是用户界面 每一层仅使用更低一层的功能（操作）和服务 微内核结构（Microkernel）: 尽可能把内核功能移到用户空间 用户模块间的通信使用消息传递 好处：灵活、安全 缺点：性能 外核结构（Exokernel）: 内核少放东西，资源管理由应用态代码完成，一个系统支起不同的操作系统服务 让内核分配机器的物理资源给多个应用程序，并让每个程序决定如何处理这些资源 程序能链接到操作系统库（libOS）实现了操作系统抽象 保护和控制隔离 VMM（虚拟机管理器，非操作系统结构）: 负责和硬件接触（隔离资源），操作系统负责资源管理 虚拟机管理器将单独的机器接口转换成很多的虚拟机，每个虚拟机都是一个原始计算机系统的有效副本，并能完成所有的处理器指令 对于主流操作系统的结构分析 WindowsWindows 属于比较接近微内核的混合内核结构，这样的结构有一个特点，那就是驱动是单独分发的，并不会和 Windows 的微内核混合在一起。因此，其驱动配置比较简单，无需改动 Windows 内核代码。 LinuxLinux 是一个宏内核的结构，在保留了微内核结构优点的基础上进行了优化。Linux 的驱动和内核是整合在一起的，要适配某种硬件，得把驱动都整合进 Linux 内核。它是模块化的、多线程的以及内核本身可调度的操作系统。Linux 仅仅是一个单块内核，单个内核负责管理 CPU、内存、进程间通信、设备驱动程序、文件系统和系统服务器调用。 Mac OSMac OS 的内核（XNU）结合了微内核（Mach）和单片内核的（BSD）的特性。根据苹果公司的 Github 页面，XNU 是将卡耐基梅隆大学开发的 Mach 内核和 FreeBSD 组件整合而成的混合内核，加上用于编写驱动程序的 C++ API。代码的 BSD 子系统部分在微内核系统中，通常实现为用户空间的服务。Mach 部分负责底层工作，例如多任务、内存保护、虚拟内存管理、内核调试支持和控制台 I/O。 启动、中断、异常和系统调用BIOS启动固件基本功能： 基本输入输出的程序 系统设置信息 开机后自检程序 系统自启程序等 BIOS 系统调用： BIOS 以中断调用的方式提供了基本的 I/O 功能 INT 10h：字符显示 INT 13h：磁盘扇区读写 INT 15h：检测内存大小 INT 16h：键盘输入 只能在 x86 的实模式下访问 系统启动流程 CPU 初始化 CPU 加电稳定后从 0XFFFF0 读第一条指令 CS:IP = 0xf000:fff0 第一条指令是跳转指令 CPU 初始状态为16位实模式 CS:IP 是16位寄存器 指令指针 PC = 16 * CS + IP 最大地址空间是 1MB BIOS 初始化 硬件自检 POST 检测系统中内存和显卡等关键部件的存在和工作状态 查找并执行显卡等接口卡 BIOS ，进行设备初始化 执行系统 BIOS ，进行系统检测，检测和配置系统中安装的即插即用设备 更新 CMOS 中的扩展系统配置数据 ESCD 按指定启动顺序从软盘、硬盘或光驱启动 主引导记录 MBR 格式 启动代码：446字节 检查分区表正确性 加载并跳转到磁盘上的引导程序 硬盘分区表：64字节 描述分区状态和位置 每个分区描述信息占据16字节 结束标志字：2字节 (55AA) 主引导记录的有效标志 分区引导扇区格式 跳转指令：跳转到启动代码 与平台相关代码 文件卷头：文件系统描述信息 启动代码：跳转到加载程序 结束标志：55AA 加载程序(bootloader) 系统启动规范 BIOS 固化到计算机主板上的程序 包括系统设置、自检程序和系统自启动程序 BIOS-MBR、BIOS-GPT、PXE UEFI 接口标准 在所有平台上一致的操作系统启动服务 中断、异常和系统调用 系统调用 (system call) 应用程序主动向操作系统发出的服务请求 异常 非法指令或者其他原因导致当前指令执行失败 (如：内存出错)后的处理请求 中断 来自硬件设备的处理请求 区别： 源头 中断：外设 异常：应用程序意想不到的行为 系统调用：应用程序请求操作提供服务 响应方式 中断：异步 异常：同步 系统调用：同步或异步 处理机制 中断：持续，对用户应用程序是透明的 异常：杀死或者重新执行意想不到的应用程序指令 系统调用：等待和持续 中断处理机制 硬件处理 在 CPU 初始化时设置中断使能的标志 依据内部或外部事件设置中断标志 依据中断向量调用相应中断的服务例程 软件 现场保存(编译器) 中断服务处理(服务例程) 清除中断标记(服务例程) 现场恢复(编译器) 中断嵌套 硬件中断服务例程可被打断 不同硬件中断源可能硬件中断处理时出现 硬件中断服务例程中需要临时禁止中断请求 中断请求会保持到 CPU 做出响应 异常处理例程可被打断 异常服务例程执行时可能出现硬件中断 异常服务例程可嵌套 异常服务例程可能出现缺页 系统调用的外界使用 操作系统服务的编程接口 通常由高级语言编写 (C 或者 C++) 程序访问通常是通过高层次的 API 接口而不是直接进行系统调用 三种最常用的应用程序编程接口 (API) Win32 API 用于 Windows POSIX API 用于 POSIX-based systems (包括 UNIX, LINUX, Mac OS X的所有版本) Java API 用于 JAVA 虚拟机 (JVM) 系统调用的内部实现 每个系统调用对应一个系统调用号 系统调用接口根据系统调用号来维护表的索引 系统调用接口调用内核态中的系统调用功能实现，并返回系统调用的状态和结果 用户不需要知道系统调用的实现 需要设置调用参数和获取返回结果 操作系统接口的细节大部分都隐藏在应用编程接口后 通过运行程序支持的库来管理 函数调用和系统调用的不同处 系统调用 INT 和 IRET 指令用于系统调用 系统调用时，堆栈切换和特权级的转换 函数调用 CALL 和 RET 用于常规调用 常规调用时没有堆栈切换 中断、异常和系统调用的开销系统调用比函数调用更安全，但是系统调用的开销超过函数调用。中断、异常和系统调用具体开销： 引导机制 建立内核堆栈 验证参数 内核态映射到用户态的地址空间 更新页面映射权限 内核态独立地址空间 TLB 内存层次 操作系统的内存管理 操作系统的内存管理方式 操作系统中采用的内存管理方式 重定位 ( relocation ) 分段 ( segmentation ) 分页 ( paging ) 虚拟存储 ( virtual memory ) 目前多数系统(如 Linux )采用按需页式虚拟存储 实现高度依赖硬件 与计算机存储架构紧耦合 MMU(内存管理单元)：处理 CPU 存储访问请求的硬件]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSR-303校验]]></title>
    <url>%2F2019%2F03%2F15%2FJSR303%E6%A0%A1%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[JSRJSR是Java Specification Requests的缩写，意思是Java 规范提案。 JSR-303JSR-303 是 JAVA EE 6 中的一项子规范，叫做 Bean Validation，Hibernate Validator 是 Bean Validation 的参考实现 . Hibernate Validator 提供了 JSR 303 规范中所有内置 constraint 的实现，除此之外还有一些附加的 constraint。 Bean Validation 中内置的 constraint Constraint 详细信息 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max, min) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(value) 被注释的元素必须符合指定的正则表达式 Hibernate Validator 附加的 constraint Constraint 详细信息 @Email 被注释的元素必须是电子邮箱地址 @Length 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range 被注释的元素必须在合适的范围内 自定义 constraint有时无论是原配的 constraint 还是 Hibernate Validator 附加的 constraint 都不满足我们的实际生产需求。这时候我们就需要自己定义属于自己的 constraint。 以验证这不是个纯数字的字符串为例。 面对这个需求我们首先会想到使用 Bean Validation 的 @Pattern(value)并结合正则表达式来解决，在 js 中，我们使用该逻辑的正则表达式很简单：!/^\d+$/。但是在 JSR 303 中，对于@Pattern(value)，我们无法做非逻辑处理。因此，这时候我们就很有必要定义自己的 constraint 来满足需求。 自定义 constraint 的步骤如下： 定义验证标签 12345678910111213141516import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import javax.validation.Constraint;import javax.validation.Payload;@Documented@Constraint(validatedBy = &#123; IsNotNumValidator.class &#125;)@Target(&#123;ElementType.METHOD, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface IsNotNum &#123; String message() default "这不是一个纯数字！"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 定义标签所依赖的验证逻辑 12345678910111213import java.util.regex.Pattern;import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;public class IsNotNumValidator implements ConstraintValidator&lt;IsNotNum, String&gt; &#123; @Override public void initialize(IsNotNum isNotNum) &#123; &#125; @Override public boolean isValid(String str, ConstraintValidatorContext ctx) &#123; Pattern pattern = Pattern.compile("^\\d+$"); return !pattern.matcher(str).matches(); &#125;&#125; 在 Java Bean 中使用自定义的 constraint 12@IsNotNum(message="用户名不能是纯数字，请重新输入！")private String userName; 这里可以不使用默认的 message。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器搭建ss]]></title>
    <url>%2F2019%2F02%2F07%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BAss%2F</url>
    <content type="text"><![CDATA[选服务器在 Deploy new server 中配置服务器。 Server Location ​ 选服务器的地址要结合 Server Size 进行选择，因为有一些地方的 3.5 刀/月的被选完了，vultr 是过一段时间放一批 3.5 刀/月这样的。 3.5 刀/月是最划算的，2.5刀/月的因为国内 ipv6 普及堪忧，所以不建议选取。另外，日本的不建议选取，经常被封，其他地方都是随缘但概率比日本低。新加坡最近好像也沦陷了…好像… Server Type ​ 这里选的是服务器的操作系统，这里使用默认的 centos 7 就可以了。 Server Size ​ 这里结合 Server Location 选 3.5刀/月比较实惠，2.5刀/月可能要等 ipv6 普及（广东还没有…）。 Additional Features ​ 额外参数第一项可选可不选，视是否需要 ipv6 。第四项选上比较好，方便 putty 等远程 ssh 软件连 接，忘记选上也不要紧，日后在 settings 可以改。至此，所以参数配置完毕，deploy 就可以。 测试是否被墙​ 在选完服务器之后先不用急着搭建 ss ，先看这个 IP 能不能用。在选完服务器之后会跳转到控制台页面，并显示新的机器在 installing。 ​ 这里安装可能会花几分钟，不过一般没安装完就会分配 IP 的，这时候先看这个 IP 有没有被墙。测试方法是使用 国内端口扫描测试 ，如果扫描结果是开放就证明没有被墙，关闭则表明 gg。一般测试 22 端口就够了 ​ 如果被关闭的话就就直接关掉服务器，换个地方开一台新的，它是按使用时间收费的，随便开关。也可以同一个地方过一阵再开，因为同一个地方马上开关大概率抽到同一个 ip。 搭建ss​ 在确认服务器在国内端口扫描结果为开放之后，就可以开始搭建 ss 了。通过控制台控制服务器，在 windows 上可以使用 putty 软件，在 mac 上可以使用一个叫 terminus 的软件连接。连接方式都差不多，填 ip ，端口选 22。如果不想下软件也可以用自带的，不过就是它好像不能复制粘贴，就很麻烦。 连接成功之后依次在控制台执行以下指令： 下载 shadowsocks 1wget --no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh 赋予文件权限 1chmod +x shadowsocks.sh 开始配置 1./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 按照提示依次配置密码，端口(建议 10000~65535 中选一个值，不过还没见过有冲突的，按提示用默认的也可以)，加密方式，然后等几分钟配置。 Additionally​ Vultr 因为用的人很多，经常被封，可以物色一下其他的海外服务器供应商，本文的配置方法适用于所有服务器。然后附上 Vultr 中文网上判断是否被墙的方法 https://www.vultrcn.com/4.html。 一些 shadowsocks 常用指令 重启 ss 1/etc/init.d/shadowsocks restart 开启 ss 1/etc/init.d/shadowsocks start 停止 ss 1/etc/init.d/shadowsocks stop 查看 ss 状态 1/etc/init.d/shadowsocks status BBR 加速​ TCP BBR 是谷歌出品的 TCP 拥塞控制算法。BBR 目的是要尽量跑满带宽，并且尽量不要有排队的情况。BBR 可以起到单边加速 TCP 连接的效果。Google 提交到 Linux 主线并发表在 ACM queue 期刊上的 TCP-BBR 拥塞控制算法。继承了 Google “先在生产环境上部署，再开源和发论文”的研究传统。TCP-BBR 已经再 YouTube 服务器和 Google 跨数据中心的内部广域网( B4 )上部署。由此可见出该算法的前途。TCP-BBR 的目标就是最大化利用网络上瓶颈链路的带宽。一条网络链路就像一条水管，要想最大化利用这条水管，最好的办法就是给这跟水管灌满水。 BBR 解决了两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟，高带宽的网络链路。 降低网络链路上的 buffer 占用率，从而降低延迟。非常适合慢速接入网络的用户。 ​ Google 在 2016年9月份开源了他们的优化网络拥堵算法 BBR，最新版本的 Linux 内核( 4.9-rc8 )中已经集成了该算法。 BBR 项目地址：https://github.com/google/bbr 部署 BBR 算法 yum 更新系统版本 1yum update 查看系统版本 123[root@server ~]# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) [root@server ~]# 安装elrepo并升级内核 123[root@server ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org[root@server ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm[root@server ~]# yum --enablerepo=elrepo-kernel install kernel-ml -y 更新grub文件并重启系统 12345678[root@server ~]# egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \'CentOS Linux 7 Rescue 8619ff5e1306499eac41c02d3b23868e (4.14.14-1.el7.elrepo.x86_64)CentOS Linux (4.14.14-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-693.11.6.el7.x86_64) 7 (Core)CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-c73a5ccf3b8145c3a675b64c4c3ab1d4) 7 (Core)[root@server ~]# grub2-set-default 0[root@server ~]# reboot 重启完成后查看内核是否已更换为4.14版本 123[root@server ~]# uname -r4.14.14-1.el7.elrepo.x86_64[root@server ~]# 开启bbr 123[root@server ~]# vim /etc/sysctl.conf # 在文件末尾添加如下内容net.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbr 加载系统参数 123456[root@vultr ~]# sysctl -pnet.ipv6.conf.all.accept_ra = 2net.ipv6.conf.eth0.accept_ra = 2net.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbr[root@vultr ~]# 如上，输出了我们添加的那两行配置代表正常。 确定bbr已经成功开启 12345[root@vultr ~]# sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = bbr cubic reno[root@vultr ~]# lsmod | grep bbrtcp_bbr 20480 2 [root@vultr ~]# 输出内容如上，则表示bbr已经成功开启。]]></content>
  </entry>
  <entry>
    <title><![CDATA[python文本替换小工具]]></title>
    <url>%2F2019%2F01%2F11%2Fpython%E6%96%87%E6%9C%AC%E6%9B%BF%E6%8D%A2%E5%B0%8F%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Python开发环境Python 3.6.0 具体代码注：Python3 开始 file() 方法将被 open() 方法代替 123456fp1=open('1.txt','r')fp2=open('2.txt','w')for s in fp1.readlines(): fp2.write(s.replace('\t',','))fp1.close()fp2.close() 注解 fp1 指定需要被修改的文本文件 fp2 指定修改后的文本文件 replace(s1,s2) 把 s1 替换成 s2]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序通过Java后台获取openid]]></title>
    <url>%2F2019%2F01%2F08%2F%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%80%9A%E8%BF%87Java%E5%90%8E%E5%8F%B0%E8%8E%B7%E5%8F%96openid%2F</url>
    <content type="text"><![CDATA[写在前面Client: 微信小程序 Server: Java Servlet running on local Tomcat 9.0 Tools: 微信开发者工具 &amp;&amp; Eclipse ​ 获取思路参考试水微信小程序与Java后台通信一文，我们可以快速建立起小程序与 Java 后台之间的通信。而获取 openid 之前，我们首先要知道微信小程序官方如何定义 openid 的工作机制。参考微信小程序公众平台的开发文档：小程序登录，可以得知 openid 的工作机制主要为下图所示： 由此可以得知小程序若想在后台获取到 openid 就必须在前端发送一个临时生成的 code 到 Java 后台，然后 Java 后台使用 code 向微信相关 API 请求并获得 session_key 以及 openid。请求的 API 为： 1https://api.weixin.qq.com/sns/jscode2session?appid=xxx&amp;secret=xxx&amp;js_code=xxx&amp;grant_type=authorization_code 其中 appid 和 secret 秘钥需要在开发者平台的开发设置中获取，且 secret 秘钥不会明文保存，生成后记得保存下来，否则如果忘记需要重新生成。js_code 则是小程序传回的临时 code。 小程序端小程序端制作一个简单的测试界面，并在 js 中向后台发送生成的 code： 123456//app.jsApp(&#123; globalData: &#123; userInfo: null &#125;&#125;) 123456789101112131415161718192021&lt;!--index.wxml--&gt;&lt;view&gt; &lt;view class="userinfo"&gt; &lt;block wx:if="&#123;&#123;!hasUserInfo &amp;&amp; canIUse&#125;&#125;"&gt; &lt;image class="userinfo-avatar" src="&#123;&#123;usernoneSrc&#125;&#125;" mode="cover"&gt;&lt;/image&gt; &lt;button open-type="getUserInfo" bindgetuserinfo="getUserInfo" style='margin-bottom:50rpx' bindtap="login"&gt; 点击授权 &lt;/button&gt; &lt;/block&gt; &lt;block wx:else&gt; &lt;image class="userinfo-avatar" src="&#123;&#123;userInfo.avatarUrl&#125;&#125;" mode="cover"&gt;&lt;/image&gt; &lt;text class="userinfo-nickname"&gt;&#123;&#123;userInfo.nickName&#125;&#125;&lt;/text&gt; &lt;/block&gt; &lt;/view&gt; &lt;view class="vipText"&gt; &lt;block wx:if="&#123;&#123;vipFlag&#125;&#125;"&gt; &lt;text style='color:orange;border:1px solid orange;border-radius:25%;'&gt;vip&lt;/text&gt; &lt;/block&gt; &lt;block wx:else&gt; &lt;text style='color:#eee;border:1px solid #eee;border-radius: 25%;'&gt;vip&lt;/text&gt; &lt;/block&gt; &lt;/view&gt;&lt;/view&gt; 1234567891011121314151617181920212223242526.userinfo &#123; display: flex; flex-direction: column; align-items: center; border-bottom: 2px solid #eee;&#125;.userinfo-avatar &#123; width: 200rpx; height: 200rpx; margin: 50rpx; border-radius: 50%; border: 1px solid #eee;&#125;.userinfo-nickname &#123; color: #aaa; font-size: 60rpx; margin-bottom: 50rpx;&#125;.vipText &#123; width: 100%; text-align: center; margin-top: 50rpx;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768const app = getApp()Page(&#123; data: &#123; userInfo: &#123;&#125;, usernoneSrc: "/images/user.png", //vipFlag: true, hasUserInfo: false, canIUse: wx.canIUse('button.open-type.getUserInfo') &#125;, //登录获取code login: function () &#123; wx.login(&#123; success: function (res) &#123; //发送请求 wx.request(&#123; url: 'http://localhost:8080/smallAPP/ConnectTest', //接口地址 data: &#123; code: res.code &#125;, header: &#123; 'content-type': 'application/x-www-form-urlencoded' &#125;, success: function (res) &#123; console.log(res.data); &#125;, fail: function (res) &#123; console.log("Fail to connect..."); &#125; &#125;) &#125; &#125;) &#125;, onLoad: function () &#123; if (app.globalData.userInfo) &#123; this.setData(&#123; userInfo: app.globalData.userInfo, hasUserInfo: true &#125;) &#125; else if (this.data.canIUse) &#123; // 由于 getUserInfo 是网络请求，可能会在 Page.onLoad 之后才返回 // 所以此处加入 callback 以防止这种情况 app.userInfoReadyCallback = res =&gt; &#123; this.setData(&#123; userInfo: res.userInfo, hasUserInfo: true &#125;) &#125; &#125; else &#123; // 在没有 open-type=getUserInfo 版本的兼容处理 wx.getUserInfo(&#123; success: res =&gt; &#123; app.globalData.userInfo = res.userInfo this.setData(&#123; userInfo: res.userInfo, hasUserInfo: true &#125;) &#125; &#125;) &#125; &#125;, getUserInfo: function (e) &#123; console.log(e) app.globalData.userInfo = e.detail.userInfo this.setData(&#123; userInfo: e.detail.userInfo, hasUserInfo: true &#125;) &#125;&#125;) 界面如下： Serverlib除了试水微信小程序与Java后台通信一文中的 json 相关包，我们还需要导入一个用于进行 Http 请求的 Apache 的工具：HttpClient。该工具本来位于 Apache 的 Commons 项目中，但是后来置于 Apache 的 HttpComponents 项目中了，具体说明详见 Apache Commons 以及 Jakarta Commons HttpClient。 Download HttpClient Servlet123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.smallAPP.common;import java.io.IOException;import java.io.PrintWriter;import java.util.HashMap;import java.util.Map;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.apache.http.client.HttpClient;import org.apache.http.client.ResponseHandler;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.BasicResponseHandler;import org.apache.http.impl.client.DefaultHttpClient;import net.sf.json.JSONObject;@WebServlet("/ConnectTest")public class ConnectTest extends HttpServlet &#123; private static final long serialVersionUID = 1L; public ConnectTest() &#123; super(); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123;//设置请求编码 request.setCharacterEncoding("utf-8"); response.setContentType("text/html;charset=utf-8"); String code = request.getParameter("code"); System.out.println(code); String appSecret = "对应secret秘钥"; String appId = "对应APPID"; if (code != null) &#123; //获取openid和access_token的连接 String getOpenIdUrl = "https://api.weixin.qq.com/sns/jscode2session?appid=" + appId +"&amp;secret=" + appSecret + "&amp;js_code=" + code +"&amp;grant_type=authorization_code"; System.out.println(getOpenIdUrl); //获取返回的code HttpClient httpClient = new DefaultHttpClient(); HttpGet httpGet = new HttpGet(getOpenIdUrl); ResponseHandler&lt;String&gt; responseHandler = new BasicResponseHandler(); //向微信发送请求并获取response String responseBody = httpClient.execute(httpGet,responseHandler); System.out.println("=========================获取token==================="); System.out.println(responseBody); JSONObject jsonObject = JSONObject.fromObject(responseBody); System.out.println(jsonObject.toString()); &#125; //转成json数据 Map&lt;String, Object&gt; result = new HashMap&lt;String, Object&gt;(); result.put("data", code); result.put("msg", "后台已收到"); JSONObject object = JSONObject.fromObject(result); PrintWriter out = response.getWriter(); out.print(object.toString()); out.close(); &#125; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; 测试在小程序界面点击绑定了发送 code 代码的按钮后，小程序端获取授权以及用户头像等基本信息，如下图： 此时观察后台的结果，看是否获取到 openid： 如上图所示已经成功获取了 openid。 一些坑 HttpClient 这个工具包已经换位置了。 如果请求返回有错误，请检查请求的 API 一万遍！因为比较长，很容易拼错。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>微信小程序</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的IO]]></title>
    <url>%2F2018%2F12%2F29%2FJava%E7%9A%84IO%2F</url>
    <content type="text"><![CDATA[File 文件类 Stream 流 文件字节流 文件字符流 中文问题 关闭流的方式 缓存流 数据流 对象流 System.in/out 一些补充 File 文件类创建文件对象的构造方法： 方法 功能 File(String pathname) 通过将给定路径名字符串转换成抽象路径名来创建一个新 File 实例。(其中路径可以是相对路径也可以是绝对路径) File(File parent, String child) 通过给定的父抽象路径名和子路径名字符串创建一个新的File实例。 File(String parent, String child) 根据 parent 路径名字符串和 child 路径名字符串创建一个新 File 实例。 File(URI uri) 通过将给定的 file: URI 转换成一个抽象路径名来创建一个新的 File 实例。 文件常用方法： 方法 功能 exists() 判断文件是否存在 isDirectory() 判断是否是文件夹 isFile() 判断是否是文件 length() 获取文件的长度 lastModified() 文件最后修改时间的 long 值，可以配合 Date 类输出直观时间 setLastModified(long time) 设置文件修改时间为 time renameTo(File dest) 文件重命名 list() 以字符串数组的形式，返回当前文件夹下的所有文件(不包含子文件及子文件夹) listFiles() 以文件数组的形式，返回当前文件夹下的所有文件(不包含子文件及子文件夹) getParent() 以字符串形式返回获取所在文件夹 getParentFile() 以文件形式返回获取所在文件夹 mkdir() 创建文件夹，如果父文件夹不存在，创建就无效 mkdirs() 创建文件夹，如果父文件夹不存在，就会创建父文件夹 creatNewFile() 创建一个空文件，如果父文件夹不存在，就会抛出异常。所以在创建一个空文件之前，通常会先创建父目录:filename.getParentFile().mkdirs() listRoots() 列出所有的盘符 c: d: e: 等等 delete() 删除文件 deleteOnExit() JVM结束的时候，删除文件，常用于临时文件的删除 Stream 流Java 语言定义了许多类专门负责各种方式的输入或者输出，这些类都被放在 java.io 包中。 流的作用：为了永久性的保存数据。 分类：根据数据流向的不同分为输入流和输出流；根据处理数据类型的不同分为字符流和字节流。其中，所有输入流类都是抽象类 InputStream (字节输入流)，或者抽象类 Reader (字符输入流)的子类；而所有输出流都是抽象类 OutputStream (字节输出流)或者 Writer (字符输出流)的子类。 文件字节流FileInputStreamInputStream 是字节输入流，同时也是抽象类，只提供方法声明，不提供方法的具体实现。FileInputStream 是 InputStream 子类，以 FileInputStream 为例进行文件读取: 12345678910111213141516171819202122232425262728293031package stream; import java.io.File;import java.io.FileInputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; try &#123; //准备文件lol.txt其中的内容是AB，对应的ASCII分别是65 66 File f =new File("d:/lol.txt"); //创建基于文件的输入流 FileInputStream fis =new FileInputStream(f); //创建字节数组，其长度就是文件的长度 byte[] all =new byte[(int) f.length()]; //以字节流的形式读取文件所有内容 fis.read(all); for (byte b : all) &#123; //打印出来是65 66 System.out.println(b); &#125; //每次使用完流，都应该进行关闭 fis.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 一些常用方法： 方法 功能 public void close() throws IOException{} 关闭此文件输入流并释放与此流有关的所有系统资源。抛出 IOException 异常。 protected void finalize()throws IOException {} 这个方法清除与该文件的连接。确保在不再引用文件输入流时调用其 close 方法。抛出 IOException 异常。 public int read(int r)throws IOException{} 这个方法从 InputStream 对象读取指定字节的数据。返回为整数值。返回下一字节数据，如果已经到结尾则返回 -1。 public int read(byte[] r) throws IOException{} 这个方法从输入流读取 r.length 长度的字节。返回读取的字节数。如果是文件结尾则返回 -1。 public int available() throws IOException{} 返回下一次对此输入流调用的方法可以不受阻塞地从此输入流读取的字节数。返回一个整数值。 FileOutputStreamOutputStream是字节输出流，同时也是抽象类，只提供方法声明，不提供方法的具体实现。FileOutputStream 是 OutputStream 子类，以 FileOutputStream 为例向文件写出数据。(注: 如果文件d:/lol2.txt不存在，写出操作会自动创建该文件。但是如果是文件 d:/xyz/lol2.txt，而目录xyz又不存在，会抛出异常。) 123456789101112131415161718192021222324252627package stream; import java.io.File;import java.io.FileOutputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; try &#123; // 准备文件lol2.txt其中的内容是空的 File f = new File("d:/lol2.txt"); // 准备长度是2的字节数组，用88,89初始化，其对应的字符分别是X,Y byte data[] = &#123; 88, 89 &#125;; // 创建基于文件的输出流 FileOutputStream fos = new FileOutputStream(f); // 把数据写入到输出流 fos.write(data); // 关闭输出流 fos.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 一些常用方法： 方法 功能 public void close() throws IOException{} 关闭此文件输入流并释放与此流有关的所有系统资源。抛出 IOException 异常。 protected void finalize()throws IOException {} 这个方法清除与该文件的连接。确保在不再引用文件输入流时调用其 close 方法。抛出 IOException 异常。 public void write(int w)throws IOException{} 这个方法把指定的字节写到输出流中。 public void write(byte[] w) 把指定数组中 w.length 长度的字节写到 OutputStream 中。 文件字符流FileReaderFileReader 是 Reader 子类，该类按字符读取流中数据。可以通过以下几种构造方法创建需要的对象。 FileReader(File file) FileReader(FileDescriptor fd) FileReader(String fileName) 以 FileReader 为例进行文件读取： 1234567891011121314151617181920212223242526package stream; import java.io.File;import java.io.FileReader;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; // 准备文件lol.txt其中的内容是AB File f = new File("d:/lol.txt"); // 创建基于文件的Reader try (FileReader fr = new FileReader(f)) &#123; // 创建字符数组，其长度就是文件的长度 char[] all = new char[(int) f.length()]; // 以字符流的形式读取文件所有内容 fr.read(all); for (char b : all) &#123; // 打印出来是A B System.out.println(b); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 一些常用方法： 方法 功能 public int read() throws IOException 读取单个字符，返回一个int型变量代表读取到的字符。 public int read(char [] c, int offset, int len) 读取字符到c数组，返回读取到字符的个数。 FileWriterFileWriter 是 Writer 的子类，该类按字符向流中写入数据。可以通过以下几种构造方法创建需要的对象。 FileWriter(File file) FileWriter(File file, boolean append) FileWriter(FileDescriptor fd) FileWriter(String fileName, boolean append) append参数：若为 true 则在文件末尾追加，若为 false 覆盖文件内容。 以 FileWriter 为例把字符串写入到文件： 123456789101112131415161718192021222324package stream; import java.io.File;import java.io.FileWriter;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; // 准备文件lol2.txt File f = new File("d:/lol2.txt"); // 创建基于文件的Writer try (FileWriter fr = new FileWriter(f)) &#123; // 以字符流的形式把数据写入到文件中 String data="abcdefg1234567890"; char[] cs = data.toCharArray(); fr.write(cs); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 一些常用方法： 方法 功能 public void write(int c) throws IOException 写入单个字符c。 public void write(char [] c, int offset, int len) 写入字符数组中开始为offset长度为len的某一部分。 public void write(String s, int offset, int len) 写入字符串中开始为offset长度为len的某一部分。 中文问题用 FileInputStream 字节流正确读取中文为了能够正确的读取中文内容 必须了解文本是以哪种编码方式保存字符的 使用字节流读取了文本后，再使用对应的编码方式去识别这些数字，得到正确的字符 以“中”为例子： 123456789101112131415161718192021222324252627282930313233343536package stream; import java.io.File;import java.io.FileInputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; File f = new File("E:\\project\\j2se\\src\\test.txt"); try (FileInputStream fis = new FileInputStream(f);) &#123; byte[] all = new byte[(int) f.length()]; fis.read(all); //文件中读出来的数据是 System.out.println("文件中读出来的数据是："); for (byte b : all) &#123; int i = b&amp;0x000000ff; //只取16进制的后两位 System.out.println(Integer.toHexString(i)); &#125; System.out.println("把这个数字，放在GBK的棋盘上去："); String str = new String(all,"GBK"); System.out.println(str); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125;/*Output:文件中读出来的数据是：d6d0把这个数字，放在GBK的棋盘上去：中*/ 用FileReader 字符流正确读取中文FileReader 得到的是字符，所以一定是已经把字节根据某种编码识别成了字符了。而 FileReader 使用的编码方式是 Charset.defaultCharset() 的返回值，如果是中文的操作系统，就是 GBK。FileReader 是不能手动设置编码方式的，为了使用其他的编码方式，只能使用 InputStreamReader 来代替，像这样： 1new InputStreamReader(new FileInputStream(f),Charset.forName("UTF-8")); 以“中”为例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package stream; import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.UnsupportedEncodingException;import java.nio.charset.Charset; public class TestStream &#123; public static void main(String[] args) throws UnsupportedEncodingException, FileNotFoundException &#123; File f = new File("E:\\project\\j2se\\src\\test.txt"); System.out.println("默认编码方式:"+Charset.defaultCharset()); //FileReader得到的是字符，所以一定是已经把字节根据某种编码识别成了字符了 //而FileReader使用的编码方式是Charset.defaultCharset()的返回值，如果是中文的操作系统，就是GBK try (FileReader fr = new FileReader(f)) &#123; char[] cs = new char[(int) f.length()]; fr.read(cs); System.out.printf("FileReader会使用默认的编码方式%s,识别出来的字符是：%n",Charset.defaultCharset()); System.out.println(new String(cs)); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; //FileReader是不能手动设置编码方式的，为了使用其他的编码方式，只能使用InputStreamReader来代替 //并且使用new InputStreamReader(new FileInputStream(f),Charset.forName("UTF-8")); 这样的形式 try (InputStreamReader isr = new InputStreamReader(new FileInputStream(f),Charset.forName("UTF-8"))) &#123; char[] cs = new char[(int) f.length()]; isr.read(cs); System.out.printf("InputStreamReader 指定编码方式UTF-8,识别出来的字符是：%n"); System.out.println(new String(cs)); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125;/*Output:默认编码方式:GBKFileReader会使用默认的编码方式%s,识别出来的字符是：(乱码)InputStreamReader 指定编码方式UTF-8,识别出来的字符是：?中*/ 解释： 为什么中字前面有一个?如果是使用记事本另存为UTF-8的格式，那么在第一个字节有一个标示符，叫做 BOM 用来标志这个文件是用 UTF-8 来编码的。 关闭流的方式在 try 中关闭在try的作用域里关闭文件输入流，在前面的示例中都是使用这种方式，这样做有一个弊端；如果文件不存在，或者读取的时候出现问题而抛出异常，那么就不会执行这一行关闭流的代码，存在巨大的资源占用隐患。 不推荐使用 1234567891011121314151617181920212223package stream; import java.io.File;import java.io.FileInputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; try &#123; File f = new File("d:/lol.txt"); FileInputStream fis = new FileInputStream(f); byte[] all = new byte[(int) f.length()]; fis.read(all); for (byte b : all) &#123; System.out.println(b); &#125; // 在try 里关闭流 fis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 在 finally 中关闭这是标准的关闭流的方式： 首先把流的引用声明在 try 的外面，如果声明在 try 里面，其作用域无法抵达 finally。 在 finally 关闭之前，要先判断该引用是否为空。 关闭的时候，需要再一次进行 try catch 处理。 这是标准的严谨的关闭流的方式，但是看上去很繁琐，所以写不重要的或者测试代码的时候，都会采用上面的有隐患try的方式，因为不麻烦~ 12345678910111213141516171819202122232425262728293031package stream; import java.io.File;import java.io.FileInputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; File f = new File("d:/lol.txt"); FileInputStream fis = null; try &#123; fis = new FileInputStream(f); byte[] all = new byte[(int) f.length()]; fis.read(all); for (byte b : all) &#123; System.out.println(b); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 在finally 里关闭流 if (null != fis) try &#123; fis.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 使用 try() 的方式把流定义在 try() 里, try, catch 或者 finally 结束的时候，会自动关闭。这种编写代码的方式叫做 try-with-resources， 这是从 JDK7 开始支持的技术。所有的流，都实现了一个接口叫做 AutoCloseable，任何类实现了这个接口，都可以在 try() 中进行实例化。 并且在try, catch, finally 结束的时候自动关闭，回收相关资源。 12345678910111213141516171819202122package stream; import java.io.File;import java.io.FileInputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; File f = new File("d:/lol.txt"); //把流定义在try()里,try,catch或者finally结束的时候，会自动关闭 try (FileInputStream fis = new FileInputStream(f)) &#123; byte[] all = new byte[(int) f.length()]; fis.read(all); for (byte b : all) &#123; System.out.println(b); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 缓存流以介质是硬盘为例，字节流和字符流的弊端：在每一次读写的时候，都会访问硬盘。 如果读写的频率比较高的时候，其性能表现不佳。 为了解决以上弊端，采用缓存流。缓存流在读取的时候，会一次性读较多的数据到缓存中，以后每一次的读取，都是在缓存中访问，直到缓存中的数据读取完毕，再到硬盘中读取。 缓存流在写入数据的时候，会先把数据写入到缓存区，直到缓存区达到一定的量，才把这些数据，一起写入到硬盘中去。按照这种操作模式，就不会像字节流，字符流那样每写一个字节都访问硬盘，从而减少了IO操作。 使用缓存流读取数据缓存字符输入流 BufferedReader 可以一次读取一行数据： 12345678910111213141516171819202122232425262728293031323334package stream; import java.io.BufferedReader;import java.io.File;import java.io.FileReader;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; // 准备文件lol.txt其中的内容是 // garen kill teemo // teemo revive after 1 minutes // teemo try to garen, but killed again File f = new File("d:/lol.txt"); // 创建文件字符流 // 缓存流必须建立在一个存在的流的基础上 try ( FileReader fr = new FileReader(f); BufferedReader br = new BufferedReader(fr); ) &#123; while (true) &#123; // 一次读一行 String line = br.readLine(); if (null == line) break; System.out.println(line); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 使用缓存流写出数据PrintWriter 缓存字符输出流， 可以一次写出一行数据： 123456789101112131415161718192021222324252627package stream; import java.io.File;import java.io.FileWriter;import java.io.IOException;import java.io.PrintWriter; public class TestStream &#123; public static void main(String[] args) &#123; // 向文件lol2.txt中写入三行语句 File f = new File("d:/lol2.txt"); try ( // 创建文件字符流 FileWriter fw = new FileWriter(f); // 缓存流必须建立在一个存在的流的基础上 PrintWriter pw = new PrintWriter(fw); ) &#123; pw.println("garen kill teemo"); pw.println("teemo revive after 1 minutes"); pw.println("teemo try to garen, but killed again"); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; flush有的时候，需要立即把数据写入到硬盘，而不是等缓存满了才写出去。 这时候就需要用到 flush： 1234567891011121314151617181920212223242526package stream; import java.io.File;import java.io.FileWriter;import java.io.IOException;import java.io.PrintWriter;public class TestStream &#123; public static void main(String[] args) &#123; //向文件lol2.txt中写入三行语句 File f =new File("d:/lol2.txt"); //创建文件字符流 //缓存流必须建立在一个存在的流的基础上 try(FileWriter fr = new FileWriter(f);PrintWriter pw = new PrintWriter(fr);) &#123; pw.println("garen kill teemo"); //强制把缓存中的数据写入硬盘，无论缓存是否已满 pw.flush(); pw.println("teemo revive after 1 minutes"); pw.flush(); pw.println("teemo try to garen, but killed again"); pw.flush(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 数据流DataInputStream 数据输入流DataOutputStream 数据输出流 直接进行字符串的读写使用数据流的 writeUTF() 和 readUTF() 可以进行数据的格式化顺序读写。如本例，通过 DataOutputStream 向文件顺序写出布尔值，整数和字符串。然后再通过 DataInputStream 顺序读入这些数据。 注：要用 DataInputStream 读取一个文件，这个文件必须是由 DataOutputStream 写出的，否则会出现 EOFException，因为 DataOutputStream 在写出的时候会做一些特殊标记，只有 DataInputStream 才能成功的读取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package stream; import java.io.DataInputStream;import java.io.DataOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException; public class TestStream &#123; public static void main(String[] args) &#123; write(); read(); &#125; private static void read() &#123; File f =new File("d:/lol.txt"); try ( FileInputStream fis = new FileInputStream(f); DataInputStream dis =new DataInputStream(fis); )&#123; boolean b= dis.readBoolean(); int i = dis.readInt(); String str = dis.readUTF(); System.out.println("读取到布尔值:"+b); System.out.println("读取到整数:"+i); System.out.println("读取到字符串:"+str); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static void write() &#123; File f =new File("d:/lol.txt"); try ( FileOutputStream fos = new FileOutputStream(f); DataOutputStream dos =new DataOutputStream(fos); )&#123; dos.writeBoolean(true); dos.writeInt(300); dos.writeUTF("123 this is gareen"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;/*Output:读取到布尔值:true读取到整数:300读取到字符串:123 this is gareen*/ 对象流对象流指的是可以直接把一个对象以流的形式传输给其他的介质，比如硬盘。 一个对象以流的形式进行传输，叫做序列化。 该对象所对应的类，必须是实现 Serializable 接口。 序列化一个对象创建一个 Hero 对象，设置其名称为 garen。把该对象序列化到一个文件 garen.lol。然后再通过序列化把该文件转换为一个 Hero 对象。 注：把一个对象序列化有一个前提是：这个对象的类，必须实现了 Serializable 接口。 TestStream.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445package stream; import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream; import charactor.Hero; public class TestStream &#123; public static void main(String[] args) &#123; //创建一个Hero garen //要把Hero对象直接保存在文件上，务必让Hero类实现Serializable接口 Hero h = new Hero(); h.name = "garen"; h.hp = 616; //准备一个文件用于保存该对象 File f =new File("d:/garen.lol"); try( //创建对象输出流 FileOutputStream fos = new FileOutputStream(f); ObjectOutputStream oos =new ObjectOutputStream(fos); //创建对象输入流 FileInputStream fis = new FileInputStream(f); ObjectInputStream ois =new ObjectInputStream(fis); ) &#123; oos.writeObject(h); Hero h2 = (Hero) ois.readObject(); System.out.println(h2.name); System.out.println(h2.hp); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; Hero.java 1234567891011package charactor; import java.io.Serializable; public class Hero implements Serializable &#123; //表示这个类当前的版本，如果有了变化，比如新设计了属性，就应该修改这个版本号 private static final long serialVersionUID = 1L; public String name; public float hp; &#125; System.in/outSystem.out 是常用的在控制台输出数据的System.in 可以从控制台输入数据 System.in1234567891011121314151617181920212223package stream; import java.io.IOException;import java.io.InputStream; public class TestStream &#123; public static void main(String[] args) &#123; // 控制台输入 try (InputStream is = System.in;) &#123; while (true) &#123; // 敲入a,然后敲回车可以看到 // 97 13 10 // 97是a的ASCII码 // 13 10分别对应回车换行 int i = is.read(); System.out.println(i); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Scanner读取字符串使用 System.in.read 虽然可以读取数据，但是很不方便。使用 Scanner 就可以逐行读取了： 12345678910111213141516package stream; import java.util.Scanner; public class TestStream &#123; public static void main(String[] args) &#123; Scanner s = new Scanner(System.in); while(true)&#123; String line = s.nextLine(); System.out.println(line); &#125; &#125;&#125; Scanner从控制台读取整数java.util.Scanner 是 Java5 的新特征，我们可以通过 Scanner 类来获取用户的输入。 使用Scanner从控制台读取整数: 12345678910111213141516171819package stream; import java.util.Scanner; public class TestStream &#123; public static void main(String[] args) &#123; Scanner s = new Scanner(System.in); int a = s.nextInt(); System.out.println("第一个整数："+a); int b = s.nextInt(); System.out.println("第二个整数："+b); &#125;&#125;/*Output:123第一个整数：123456第二个整数：456*/ 一些补充上述常用流关系图 flush() 和 close() 的区别 flush()方法 用来刷新缓冲区的，刷新后可以再次写出。字节缓冲流内置缓冲区，如果没有读取出来，可以使用 flush() 刷新来。 close()方法 用来关闭流释放资源的的，如果是带缓冲区的流对象的 close() 方法，不但会关闭流，还会再关闭流之前刷新缓冲区，关闭后不能再写出。 什么情况下使用字符流？ 字符流也可以拷贝文本文件，但不推荐使用。 因为读取时会把字节转为字符，写出时还要把字符转回字节。 程序需要读取一段文本，或者需要写出一段文本的时候可以使用字符流。 读取的时候是按照字符的大小读取的，不会出现半个中文。 写出的时候可以直接将字符串写出，不用转换为字节数组。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[试水微信小程序与Java后台通信]]></title>
    <url>%2F2018%2F12%2F28%2F%E8%AF%95%E6%B0%B4%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E4%B8%8EJava%E5%90%8E%E5%8F%B0%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[写在前面Client: 微信小程序 Server: Java Servlet running on local Tomcat 9.0 Tools: 微信开发者工具 &amp;&amp; Eclipse Client弄一个简陋的微信小程序进行测试，wxml 只需要绘制一个向后台发送信息的按钮即可： 1234&lt;!--index.wxml--&gt;&lt;view class="container"&gt; &lt;button bindtap='connecttest'&gt;test&lt;/button&gt;&lt;/view&gt; 在 js 中绑定按键发送请求并在 console 中输出返回值： 12345678910111213141516171819202122232425//index.js//获取应用实例const app = getApp()Page(&#123; connecttest: function () &#123; wx.request(&#123; url: 'http://localhost:8080/smallAPP/ConnectTest', data: &#123; username: 'pomo', password: '123' &#125;, method: 'POST', header: &#123; //'content-type': 'application/json' // 默认值以 json 形式发送请求 'content-type': 'application/x-www-form-urlencoded' &#125;, success: function (res) &#123; console.log(res.data); &#125;, fail: function (res) &#123; console.log("Fail to connect..."); &#125; &#125;) &#125;)&#125; 到目前为止，Client 已经完成。 Serverlib在 Eclipse 中新建一个名叫 smallApp 的 Dynamic Web Project 。先在 WEB-INF 的 lib 中导入 json 相关包：json-lib-2.4-jdk15.jar download 但是这里要注意一个陷阱，这个 json 包需要依赖其他包才能正常运行： jakarta commons-lang 2.5 jakarta commons-beanutils 1.8.0 jakarta commons-collections 3.2.1 jakarta commons-logging 1.1.1 ezmorph 1.0.6 官方网址：http://json-lib.sourceforge.net/ DTO定义一个名叫 user 的 DTO 并带有 userName 和 password 两个属性： 123456789101112131415161718192021package com.smallAPP.common;public class User &#123; private String userName; private String password; public User() &#123; super(); &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125; Servlet在 DTO 的同一目录下新建一个 Servlet: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.smallAPP.common;import java.io.IOException;import java.io.PrintWriter;import java.util.HashMap;import java.util.Map;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import net.sf.json.JSONObject;@WebServlet("/ConnectTest")public class ConnectTest extends HttpServlet &#123; private static final long serialVersionUID = 1L; public ConnectTest() &#123; super(); &#125; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //设置请求编码 request.setCharacterEncoding("utf-8"); response.setContentType("text/html;charset=utf-8"); /* 设置响应头允许ajax跨域访问 */ //response.setHeader("Access-Control-Allow-Origin", "*"); /* 星号表示所有的异域请求都可以接受， */ //response.setHeader("Access-Control-Allow-Methods", "GET,POST"); User user = new User(); //获取微信小程序get的参数值并打印 user.setUserName(request.getParameter("username")); user.setPassword(request.getParameter("password")); System.out.println("username="+user.getUserName()+" ,password="+user.getPassword()); //转成json数据 Map&lt;String, Object&gt; result = new HashMap&lt;String, Object&gt;(); result.put("data", user); result.put("msg", "后台已收到"); JSONObject object = JSONObject.fromObject(result); PrintWriter out = response.getWriter(); out.print(object.toString()); out.close(); &#125; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; 这个 Servlet 的功能就是把 Client 发送过来的 userName 和 password 在控制台输出，然后把这两个信息转换成 json 数据返回给微信小程序。 测试在微信开发者工具详情中设置不校验域名(若非本地服务器测试，需要配置好服务器，服务器配置好之后无需关心跨域问题)： 在 Eclipse 运行 Servlet，然后点击微信小程序页面上的 test 按钮向后台发送请求，在 Eclipse 中查看后台的接收结果： 然后再观察微信小程序的 console： 如上图所示，微信小程序与 Java 后台成功进行通讯。]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>微信小程序</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP中常用的状态码]]></title>
    <url>%2F2018%2F11%2F19%2FHTTP%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E7%8A%B6%E6%80%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[状态码状态码的职责是当客户端向服务器发送请求时，描述返回的请求结果。 状态码数字中的第一位指定了响应类别，后两位无分类。响应类别有以下五种： 类别 原因短语 1xx Informational (信息性状态码) 接收的请求正在处理 2xx Success (成功状态码) 请求正常处理完毕 3xx Redirection (重定向状态码) 需要进行附加操作以完成请求 4xx Client Error (客户端错误状态码) 服务器无法处理请求 5xx Server Error (服务器错误状态码) 服务器处理请求出错 仅记录在 rfc2616 的 HTTP 状态码就有 40 多种，再加上其他 rfc 文档的扩展就更多了，不过 fortunately，常用的可以归纳为 14 种。 常用状态码2xx 成功2xx 的响应结果表明请求被正常处理了。 状态码 说明 200 OK 客户端发来的请求在服务端被正常处理了。 204 No Content 服务器接收的请求已成功处理，但在返回的响应报文中不含实体的主体部分。另外，也不允许返回任何实体的主体。一般在只需要从客户端往服务端发送信息，而对客户端不需要发送新信息内容的情况下使用。 206 Partial Content 客户端进行了范围请求，而服务器成功执行了这部分的 GET 请求，响应报文中包含由 Content-Range 指定范围的实体内容。 3xx 重定向3xx 响应结果表明浏览器需要执行某些特殊的处理以正确处理请求。 状态码 说明 301 Moved Permanently 永久性重定向。该状态码表示请求的资源已被分配了新的 URI，以后应使用资源现在所指的 URI。也就是说，如果已经把资源对应的 URI 保存为书签了，这时候应该按 Location 首部字段提示的 URI 重新保存。 302 Found 临时性重定向。该状态码表示请求的资源已被分配了新的 URI，希望用户(本次)能使用新的 URI 访问。和 301 类似，但它不会更改书签。 303 See Other 该状态码表示由于请求对应的资源存在着另一个 URI，应使用 GET 方法定向获取请求的资源。和 302 有着同样的功能，但 303 明确表示客户端应采用 GET 方法获取资源。 304 Not Modified 该状态码表示客户端发送附带条件的请求时，服务器端允许请求访问资源，但因发生请求未满足条件的情况后，直接返回 304 Not Modified (服务器端资源未改变，可直接使用客户端未过期的缓存)。304 状态码返回时，不包含任何响应的主体部分。 307 Temporary Redirect 临时重定向。和 302 Found 含义相同。尽管 302 标准禁止 POST 变换成 GET，但实际使用时大家并不遵守。307 会遵守浏览器标准，不会从 POST 变成 GET，但是不同浏览器处理响应的方式并不相同。 当 301、302、303 响应状态码返回时，几乎所有的浏览器都会把 POST 改成 GET，并删除请求报文内的主体，之后请求会自动再次发送。301 和 302 标准是禁止将 POST 方法改变成 GET 方法的，但实际使用时大家都会这么做。 4xx 客户端错误4xx 的响应结果表明客户端是发生错误的原因所在。 状态码 说明 400 Bad Request 该状态码表示请求报文中存在语法错误。当错误发生时，需修改请求的内容后再次发送请求。另外浏览器会像对待 200 OK 一样对待该状态码。 401 Unauthorized 该状态码表示发送的请求需要有通过 HTTP 认证( BASIC认证、DIGEST 认证 )的认证信息。另外若之前已进行过 1 次请求，则表示用户认证失败。返回含有 401 的响应必须包含一个适用于被请求资源的 WWW-Authenticate 首部用以质询（ challenge）用户信息。当浏览器初次接收到 401 响应，会弹出认证用的对话窗口。 403 Forbidden 该状态码表示对请求资源的访问被服务器拒绝了。服务器端没有必要给出拒绝的详细理由，但如果想作说明的话，可以在实体的主体部分对原因进行描述，这样就能让用户看见了。未获得文件系统的访问授权，访问权限出现了某些问题（从未授权的发送源 IP 地址试图访问）等列举的情况都可能是发生 403 的原因。 404 Not Found 该状态码表明服务器上无法找到请求的资源。除此之外，也可以在服务器端拒绝请求且不想说明理由时使用。 5xx 服务器错误5xx 的响应结果表明服务器本身发生错误。 状态码 说明 500 internal Server Error 该状态码表明服务器端在执行请求时发生了错误。也有可能是 Web 应用存在的 bug 或某些临时的故障。 503 Service Unavailable 该状态码表明服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。如果事先得知解除以上状况需要的时间，最好写入 Retry-After 首部字段再返回给客户端。 状态码和状况不一致不少返回的状态码响应都是错误的，但是用户可能察觉不到这一点。比如 Web 应用程序内部发生错误，状态码依然返回 200 OK，这种情况也经常会遇到。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java之封装 继承 多态]]></title>
    <url>%2F2018%2F11%2F15%2FJava%E4%B9%8B%E5%B0%81%E8%A3%85%20%E7%BB%A7%E6%89%BF%20%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[封装在面向对象程式设计方法中，封装(Encapsulation)是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。封装可以被认为是一个保护屏障，防止该类的代码和数据被外部类定义的代码随机访问。要访问该类的代码和数据，必须通过严格的接口控制。封装最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。适当的封装可以让程式码更容易理解与维护，也加强了程式码的安全性。 封装的优点 良好的封装能够减少耦合。 类内部的结构可以自由修改。 可以对成员变量进行更精确的控制。 隐藏信息，实现细节。 为什么要封装封装符合面向对象设计原则的第一条：单一性原则，一个类把自己该做的事情封装起来，而不是暴露给其他类去处理，当内部的逻辑发生变化时，外部调用不用因此而修改，他们只调用开放的接口，而不用去关心内部的实现。 如何实现Java封装 修改属性的可见性来限制对属性的访问（一般限制为private），例如： 1234public class Person &#123; private String name; private int age;&#125; 这段代码中，将 name 和 age 属性设置为私有的，只能本类才能访问，其他类都访问不了，如此就对信息进行了隐藏。 对每个值属性提供对外的公共方法访问，也就是创建一对赋取值方法，用于对私有属性的访问，例如： 1234567891011121314151617181920public class Person&#123; private String name; private int age; public int getAge()&#123; return age; &#125; public String getName()&#123; return name; &#125; public void setAge(int age)&#123; this.age = age; &#125; public void setName(String name)&#123; this.name = name; &#125;&#125; 采用 this 关键字是为了解决实例变量（private String name）和局部变量（ setName(String name)中的name变量）之间发生的同名的冲突。 Java封装使用实例封装类： 123456789101112131415161718192021222324252627282930public class EncapTest&#123; private String name; private String idNum; private int age; public int getAge()&#123; return age; &#125; public String getName()&#123; return name; &#125; public String getIdNum()&#123; return idNum; &#125; public void setAge( int newAge)&#123; age = newAge; &#125; public void setName(String newName)&#123; name = newName; &#125; public void setIdNum( String newId)&#123; idNum = newId; &#125;&#125; 以上实例中 public 方法是外部类访问该类成员变量的入口。通常情况下，这些方法被称为 getter 和 setter 方法。因此，任何要访问类中私有成员变量的类都要通过这些 getter 和 setter 方法。通过如下的例子说明 EncapTest 类的变量怎样被访问： 1234567891011121314public class RunEncap&#123; public static void main(String args[])&#123; EncapTest encap = new EncapTest(); encap.setName("James"); encap.setAge(20); encap.setIdNum("12343ms"); System.out.print("Name : " + encap.getName()+ " Age : "+ encap.getAge()); &#125;&#125;/*Output:Name : James Age : 20*/ 继承继承是java面向对象编程技术的一块基石，因为它允许创建分等级层次的类。继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。继承需要符合的关系是：is-a，父类更通用，子类更具体。 Java 的类可以分为三类： 类：使用 class 定义，没有抽象方法 抽象类：使用 abstract class 定义，可以有也可以没有抽象方法 接口：使用 inerface 定义，只能有抽象方法 在这三个类型之间存在如下关系： 类可以 extends：类、抽象类（必须实现所有抽象方法），但只能 extends 一个，可以 implements 多个接口（必须实现所有接口方法） 抽象类可以 extends：类，抽象类（可全部、部分、或者完全不实现父类抽象方法），可以 implements 多个接口（可全部、部分、或者完全不实现接口方法） 接口只能 extends 一个接口 继承以后子类可以得到什么： 子类拥有父类非 private 的属性和方法 子类可以添加自己的方法和属性，即对父类进行扩展 子类可以重新定义父类的方法，即多态里面的覆盖，后面会详述 类的继承格式在 Java 中通过 extends 关键字可以申明一个类是从另外一个类继承而来的，一般形式如下： 12345class 父类 &#123;&#125; class 子类 extends 父类 &#123;&#125; 继承类型 继承的特性 子类拥有父类非private的属性，方法。 子类可以拥有自己的属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 Java的继承是单继承，但是可以多重继承，单继承就是一个子类只能继承一个父类，多重继承就是，例如A类继承B类，B类继承C类，所以按照关系就是C类是B类的父类，B类是A类的父类，这是java继承区别于C++继承的一个特性。 提高了类之间的耦合性（继承的缺点，耦合度高就会造成代码之间的联系越紧密，代码独立性越差）。 继承的关键字继承可以使用 extends 和 implements 这两个关键字来实现继承，而且所有的类都是继承于 java.lang.Object，当一个类没有继承的两个关键字，则默认继承 object（这个类在 java.lang 包中，所以不需要 import）祖先类。 extends 关键字：在 Java 中，类的继承是单一继承，也就是说，一个子类只能拥有一个父类，所以 extends 只能继承一个类。 implements 关键字：使用 implements 关键字可以变相的使 java 具有多继承的特性，使用范围为类继承接口的情况，可以同时继承多个接口（接口跟接口之间采用逗号分隔）。 super 关键字：我们可以通过 super 关键字来实现对父类成员的访问，用来引用当前对象的父类。 this 关键字：指向自己的引用。 final 关键字：final 关键字声明类可以把类定义为不能继承的，即最终类；或者用于修饰方法，该方法不能被子类重写。实例变量也可以被定义为 final，被定义为 final 的变量不能被修改。被声明为 final 类的方法自动地声明为 final，但是实例变量并不是 final。 关于继承中的构造器子类是不继承父类的构造器（构造方法或者构造函数）的，它只是调用（隐式或显式）。如果父类的构造器带有参数，则必须在子类的构造器中显式地通过 super 关键字调用父类的构造器并配以适当的参数列表。如果父类构造器没有参数，则在子类的构造器中不需要使用 super 关键字调用父类构造器，系统会自动调用父类的无参构造器。 1234567891011121314151617181920212223242526272829303132333435class SuperClass &#123; private int n; SuperClass()&#123; System.out.println("SuperClass()"); &#125; SuperClass(int n) &#123; System.out.println("SuperClass(int n)"); this.n = n; &#125;&#125;class SubClass extends SuperClass&#123; private int n; SubClass()&#123; super(300); System.out.println("SubClass"); &#125; public SubClass(int n)&#123; System.out.println("SubClass(int n):"+n); this.n = n; &#125;&#125;public class TestSuperSub&#123; public static void main (String args[])&#123; SubClass sc = new SubClass(); SubClass sc2 = new SubClass(200); &#125;&#125;/*Output:SuperClass(int n)SubClassSuperClass()SubClass(int n):200*/ 多态多态是同一个行为具有多个不同表现形式或形态的能力。多态就是同一个接口，使用不同的实例而执行不同操作。多态的好处：可以使程序有良好的扩展，并可以对所有类的对象进行通用处理。 多态存在的三个必要条件 继承：在多态中必须存在有继承关系的子类和父类。 重写：子类对父类中某些方法进行重新定义，在调用这些方法时就会调用子类的方法。 向上转型：在多态中需要将子类的引用赋给父类对象，只有这样该引用才能够具备技能调用父类的方法和子类的方法。比如： 1Parent p = new Child(); 多态中的方法当使用多态方式调用方法时，首先检查父类中是否有该方法，如果没有，则编译错误；如果有，再去调用子类的同名方法。 当子类对象调用重写的方法时，调用的是子类的方法，而不是父类中被重写的方法。 要想调用父类中被重写的方法，则必须使用关键字super。 多态的实现 继承 基于继承的实现机制主要表现在父类和继承该父类的一个或多个子类对某些方法的重写，多个子类对同一方法的重写可以表现出不同的行为。 对于引用子类的父类类型，在处理该引用时，它适用于继承该父类的所有子类，子类对象的不同，对方法的实现也就不同，执行相同动作产生的行为也就不同。 如果父类是抽象类，那么子类必须要实现父类中所有的抽象方法，这样该父类所有的子类一定存在统一的对外接口，但其内部的具体实现可以各异。这样我们就可以使用顶层类提供的统一接口来处理该层次的方法。 接口 继承是通过重写父类的同一方法的几个不同子类来体现的，接口就是通过实现接口并覆盖接口中同一方法的不同的类体现的。 在接口的多态中，指向接口的引用必须指定是实现了该接口的一个类的实例程序，在运行时，根据对象引用的实际类型来执行对应的方法。 继承都是单继承，只能为一组相关的类提供一致的服务接口。但是接口可以是多继承多实现，它能够利用一组相关或者不相关的接口进行组合与扩充，能够对外提供一致的服务接口。所以它相对于继承来说有更好的灵活性。 多态的经典实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 传入参数的类型不同，使用的方法也不同，可传入D or A，输出AD or AApublic class A &#123; public String show(D obj) &#123; return ("A and D"); &#125; public String show(A obj) &#123; return ("A and A"); &#125;&#125; //B继承自A，可传入B or A，输出BB or BApublic class B extends A&#123; public String show(B obj)&#123; return ("B and B"); &#125; public String show(A obj)&#123; return ("B and A"); &#125; &#125; //C继承自Bpublic class C extends B&#123; &#125; //D继承自Bpublic class D extends B&#123; &#125; public class Test &#123; public static void main(String[] args) &#123; A a1 = new A(); A a2 = new B(); B b = new B(); C c = new C(); D d = new D(); System.out.println("1--" + a1.show(b)); System.out.println("2--" + a1.show(c)); System.out.println("3--" + a1.show(d)); System.out.println("4--" + a2.show(b)); System.out.println("5--" + a2.show(c)); System.out.println("6--" + a2.show(d)); System.out.println("7--" + b.show(b)); System.out.println("8--" + b.show(c)); System.out.println("9--" + b.show(d));//B b = new B(); ，B中无Show（D），所以看super父类A，A中有SHow（D），所以输出AD &#125; &#125;/*Output:1--A and A 2--A and A 3--A and D 4--B and A 5--B and A 6--A and D 7--B and B 8--B and B 9--A and D*/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java的String,StringBuffer和StringBuilder类]]></title>
    <url>%2F2018%2F11%2F12%2FJava%E7%9A%84String-StringBuffer%E5%92%8CStringBuilder%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[String StringBuffer 和 StringBuilder 三者区别 一些常用字符串实例 三者继承结构： String 创建字符串 字符串长度 一些字符串常用方法 String对象是不可变的，任何String类看起来会修改String值的方法实际上都是创建一个新的String对象。 特点： final：String 被修饰为final,所以是不能被继承的。 immutable：immutable 是指不可改变的，里面的内容永远无法改变，不可改变的具体含义是指：不能增加长度，不能减少长度，不能插入字符，不能删除字符，不能修改字符。String 的表现就像是一个常量。 创建字符串字符串即字符的组合，在Java中，字符串是一个类，所以我们见到的字符串都是对象 。String 类有 11 种构造方法，这些方法提供不同的参数来初始化字符串，常见创建字符串手段： 每当有一个字面值出现的时候，虚拟机就会创建一个字符串 调用String的构造方法创建一个字符串对象 通过+加号进行字符串拼接也会创建新的字符串对象 12345678910111213141516package character; public class TestString &#123; public static void main(String[] args) &#123; String garen ="盖伦"; //字面值,虚拟机碰到字面值就会创建一个字符串对象 String teemo = new String("提莫"); //创建了两个字符串对象 char[] cs = new char[]&#123;'崔','斯','特'&#125;; String hero = new String(cs);// 通过字符数组创建一个字符串对象 String hero3 = garen + teemo;// 通过+加号进行字符串拼接 &#125;&#125; new String()和new String(“”)都是申明一个新的空字符串，是空串不是null 字符串长度用于获取有关对象的信息的方法称为访问器方法。String 类的一个访问器方法是 length() 方法，它返回字符串对象包含的字符数，可以有长度为0的字符串，即空字符串。用法 stringName.length()。 一些字符串常用方法 StringBuffer 和 StringBuilder当对字符串进行修改的时候，需要使用 StringBuffer 和 StringBuilder 类。 和 String 类不同的是，StringBuffer 和 StringBuilder 类的对象能够被多次的修改，并且不产生新的未使用对象。 StringBuilder 类在 Java 5 中被提出，它和 StringBuffer 之间的最大不同在于 StringBuilder 的方法不是线程安全的（不能同步访问）。 由于 StringBuilder 相较于 StringBuffer 有速度优势，所以多数情况下建议使用 StringBuilder 类。然而在应用程序要求线程安全的情况下，则必须使用 StringBuffer 类。 StringBuffer常用方法 方法 描述 public StringBuffer append(String s) 将指定的字符串追加到此字符序列。 public StringBuffer reverse() 将此字符序列用其反转形式取代。 public delete(int start, int end) 移除此序列的子字符串中的字符。 public insert(int offset, int i) 将 int 参数的字符串表示形式插入此序列中。 replace(int start, int end, String str) 使用给定 String 中的字符替换此序列的子字符串中的字符。 int capacity() 返回当前容量。 char charAt(int index) 返回此序列中指定索引处的 char 值。 void ensureCapacity(int minimumCapacity) 确保容量至少等于指定的最小值。 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin) 将字符从此序列复制到目标字符数组 dst。 int indexOf(String str) 返回第一次出现的指定子字符串在该字符串中的索引。 int indexOf(String str, int fromIndex) 从指定的索引处开始，返回第一次出现的指定子字符串在该字符串中的索引。 int lastIndexOf(String str) 返回最右边出现的指定子字符串在此字符串中的索引。 int lastIndexOf(String str, int fromIndex) 返回 String 对象中子字符串最后出现的位置。 int length() 返回长度（字符数）。 void setCharAt(int index, char ch) 将给定索引处的字符设置为 ch。 void setLength(int newLength) 设置字符序列的长度。 CharSequence subSequence(int start, int end) 返回一个新的字符序列，该字符序列是此序列的子序列。 String substring(int start) 返回一个新的 String，它包含此字符序列当前所包含的字符子序列。 String substring(int start, int end) 返回一个新的 String，它包含此序列当前所包含的字符子序列。 String toString() 返回此序列中数据的字符串表示形式。 三者区别 运行速度：StringBuilder &gt; StringBuffer &gt; String why：String长度不可变，每次操作都是不断地创建新的对象并不断触发GC(Garbage Collection，垃圾回收机制)。而StringBuilder则不会，它从头到尾一直是一个实例对象。实现方法是初始化的时候放在一个叫value的char数组里，而数组是可以扩容的，这样就无需创建新的对象。源码中，数组默认的初始长度是16,也可以根据构造方法指定。扩容系数: value.length * 2 + 2,而且只有当append是数据长度+value.count &gt; value.length时才会扩容一次,不会每次都扩容去调用Arrays.copyof()。 线程安全：在线程安全上，StringBuilder 是线程不安全的，而 StringBuffer 是线程安全的。如果要进行的操作是多线程的，那么就要使用 StringBuffer，但是在单线程的情况下，还是建议使用速度比较快的 StringBuilder。 初始化上的区别：String可以空赋值，StringBuffer 和 StringBuilder 不行。 字符修改上的区别： String：不可变字符串 StringBuffer：可变字符串 StringBuilder：可变字符序列 适用场景 String：适用于少量的字符串操作的情况 StringBuilder：适用于单线程下在字符缓冲区进行大量操作的情况 StringBuffer：适用多线程下在字符缓冲区进行大量操作的情况 一些常用字符串实例以下内容搬运自菜鸟教程 字符串比较 查找字符串最后一次出现的位置 删除字符串中的一个字符 字符串替换 字符串反转 字符串查找 字符串分割 字符串分割(StringTokenizer)) 字符串小写转大写 测试两个字符串区域是否相等 字符串性能比较测试 字符串优化 字符串格式化 连接字符串 字符串比较以下实例中我们通过字符串函数 compareTo (string) ，compareToIgnoreCase(String) 及 compareTo(object string) 来比较两个字符串，并返回字符串中第一个字母ASCII的差值。 实例代码如下： 12345678910111213141516public class StringCompareEmp&#123; public static void main(String args[])&#123; String str = "Hello World"; String anotherString = "hello world"; Object objStr = str; System.out.println( str.compareTo(anotherString) ); System.out.println( str.compareToIgnoreCase(anotherString) ); //忽略大小写 System.out.println( str.compareTo(objStr.toString())); &#125;&#125;/*Output:-3200*/ compareTo() 方法的实现思路：两个字符数组依次从前开始比较，如果对象位置出现字符不同则返回两个字符的编码之差，后面的字符不再比较；如果两个字符数组的长度不一样，并且较短的数组和较长数组所有对应位置的字符都相同，则返回两个数组的长度之差。 查找字符串最后一次出现的位置以下实例中我们通过字符串函数 strOrig.lastIndexOf(Stringname) 来查找子字符串 Stringname 在 strOrig 出现的位置： 实例代码如下： 1234567891011121314public class SearchlastString &#123; public static void main(String[] args) &#123; String strOrig = "Hello world ,Hello Runoob"; int lastIndex = strOrig.lastIndexOf("Runoob"); if(lastIndex == - 1)&#123; System.out.println("没有找到字符串 Runoob"); &#125;else&#123; System.out.println("Runoob 字符串最后出现的位置： "+ lastIndex); &#125; &#125;&#125;/*Output:Runoob 字符串最后出现的位置： 19*/ 删除字符串中的一个字符以下实例中我们通过字符串函数 substring() 函数来删除字符串中的一个字符，我们将功能封装在 removeCharAt 函数中。 实例代码如下： 123456789101112public class Main &#123; public static void main(String args[]) &#123; String str = "this is Java"; System.out.println(removeCharAt(str, 3)); &#125; public static String removeCharAt(String s, int pos) &#123; return s.substring(0, pos) + s.substring(pos + 1); &#125;&#125;/*Output:thi is Java*/ 字符串替换以下实例中我们使用 java String 类的 replace 方法来替换字符串中的字符： 12345678910111213public class StringReplaceEmp&#123; public static void main(String args[])&#123; String str="Hello World"; System.out.println( str.replace( 'H','W' ) ); System.out.println( str.replaceFirst("He", "Wa") ); System.out.println( str.replaceAll("He", "Ha") ); &#125;&#125;/*Output:Wello WorldWallo WorldHallo World*/ 字符串反转以下实例演示了如何使用 Java 的反转函数 reverse() 将字符串反转： 123456789101112public class StringReverseExample&#123; public static void main(String[] args)&#123; String string="runoob"; String reverse = new StringBuffer(string).reverse().toString(); System.out.println("字符串反转前:"+string); System.out.println("字符串反转后:"+reverse); &#125;&#125;/*Output:字符串反转前:runoob字符串反转后:boonur*/ 字符串查找以下实例使用了 String 类的 indexOf() 方法在字符串中查找子字符串出现的位置，如果存在返回字符串出现的位置（第一位为0），如果不存在返回 -1： 1234567891011121314public class SearchStringEmp &#123; public static void main(String[] args) &#123; String strOrig = "Google Runoob Taobao"; int intIndex = strOrig.indexOf("Runoob"); if(intIndex == - 1)&#123; System.out.println("没有找到字符串 Runoob"); &#125;else&#123; System.out.println("Runoob 字符串位置 " + intIndex); &#125; &#125;&#125;/*Output:Runoob 字符串位置 7*/ 字符串分割以下实例使用了 split(string) 方法通过指定分隔符将字符串分割为数组： 12345678910111213141516public class JavaStringSplitEmp &#123; public static void main(String args[])&#123; String str1 = "www.runoob.com"; String[] temp1; String delimeter1 = "\\."; // 指定分割字符， . 号需要转义 temp1 = str1.split(delimeter1); // 分割字符串 for(String x : temp1)&#123; System.out.println(x); &#125; &#125;&#125;/*Output:wwwrunoobcom*/ 字符串分隔(StringTokenizer)Java 中我们可以使用 StringTokenizer 设置不同分隔符来分隔字符串，默认的分隔符是：空格、制表符（\t）、换行符(\n）、回车符（\r）。 以下实例演示了 StringTokennizer 使用空格和等号来分隔字符串: 123456789101112131415161718192021222324252627282930313233import java.util.StringTokenizer; public class Main &#123; public static void main(String[] args) &#123; String str = "This is String , split by StringTokenizer, created by runoob"; StringTokenizer st = new StringTokenizer(str); System.out.println("----- 通过空格分隔 ------"); while (st.hasMoreElements()) &#123; System.out.println(st.nextElement()); &#125; System.out.println("----- 通过逗号分隔 ------"); StringTokenizer st2 = new StringTokenizer(str, ","); while (st2.hasMoreElements()) &#123; System.out.println(st2.nextElement()); &#125; &#125;&#125;/*Output:----- 通过空格分隔 ------ThisisString,splitbyStringTokenizer,createdbyrunoob----- 通过等号分隔 ------This is String split by StringTokenizer created by runoob*/ 字符串小写转大写以下实例使用了 String toUpperCase() 方法将字符串从小写转为大写： 123456789101112public class StringToUpperCaseEmp &#123; public static void main(String[] args) &#123; String str = "string runoob"; String strUpper = str.toUpperCase(); System.out.println("原始字符串: " + str); System.out.println("转换为大写: " + strUpper); &#125;&#125;/*Output:原始字符串: string runoob转换为大写: STRING RUNOOB*/ 测试两个字符串区域是否相等以下实例使用了 regionMatches() 方法测试两个字符串区域是否相等： 12345678910111213141516public class StringRegionMatch&#123; public static void main(String[] args)&#123; String first_str = "Welcome to Microsoft"; String second_str = "I work with microsoft"; boolean match1 = first_str. regionMatches(11, second_str, 12, 9); boolean match2 = first_str. regionMatches(true, 11, second_str, 12, 9); //第一个参数 true 表示忽略大小写区别 System.out.println("区分大小写返回值：" + match1); System.out.println("不区分大小写返回值：" + match2); &#125;&#125;/*Output:区分大小写返回值：false 不区分大小写返回值：true*/ first_str.regionMatches(11, second_str, 12, 9) 表示将 first_str 字符串从第11个字符”M”开始和 second_str 字符串的第12个字符”M”开始逐个比较，共比较 9 对字符，由于字符串区分大小写，所以结果为false。如果设置第一个参数为 true ，则表示忽略大小写区别，所以返回 true。 字符串性能比较测试以下实例演示了通过两种方式创建字符串，并测试其性能： 123456789101112131415161718192021222324252627public class StringComparePerformance&#123; public static void main(String[] args)&#123; long startTime = System.currentTimeMillis(); for(int i=0;i&lt;50000;i++)&#123; String s1 = "hello"; String s2 = "hello"; &#125; long endTime = System.currentTimeMillis(); System.out.println("通过 String 关键词创建字符串" + " : "+ (endTime - startTime) + " 毫秒" ); long startTime1 = System.currentTimeMillis(); for(int i=0;i&lt;50000;i++)&#123; String s3 = new String("hello"); String s4 = new String("hello"); &#125; long endTime1 = System.currentTimeMillis(); System.out.println("通过 String 对象创建字符串" + " : " + (endTime1 - startTime1) + " 毫秒"); &#125;&#125;/*Output:通过 String 关键词创建字符串 : 6 毫秒 通过 String 对象创建字符串 : 14 毫秒*/ 字符串优化intern()方法设计的初衷，就是重用String对象，以节省内存消耗。但是缺点是比较耗时。 以下实例演示了通过 String.intern() 方法来优化字符串： 12345678910111213141516171819202122232425262728293031323334public class StringOptimization &#123; public static void main(String[] args)&#123; String variables[] = new String[50000]; for( int i=0;i &lt;50000;i++)&#123; variables[i] = "s"+i; &#125; long startTime0 = System.currentTimeMillis(); for(int i=0;i&lt;50000;i++)&#123; variables[i] = "hello"; &#125; long endTime0 = System.currentTimeMillis(); System.out.println("直接使用字符串： "+ (endTime0 - startTime0) + " ms" ); long startTime1 = System.currentTimeMillis(); for(int i=0;i&lt;50000;i++)&#123; variables[i] = new String("hello"); &#125; long endTime1 = System.currentTimeMillis(); System.out.println("使用 new 关键字：" + (endTime1 - startTime1) + " ms"); long startTime2 = System.currentTimeMillis(); for(int i=0;i&lt;50000;i++)&#123; variables[i] = new String("hello"); variables[i] = variables[i].intern(); &#125; long endTime2 = System.currentTimeMillis(); System.out.println("使用字符串对象的 intern() 方法: " + (endTime2 - startTime2) + " ms"); &#125;&#125;/*Output:直接使用字符串： 3 ms使用 new 关键字：5 ms使用字符串对象的 intern() 方法: 10 ms*/ 存在于 .class 文件中的常量池(常量池( constant pool )指的是在编译期被确定，并被保存在已编译的 .class 文件中的一些数据。它包括了关于类、方法、接口等中的常量，也包括字符串常量。)，在运行期被 JVM 装载，并且可以扩充。String 的 intern() 方法就是扩充常量池的一个方法；当一个 String 实例 str 调用 intern() 方法时，Java 查找常量池中是否有相同 Unicode 的字符串常量，如果有，则返回其的引用，如果没有，则在常量池中增加一个Unicode等于str的字符串并返回它的引用。 字符串格式化以下实例演示了通过 format() 方法来格式化字符串，还可以指定地区来格式化： 123456789101112import java.util.*;public class StringFormat &#123; public static void main(String[] args)&#123; double e = Math.E; System.out.format("%f%n", e); System.out.format(Locale.CHINA , "%-10.4f%n%n", e); //指定本地为中国（CHINA） &#125;&#125;/*Output:2.7182822.7183*/ 连接字符串以下实例演示了通过 “+” 操作符和StringBuffer.append() 方法来连接字符串，并比较其性能： 123456789101112131415161718192021222324252627282930313233343536public class Main &#123; public static void main(String[] args)&#123; String result1 = null; StringBuffer result = new StringBuffer(); long startTime = System.currentTimeMillis(); for(int i=0;i&lt;5000;i++)&#123; result1 += "This is" + "testing the" + "difference"+ "between" + "String"+ "and"+ "StringBuffer"; &#125; long endTime = System.currentTimeMillis(); System.out.println("字符串连接" + " - 使用 + 操作符 : " + (endTime - startTime)+ " ms"); long startTime1 = System.currentTimeMillis(); for(int i=0;i&lt;5000;i++)&#123; result.append("This is"); result.append("testing the"); result.append("difference"); result.append("between"); result.append("String"); result.append("and"); result.append("StringBuffer"); &#125; long endTime1 = System.currentTimeMillis(); System.out.println("字符串连接" + " - 使用 StringBuffer : " + (endTime1 - startTime1)+ " ms"); &#125;&#125;/*Output:字符串连接 - 使用 + 操作符 : 1151 ms字符串连接 - 使用 StringBuffer : 2 ms*/ 当需要对字符串对象的长度进行变化时，用 + 拼接的性能在循环时就会慢的慢的多，实际上 + 号拼接字符串也是通过 StringBuild 或 StringBuffer 实现的，但当进行频繁的修改本身时，+ 拼接会比直接用方法拼接产生更多的中间垃圾对象，耗用更多的内存，因此更推荐使用 StringBuild。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java数组]]></title>
    <url>%2F2018%2F11%2F12%2FJava%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[Java 里的数组 声明数组变量 初始化数组 处理数组 多维数组 复制数组 Arrays类 一些常见数组实例 Java 里的数组​数组是一个固定长度的，包含了相同类型数据的无序排列的容器。 ​在 Java 中，数组属于引用数据类型。访问时直接由下标访问。数组只有一个唯一的属性，length。length 可以用来获取数组中能存储的元素个数。用法为： 数组名.length。(注: length 的计数从1开始，数组下标的计数从0开始，所以索引值从 0 到 arrayRefVar.length - 1) ​在存储空间的分配上，数组名和数组的首地址被存放在内存的栈空间，而数组的内容被分配在堆空间。数组操作中，只开辟了栈内的空间，数组是无法使用的，必须有指向堆内存才可以使用，开辟堆内存空间需要使用new关键字，之后就是将对内存的使用权交给对应的栈内存，一个堆内存空间可以被多个栈内存指向。二维数组元素空间的分配有时会分配为一个大空间，有时会分配为多个小空间，视内存空间而定。二维数组根据定义方式的不同，就有可能变为并不规则的数组，行数乘列数就不一定等于数组长度。 声明数组变量首先必须声明数组变量，才能在程序中使用数组。下面是声明数组变量的语法： 12dataType[] arrayRefVar; // 首选的方法dataType arrayRefVar[]; // 效果相同，但不是首选方法 初始化数组​Java 数组对象自动初始化为 null，范围检查保证了数组无法越界访问。数组元素中的基本数据类型值会自动初始化成空值(对于数字和字符，就是0;对于布尔型，是 false )。如果创建的是一个引用数组，需要先创建新的对象，并把对象赋值给引用，初始化才算结束。如果忘记了创建对象，并且试图使用数组中的空引用，就会在运行时产生异常。 使用 new 操作符来创建数组： 1arrayRefVar = new dataType[arraySize]; 数组变量的声明，和创建数组可以用一条语句完成，如下所示： 123dataType[] arrayRefVar = new dataType[arraySize];dataType[] arrayRefVar = &#123;value0, value1, ..., valuek&#125;;dataType[] arrayRefVar = new dataType[]&#123;value0, value1, ..., valuek&#125;; 静态初始化和动态初始化​静态初始化和动态初始化开始都指定了数组大小，java也在栈空间分配了相应的大小，只是静态初始化开始就指定了值，而动态初始化是在后面指定数组的值，刚开始数组的值都是默认值。 ​数组静态初始化，初始化为给定值： 1int[] a=&#123;1,2,3,4&#125;; ​数组动态初始化，初始化为空值： 12int[] intArray;//声明数组intArray = new int[5];//为数组分配空间 处理数组​数组的元素类型和数组的大小都是确定的，所以当处理数组元素时候，我们通常使用基本循环或者 foreach 循环。 123456789101112131415public class TestArray &#123; public static void main(String[] args) &#123; double[] myList = &#123;1.9, 2.9, 3.4, 3.5&#125;; // 普通for语法打印所有数组元素 for (int i = 0; i &lt; myList.length; i++) &#123; System.out.println(myList[i]); &#125; // foreach语法打印所有数组元素 for (double element: myList) &#123; System.out.println(element); &#125; &#125; &#125; foreach​JDK 1.5 引进了一种新的循环类型，被称为 foreach 循环或者加强型循环，它能在不使用下标的情况下遍历数组。语法： 12for(Type ATypeName : TheSameTypeArray) &#123; statement; &#125; 多维数组​多维数组可以看成是数组的数组，比如二维数组就是一个特殊的一维数组，其每一个元素都是一个一维数组，例如： 1String str[][] = new String[3][4]; ​对二维数组中的每个元素，引用方式为 arrayName[index1][index2]。 复制数组​System 提供了一个静态方法 arraycopy()，我们可以使用它来实现数组之间的复制。用法： 1System.arraycopy(src, srcPos, dest, destPos, length) src: 源数组 srcPos: 从源数组复制数据的起始位置 dest: 目标数组 destPos: 复制到目标数组的起始位置 length: 复制的长度 Arrays类​java.util.Arrays 类能方便地操作数组，可以进行排序，查找，复制填充等功能，它提供的所有方法都是静态的。常用方法如下： copyOfRange fill sort toString()) binarySearch equals copyOfRange 功能：数组复制 说明：与 arraycopy() 类似的， Arrays提供了一个copyOfRange方法进行数组复制。不同的是System.arraycopy，需要事先准备好目标数组，并分配长度。 copyOfRange 只需要源数组就就可以了，通过返回值，就能够得到目标数组了。除此之外，需要注意的是 copyOfRange 的第3个参数，表示源数组的结束位置，是取不到的。 实例： 1234567891011121314151617181920import java.util.Arrays; public class HelloWorld &#123; public static void main(String[] args) &#123; int a[] = new int[] &#123; 18, 62, 68, 82, 65, 9 &#125;; // copyOfRange(int[] original, int from, int to) // 第一个参数表示源数组 // 第二个参数表示开始位置(取得到) // 第三个参数表示结束位置(取不到) int[] b = Arrays.copyOfRange(a, 0, 3); for (int i = 0; i &lt; b.length; i++) &#123; System.out.print(b[i] + " "); &#125; &#125;&#125;/*Output:18 62 68*/ fill 功能：使用同一个值，填充整个数组 说明：将指定的值分配给指定类型数组指定范围中的每个元素。适用于所有的基本数据类型。 实例： 123456789101112131415import java.util.Arrays; public class HelloWorld &#123; public static void main(String[] args) &#123; int a[] = new int[10]; Arrays.fill(a, 5); System.out.println(Arrays.toString(a)); &#125;&#125;/*Output：[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]*/ sort 功能：对数组排序 说明：将指定的值分配给指定类型数组指定范围中的每个元素。适用于所有的基本数据类型。 实例： 12345678910111213141516171819import java.util.Arrays; public class HelloWorld &#123; public static void main(String[] args) &#123; int a[] = new int[] &#123; 18, 62, 68, 82, 65, 9 &#125;; System.out.println("排序之前 :"); System.out.println(Arrays.toString(a)); Arrays.sort(a); System.out.println("排序之后:"); System.out.println(Arrays.toString(a)); &#125;&#125;/*Output：排序之前：[18, 62, 68, 82, 65, 9]排序之后：[9, 18, 62, 65, 68, 82]*/ toString() 功能：转换为字符串 说明：直接把一个数组转换为字符串，这样方便观察数组的内容。 实例： 12345678910111213import java.util.Arrays; public class HelloWorld &#123; public static void main(String[] args) &#123; int a[] = new int[] &#123; 18, 62, 68, 82, 65, 9 &#125;; String content = Arrays.toString(a); System.out.println(content); &#125;&#125;/*Output:[18, 62, 68, 82, 65, 9]*/ binarySearch 功能：查找数组元素 说明：用二分查找算法在给定数组中搜索给定值的对象(Byte,Int,double等)。需要注意的是，使用 binarySearch 进行查找之前，必须使用 sort 进行排序。如果数组中有多个相同的元素，查找结果是不确定的。如果查找值包含在数组中，则返回搜索键的索引；否则返回-1或(-插入点)。插入点是索引键将要插入数组的那一点，即第一个大于该键的元素的索引。 实例： 1234567891011121314151617import java.util.Arrays; public class HelloWorld &#123; public static void main(String[] args) &#123; int a[] = new int[] &#123; 18, 62, 68, 82, 65, 9 &#125;; Arrays.sort(a); System.out.println(Arrays.toString(a)); //使用binarySearch之前，必须先使用sort进行排序 System.out.println("数字 62出现的位置:"+Arrays.binarySearch(a, 62)); &#125;&#125;/*Output:[9, 18, 62, 65, 68, 82]数字 62出现的位置:2*/ equals 功能：比较数组 说明：如果两个指定的数组彼此相等，则返回 true。如果两个数组以相同顺序包含相同的元素，则两个数组是相等的。适用于所有的基本数据类型。 实例：12345678910import java.util.Arrays; public class HelloWorld &#123; public static void main(String[] args) &#123; int a[] = new int[] &#123; 18, 62, 68, 82, 65, 9 &#125;; int b[] = new int[] &#123; 18, 62, 68, 82, 65, 8 &#125;; System.out.println(Arrays.equals(a, b)); &#125;&#125; 一些常见数组实例以下内容搬运自菜鸟教程 数组排序及元素查找 数组添加元素 获取数组长度 数组反转 数组输出 数组获取最大和最小值 数组合并 数组填充 数组扩容 查找数组中的重复元素 删除数组元素 数组差集 数组交集 在数组中查找指定元素 判断数组是否相等 数组并集 数组排序及元素查找以下实例演示了如何使用sort()方法对Java数组进行排序，及如何使用 binarySearch() 方法来查找数组中的元素, 这边我们定义了 printArray() 方法来打印数组： 1234567891011121314151617181920212223242526import java.util.Arrays; public class MainClass &#123; public static void main(String args[]) throws Exception &#123; int array[] = &#123; 2, 5, -2, 6, -3, 8, 0, -7, -9, 4 &#125;; Arrays.sort(array); printArray("数组排序结果为", array); int index = Arrays.binarySearch(array, 2); System.out.println("元素 2 在第 " + index + " 个位置"); &#125; private static void printArray(String message, int array[]) &#123; System.out.println(message + ": [length: " + array.length + "]"); for (int i = 0; i &lt; array.length; i++) &#123; if(i != 0)&#123; System.out.print(", "); &#125; System.out.print(array[i]); &#125; System.out.println(); &#125;&#125;/*Output:数组排序结果为: [length: 10] -9, -7, -3, -2, 0, 2, 4, 5, 6, 8元素 2 在第 5 个位置*/ 数组添加元素以下实例演示了如何使用sort()方法对Java数组进行排序，及如何使用 insertElement () 方法向数组插入元素, 这边我们定义了 printArray() 方法来打印数组： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Arrays; public class MainClass &#123; public static void main(String args[]) throws Exception &#123; int array[] = &#123; 2, 5, -2, 6, -3, 8, 0, -7, -9, 4 &#125;; Arrays.sort(array); printArray("数组排序", array); int index = Arrays.binarySearch(array, 1); System.out.println("元素 1 所在位置（负数为不存在）：" + index); int newIndex = -index - 1; array = insertElement(array, 1, newIndex); printArray("数组添加元素 1", array); &#125; private static void printArray(String message, int array[]) &#123; System.out.println(message + ": [length: " + array.length + "]"); for (int i = 0; i &lt; array.length; i++) &#123; if (i != 0)&#123; System.out.print(", "); &#125; System.out.print(array[i]); &#125; System.out.println(); &#125; private static int[] insertElement(int original[], int element, int index) &#123; int length = original.length; int destination[] = new int[length + 1]; System.arraycopy(original, 0, destination, 0, index); destination[index] = element; System.arraycopy(original, index, destination, index + 1, length - index); return destination; &#125;&#125;/*Output:数组排序: [length: 10] -9, -7, -3, -2, 0, 2, 4, 5, 6, 8 元素 1 所在位置（负数为不存在）：-6 数组添加元素 1: [length: 11] -9, -7, -3, -2, 0, 1, 2, 4, 5, 6, 8*/ 获取数组长度使用数组的属性 length 来获取数组的长度，以下实例中我们定义了二维数组，并获取数组的长度： 1234567891011public class Main &#123; public static void main(String args[]) &#123; String[][] data = new String[2][5]; System.out.println("第一维数组长度: " + data.length); System.out.println("第二维数组长度: " + data[0].length); &#125;&#125;/*Output:第一维数组长度: 2 第二维数组长度: 5*/ 数组反转以下实例中我们使用 Collections.reverse ( ArrayList ) 将数组进行反转： 1234567891011121314151617181920import java.util.ArrayList;import java.util.Collections; public class Main &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; arrayList = new ArrayList&lt;String&gt;(); arrayList.add("A"); arrayList.add("B"); arrayList.add("C"); arrayList.add("D"); arrayList.add("E"); System.out.println("反转前排序: " + arrayList); Collections.reverse(arrayList); System.out.println("反转后排序: " + arrayList); &#125;&#125;/*Output:反转前排序: [A, B, C, D, E] 反转后排序: [E, D, C, B, A] */ 数组输出以下实例演示了如何通过循环输出数组： 1234567891011121314151617import java.lang.*;public class Main&#123; public static void main(String args[]) &#123; String[] runoobs = new String[3]; runoobs[0] = "菜鸟教程"; runoobs[1] = "菜鸟工具"; runoobs[2] = "菜鸟笔记"; for (String obj:runoobs)&#123; System.out.println(obj+"\n"); &#125; &#125;&#125;/*Output:菜鸟教程菜鸟工具菜鸟笔记*/ 数组获取最大和最小值以下实例演示了如何通过 Collections 类的 Collections.max() 和 Collections.min() 方法来查找数组中的最大和最小值： 12345678910111213141516import java.util.Arrays;import java.util.Collections; public class Main &#123; public static void main(String[] args) &#123; Integer[] numbers = &#123; 8, 2, 7, 1, 4, 9, 5&#125;; int min = (int) Collections.min(Arrays.asList(numbers)); int max = (int) Collections.max(Arrays.asList(numbers)); System.out.println("最小值: " + min); System.out.println("最大值: " + max); &#125;&#125;/*Output:最小值: 1最大值: 9*/ 数组合并以下实例演示了如何通过 List 类的 Arrays.toString () 方法和 List 类的 list.Addall(array1.asList(array2) 方法将两个数组合并为一个数组： 1234567891011121314151617import java.util.ArrayList;import java.util.Arrays;import java.util.List; public class Main &#123; public static void main(String args[]) &#123; String a[] = &#123; "A", "E", "I" &#125;; String b[] = &#123; "O", "U" &#125;; List list = new ArrayList(Arrays.asList(a)); list.addAll(Arrays.asList(b)); Object[] c = list.toArray(); System.out.println(Arrays.toString(c)); &#125;&#125;/*Output:[A, E, I, O, U]*/ 数组填充以下实例我们通过 Java Util 类的 Arrays.fill(arrayname,value) 方法和Arrays.fill(arrayname ,starting index ,ending index ,value) 方法向数组中填充元素： 12345678910111213141516171819202122232425262728293031import java.util.*; public class FillTest &#123; public static void main(String args[]) &#123; int array[] = new int[6]; Arrays.fill(array, 100); for (int i=0, n=array.length; i &lt; n; i++) &#123; System.out.println(array[i]); &#125; System.out.println(); Arrays.fill(array, 3, 6, 50); for (int i=0, n=array.length; i&lt; n; i++) &#123; System.out.println(array[i]); &#125; &#125;&#125;/*Output:100100100100100100100100100505050*/ 数组扩容以下实例演示了如何在数组初始化后对数组进行扩容： 12345678910111213141516171819public class Main &#123; public static void main(String[] args) &#123; String[] names = new String[] &#123; "A", "B", "C" &#125;; String[] extended = new String[5]; extended[3] = "D"; extended[4] = "E"; System.arraycopy(names, 0, extended, 0, names.length); for (String str : extended)&#123; System.out.println(str); &#125; &#125;&#125;/*Output:ABCDE*/ 查找数组中的重复元素以下实例演示了如何在 java 中找到重复的元素： 1234567891011121314151617181920212223import java.util.Arrays; public class MainClass &#123; public static void main(String[] args) &#123; int[] my_array = &#123;1, 2, 5, 5, 6, 6, 7, 2&#125;; for (int i = 0; i &lt; my_array.length-1; i++) &#123; for (int j = i+1; j &lt; my_array.length; j++) &#123; if ((my_array[i] == my_array[j]) &amp;&amp; (i != j)) &#123; System.out.println("重复元素 : "+my_array[j]); &#125; &#125; &#125; &#125;&#125;/*Output:重复元素 : 2重复元素 : 5重复元素 : 6*/ 删除数组元素以下实例演示了如何使用 remove () 方法来删除数组元素： 12345678910111213141516171819import java.util.ArrayList; public class Main &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; objArray = new ArrayList&lt;String&gt;(); objArray.clear(); objArray.add(0,"第 0 个元素"); objArray.add(1,"第 1 个元素"); objArray.add(2,"第 2 个元素"); System.out.println("数组删除元素前："+objArray); objArray.remove(1); objArray.remove("第 0 个元素"); System.out.println("数组删除元素后："+objArray); &#125;&#125;/*Output:数组删除元素前：[第 0 个元素, 第 1 个元素, 第 2 个元素]数组删除元素后：[第 2 个元素]*/ 数组差集以下实例演示了如何使用 removeAll () 方法来计算两个数组的差集： 123456789101112131415161718192021222324import java.util.ArrayList; public class Main &#123; public static void main(String[] args) &#123; ArrayList objArray = new ArrayList(); ArrayList objArray2 = new ArrayList(); objArray2.add(0,"common1"); objArray2.add(1,"common2"); objArray2.add(2,"notcommon"); objArray2.add(3,"notcommon1"); objArray.add(0,"common1"); objArray.add(1,"common2"); objArray.add(2,"notcommon2"); System.out.println("array1 的元素" +objArray); System.out.println("array2 的元素" +objArray2); objArray.removeAll(objArray2); System.out.println("array1 与 array2 数组差集为："+objArray); &#125;&#125;/*Output:array1 的元素[common1, common2, notcommon2]array2 的元素[common1, common2, notcommon, notcommon1]array1 与 array2 数组差集为：[notcommon2]*/ 数组交集以下实例演示了如何使用 retainAll () 方法来计算两个数组的交集： 12345678910111213141516171819202122232425262728/*Output:array1 的元素[common1, common2, notcommon2]array2 的元素[common1, common2, notcommon, notcommon1]array1 与 array2 数组差集为：[notcommon2]*/import java.util.ArrayList; public class Main &#123; public static void main(String[] args) &#123; ArrayList objArray = new ArrayList(); ArrayList objArray2 = new ArrayList(); objArray2.add(0,"common1"); objArray2.add(1,"common2"); objArray2.add(2,"notcommon"); objArray2.add(3,"notcommon1"); objArray.add(0,"common1"); objArray.add(1,"common2"); objArray.add(2,"notcommon2"); System.out.println("array1 数组元素："+objArray); System.out.println("array2 数组元素："+objArray2); objArray.retainAll(objArray2); System.out.println("array2 &amp; array1 数组交集为："+objArray); &#125;&#125;/*Output:array1 数组元素：[common1, common2, notcommon2]array2 数组元素：[common1, common2, notcommon, notcommon1]array2 &amp; array1 数组交集为：[common1, common2]*/ 在数组中查找指定元素以下实例演示了如何使用 contains () 方法来查找数组中的指定元素： 1234567891011121314151617181920212223242526import java.util.ArrayList; public class Main &#123; public static void main(String[] args) &#123; ArrayList&lt;String&gt; objArray = new ArrayList&lt;String&gt;(); ArrayList&lt;String&gt; objArray2 = new ArrayList&lt;String&gt;(); objArray2.add(0,"common1"); objArray2.add(1,"common2"); objArray2.add(2,"notcommon"); objArray2.add(3,"notcommon1"); objArray.add(0,"common1"); objArray.add(1,"common2"); System.out.println("objArray 的数组元素："+objArray); System.out.println("objArray2 的数组元素："+objArray2); System.out.println("objArray 是否包含字符串common2? ： " +objArray.contains("common2")); System.out.println("objArray2 是否包含数组 objArray? ：" +objArray2.contains(objArray) ); &#125;&#125;/*Output:objArray 的数组元素：[common1, common2]objArray2 的数组元素：[common1, common2, notcommon, notcommon1]objArray 是否包含字符串common2? ： trueobjArray2 是否包含数组 objArray? ：false*/ 判断数组是否相等以下实例演示了如何使用 equals ()方法来判断数组是否相等： 1234567891011121314151617import java.util.Arrays; public class Main &#123; public static void main(String[] args) throws Exception &#123; int[] ary = &#123;1,2,3,4,5,6&#125;; int[] ary1 = &#123;1,2,3,4,5,6&#125;; int[] ary2 = &#123;1,2,3,4&#125;; System.out.println("数组 ary 是否与数组 ary1相等? ：" +Arrays.equals(ary, ary1)); System.out.println("数组 ary 是否与数组 ary2相等? ：" +Arrays.equals(ary, ary2)); &#125;&#125;/*Output:数组 ary 是否与数组 ary1相等? ：true数组 ary 是否与数组 ary2相等? ：false*/ 数组并集以下实例演示了如何使用 union ()方法来计算两个数组的并集： 123456789101112131415161718192021222324252627282930313233343536import java.util.Arrays;import java.util.HashSet;import java.util.Set;public class Main &#123; public static void main(String[] args) throws Exception &#123; String[] arr1 = &#123; "1", "2", "3" &#125;; String[] arr2 = &#123; "4", "5", "6" &#125;; String[] result_union = union(arr1, arr2); System.out.println("并集的结果如下："); for (String str : result_union) &#123; System.out.println(str); &#125; &#125; // 求两个字符串数组的并集，利用set的元素唯一性 public static String[] union(String[] arr1, String[] arr2) &#123; Set&lt;String&gt; set = new HashSet&lt;String&gt;(); for (String str : arr1) &#123; set.add(str); &#125; for (String str : arr2) &#123; set.add(str); &#125; String[] result = &#123; &#125;; return set.toArray(result); &#125;&#125;/*Output:并集的结果如下：321654*/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客见闻录之站内搜索]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%8D%9A%E5%AE%A2%E8%A7%81%E9%97%BB%E5%BD%95%E4%B9%8B%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[摸石过河在 Maupassant主题文档 中，我发现了该主题支持五种站内搜索： 12345google_search:baidu_search:swiftype:tinysou:self_search: 我直到踩中最后一块石头，才真正的过了河。下面是每种搜索的体 (cai) 验 (keng): 谷歌搜索: 其实就是把关键词丢进谷歌搜索，然后用 site 语法尽可能地把搜索结果定位与你的页面相关。需要等谷歌爬取你的网页，不然啥都搜不到。而且用谷歌要科学上网，第三方的站点也加载较慢。 百度搜索: 原理基本同上，不过不用科学上网了。还是要等百度爬你的页面才能用。 swiftype: 这好像是一个神器，你可以让它主动爬取的网页，没有前面的方法那么被动。但是在配置过程中出现了很多错误，比如识别不了 API key 之类的，据说这个引擎开始收费了，不知道有没有关系，反正我是没有部署成功。 tinysou: 同 swiftype 是第三方搜索引擎服务，鉴于 swiftype 的经历，不愿再折腾第三方搜索服务，未测试。 self_search: 对于上述所有搜索方式，本引擎只能说在座的各位都是lj。这是内嵌的 jQuery 搜索引擎，相比于第三方搜索服务，其用户体验顺滑无比。又快又稳!(被 swiftype 折磨得心灰意冷的时候，抱着尝试的态度使用 self_search 居然成功的感觉真有点喜极而泣的feel) 使用方法因为是站在前人的肩膀上使用，所以简单到不行： 在 hexo 根目录下的配置文件添加下列代码： 12345search: path: search.xml field: post format: html limit: 10000 在主题的配置文件中设置下列配置为true： self_search: true 啥也不说了，都是泪，上个效果图就去上课：]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
        <tag>jQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客见闻录之RSS]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%8D%9A%E5%AE%A2%E8%A7%81%E9%97%BB%E5%BD%95%E4%B9%8BRSS%2F</url>
    <content type="text"><![CDATA[安装插件npm install hexo-generator-feed --save 配置在hexo根目录下的_config.yml中添加插件引用：12345678910# Extensions## Plugins: http://hexo.io/plugins/#RSS订阅plugin:- hexo-generator-feed#Feed Atomfeed:type: atompath: atom.xmllimit: 20 然后检查一下主题目录下的_config.yml中，RSS 的目录导向是否和上述代码中的 path 一致：123- page: rss directory: atom.xml icon: fa-rss 部署上线：12hexo cleanhexo d -g 验证perfect!]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>RSS</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七牛云融合CDN加速域名]]></title>
    <url>%2F2018%2F11%2F06%2F%E4%B8%83%E7%89%9B%E4%BA%91%E8%9E%8D%E5%90%88CDN%E5%8A%A0%E9%80%9F%E5%9F%9F%E5%90%8D%2F</url>
    <content type="text"><![CDATA[前言之前使用七牛云的外链是由其分配的测试域名提供的，想起自己在腾讯云有一个域名就顺手拿自己的域名来做融合 CDN 加速。 第一次接触七牛云是时候感觉良好，毕竟以前的博客连图床都没有，这种用图片信手拈来的感觉爽的飞起。 但是这又引来一个思考了。 由于每个存储空间生成的外链需要有所区别，用户创建的每个对象存储空间都带有七牛云随机分配的一个 CDN 加速域名。那么在这种规则之下，就很难做到永久免费服务了。认真查看七牛云的服务，果不其然： 这分配的外链貌似只有30天的寿命。。 忽然想起之前在 腾讯云 买了一个三块钱一年的域名，于是就打算拿它来做融合 CDN 加速域名。 实践添加自定义域名 点击添加自定义域名： 填写完成 ICP 备案的域名，这个域名就是以后外链的开头部分，其前缀也是在域名解析时 CNAME 记录的前缀。 配置CNAME七牛云官方 CNAME 配置教程 获取 CNAME 进入 腾讯云 配置 CNAME 在域名解析列表中选择解析的域名 在解析中添加记录，填写格式如下：(前缀和在七牛云填写的域名前缀一致，实际上相当于是二级域名进行 CNAME 解析，因为一般主域名都是做了 A 解析的，而同级域名下两种 DNS 解析不能共存。) 验证与使用验证成功配置 CNAME 后等待一段时间，在七牛云控制台看到融合 CDN 加速域名状态为成功即可。 使用新域名在内容管理中修改外链默认域名： 最后记得修改调用外链的网页代码，把原来的外链替换为新的外链。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>七牛云</tag>
        <tag>CDN</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux学习笔记之常用命令]]></title>
    <url>%2F2018%2F11%2F06%2Flinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[总结了学习linux时的一些笔记,重新排版。 基础 文件处理命令 权限处理命令 文件搜索命令 帮助命令 压缩解压命令 网络通信命令 系统关机命令 shell应用技巧 基础文件命名规则:1.除/以外,所有的字符都合法2.有些字符最好不用,如空格符, 制表符, 退格符和字符@#$&amp;()-等3.避免使用.作为普通文件名的第一个字符(避免被隐藏)4.大小写敏感,严格区分大小写 命令格式:命令 -选项 参数说明: 当有多个选项时,可以写在一起 .和..分别代表当前目录和当前目录的父目录(上一级目录) 按键 功能 Ctrl+c 终止指令 Ctrl+d 退出shell Ctrl+a 光标移至行首 Ctrl+e 光标移至行末 转义符\可保留本意 用户管理 命令 改变用户 su - 用户名 添加用户 useradd 用户名 设置用户密码 passwd 用户名 由命令所在的路径可判断命令的权限: 权限 路径 root /sbin ,/usr/sbin all user /bin,/usr/bin 文件处理命令ls功能:显示目录文件路径:/bin/ls语法:ls 选项[-ald] [文件或目录] -a 显示所有文件,包括隐藏文件(all) -d 查看目录属性(directory)(一般配合l,即-ld连用查看某个目录的属性) -l 详细信息显示(long)例子:drwxr-xr-x 2 root root 4096 12-01 20:52 bin d rwxr-xr-x 2 root root 4096 12-01 20:52 bin 文件类型,常见文件类型有:d:目录 directory-: 二进制文件l: 软链接文件 link 代表权限( r-read 读, w-write 写, x-execute 执行), 所有者 u(user, owner) 权限 rwx, 所有组 g(group) 权限 r-x, 其他人 o(other) 硬链接数 管理者是root,管理组是root 文件大小(不准确),linux一般用数据块大小来做储存文件大小的单位,即数据块是最小储存单位 创建时间或最后修改时间 文件名 cd功能:切换目录路径:shell内置命令权限:所有用户语法:cd [目录]例子:1234// 切换到根目录cd / // 回到上一级目录cd .. pwd功能:显示当前所在的工作目录路径:/bin/pwd语法:pwd touch功能:创建空文件路径:/bin/touch语法:touch [文件名]例子:touch newfile mkdir功能:创建新目录路径:/bin/mkdir语法:mkdir [目录名]例子:mkdir newdir cp功能:复制文件或目录路径:/bin/cp语法:cp -R [源文件或目录][目的目录] -R 复制目录 -p 拷贝之后不改变时间值 例子:1234// 将文件file1, file2复制到目录dir1cp file1 file2 dir1// 将目录dir1下的所有文件复制到目录dir2下cp -R dir1 dir2 mv功能:移动文件, 更名路径:/bin/mv语法:mv [源文件或目录][目的目录]例子:123456// 将当前目录下文件file1更名为file3mv file1 file3// 将文件file2移动到目录dir2下mv file2 dir2// 改名和移动可同时操作mv /dir1/file1 /dir2/file2 rm功能:删除文件路径:/bin/rm语法:rm -r [文件或目录] -r 删除目录 -f 强制删除(避开询问) 例子:1234// 删除文件file1rm file1// 删除目录dir1rm -r dir1 cat功能:显示文件内容(显示大文件的话不完整)路径:/bin/cat语法:cat [文件名]例子:cat /etc/issue more功能:分页显示文件内容(可以完整显示文件)路径:/bin/more语法:more [文件名] 命令 功能 (空格)或 f 显示下一页 (Enter) 显示下一行 q 或 Q 退出 例子:more /etc/services head功能:查看文件前几行路径:/bin/head语法:head -num [文件名] -num 显示文件的前num行 例子:head -20 etc/services tail功能:查看文件后几行路径: bin/tail语法:tail -num [文件名] -num 显示文件后num行 -f 动态显示文件内容 例子:tail -30 /etc/services ln功能:产生链接文件路径:/bin/ln语法:ln -s [源文件][目标文件] -s 创建软链接 例子:1234// 创建文件/etc/issue的软链接/issue.softln -s /etc/issue/issue.soft// 创建文件/etc/issue的硬链接/issue.hardln /etc/issue/issue.hard 链接类型 等价理解 特性 软链接 类似于快捷方式 文件很小,若源文件消失,软链接失效。 硬链接 cp -p + 同步更新 与源文件有相同的inode,不能跨文件系统(不能跨系统文件分区),若源文件消失,硬链接不变。 权限处理命令chmod功能:改变文件或目录权限路径:/bin/chmod语法:123chmod u + rchmod g - wchmod o = x 关于权限值:u-所有者, g-所属组, o-其他人rwx: 可写可读可执行r-4, w-2, x-1(重点掌握)例如:rwxr-xr– 等价于 754, 752 等价于 rwxr-x-w- 例子:1234// 赋予文件file1所属组写权限chmod g+w file1// 设定目录dir1为所有用户具有全部权限chmod 777 dir1 对于权限的理解:对文件:r(可以查看文件内容)cat, more, head, tailw(可以修改文件内容)echo, vix(可以执行文件)命令, 脚本对目录:r(可以列出目录中的内容)lsw(可以在目录中创建, 删除文件)touch, mkdir, rmx(可以进入目录)cd chown功能:改变文件或目录的所有者路径:/bin/chown语法:chown [用户][文件或目录]例子:12// 改变文件file1的所有者为nobodychown nobody file1 chgrp功能:改变文件或目录的所属组路径:/bin/chgrp语法:chgrp [用户组][文件或目录]例子:12// 改变文件file1的所属组为admchgrp adm file1 umask功能:显示, 设置文件的缺省权限路径:/bin/umask语法:umask -S -S 以rwx形式显示新建文件或目录缺省权限 例子:umask直观写法:umask -S 假如umask得到00220-特殊权限位022-用户权限位,权限掩码值777 - 022 = 755755表示权限 改变默认权限:umask 权限掩码值 Linux权限规则:缺省创建的文件不能授予可执行x权限 文件搜索命令which功能:显示系统命令所在目录路径:/usr/bin/which语法:which [命令名称]例子:which ls find功能:查找文件或目录路径:/usr/bin/find语法:find [搜索路径] [搜寻关键字]例子:123456789101112131415161718192021222324252627282930313233343536// 在目录/etc中查找文件init,注:*,匹配任意字符,若查找init开头的文件则init*; ?,匹配单个字符,若查找带有init的七个字母的文件则init???find /etc -name init// 在根目录下查找大于100MB的文件,以数据块为单位512字节,注:大于+,小于-find / -size +204800// 在根目录下查找所有者为sanlee的文件find /home -user samlee// 120分钟以内被修改的文件find /etc -mmin -120/*时间:- 1.天 ctime, atime, mtime- 2.分钟 cmin, amin, mmin- c-change改变,表示文件属性被修改过,所有者, 所属组, 权限- a-access访问- m-modify修改,表示文件内容被修改过- -之内,+超过*/ // 在/etc下查找大于80MB小于100MB的文件,注:连接符-a表示and逻辑与,-o表示or逻辑或find /etc -size +163840 -a -size -204800 // 在/etc下查找inittab文件并显示其详细信息find /etc -name inittab -exec ls -l &#123;&#125; \; // 在/etc下查找用户名为samlee的文件并删除find /etc -user samlee -exec rm -rf &#123;&#125; \;// find .... -exec 命令 &#123;&#125; \;(若用-ok代替-exec则会询问确认) // 在/etc下查找软链接文件(f二进制文件, l软链接文件, d目录)find /etc -type l// 在当前目录寻找i节点为16的文件find . -inum 16 locate功能:寻找文件或目录(在updatedb数据库里寻找,速度快)路径:/usr/bin/locate语法:locate [搜索关键字]例子:12// 列出所有跟file相关的文件locate file updatedb功能:建立整个系统目录文件的数据库路径:/usr/bin/updatedb例子:updatedb grep功能:在文件中搜寻字符匹配的行并输出路径:/usr/bin/grep语法:grep [指定字母][源文件]例子:grep ftp /etc/services 帮助命令:man功能:获取帮助信息(遵循more的指令来换行换页和退出)路径:/usr/bin/man语法:man [命令或配置文件]例子:1234// 查看ls命令的帮助信息man ls// 查看配置文件services的帮助信息man services 若命令与配置文件同名则优先查看命令的帮助,通过man 5 […]来查看,即帮助有很多种,第五种为配置文件帮助。 info功能:获取帮助信息(与man功能完全相同,只是呈现方式不同)路径:/usr/bin/info语法:info [任何关键字]例子:12// 查看ls指令的帮助信息info ls whatis和apropos功能:获得索引的简短说明信息路径:/usr/bin/whatis /usr/bin/apropos语法:whatis [任何关键字]apropos [任何关键字]例子:123whatis ls//相当于man -kapropos fstab 注:与locate类似,需要用makewhatis指令建立whatis和apropos搜索使用的数据库(例子:makewhatis) help功能:查看shell内置命令的帮助 压缩解压命令:(.gz,.tar.gz,.zip,.bz2)gzip功能:压缩文件路径:/bin/gzip语法:gzip 选项[文件]压缩后文件格式:.gz1.只能压缩文件,不能压缩目录2.不保留原文件 gunzip功能:解压缩.gz的压缩文件(同gzip -d)路径:/bin/gunzip语法:gunzip 选项[压缩文件]例子:gunzip file1.gz tar功能:打包目录路径:/bin/tar压缩语法:tar 选项 [目录] -c 产生tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -z 打包同时压缩 压缩后文件格式:.tar.gz例子:12// 将目录dir1压缩成一个打包并压缩的文件tar -zcvf dir.tar.gz dir1 注:老系统不支持打包压缩一起进行 解压缩语法:tar 选项 [压缩文件] -x 解包.tar文件 -v 显示详细信息 -f 指定解压文件 -z 解压缩 例子:12// 将dir1解压到当前目录下tar -zxvf dir1.tar.gz file指令功能:判断文件类型语法:file [文件名] zip(默认的win和linux通用格式)功能:压缩文件或目录(保留原文件)路径:/usr/bin/zip语法:zip 选项[-r] [压缩后文件名称] [文件或目录] -r 压缩目录 压缩后文件格式:.zip例子:1234// 压缩文件zip services.zip /etc/services// 压缩目录zip -r test.zip /test unzip功能:解压.zip的压缩文件路径:/usr/bin/unzip语法:unzip [压缩文件]例子:12// 解压文件unzip test.zip bzip2功能:压缩文件路径:/usr/bin/bzip2语法:bzip2 选项[-k] [文件] -k 产生压缩文件后保留文件 压缩后文件格式:.bz2例子:bzip2 -k file1 bunzip2功能:解压缩路径:/usr/bin/bunzip2语法:bunzip2 选项[-k] [压缩文件] -k 解压缩后保留文件 例子:bunzip2 -k file1.bz2 网络通信指令:write功能:向另一个用户发信息,以ctrl+D作为结束(实时)路径:/usr/bin/write语法:write &lt;用户&gt;例子:write samlee wall功能:向所有用户广播信息路径:/usr/bin/wall语法:wall [message] [文件名]例子:wall Happy new year! ping功能:测试网络连通性路径:/usr/sbin/ping语法:ping 选项 IP地址 -c [num] pingnum次(类似windows的ping) -s [num] 用num大小的数据包来测试(num不能大于65507) 例子:ping 192.168.1.1用ifconfig可查看本机IP地址 ifconfig功能:查看网络设置信息路径:/usr/sbin/ifconfig语法:ifconfig 选项[-a] [网卡设备标识] -a 显示所有网卡信息 例子:123456// 显示所有网卡信息ifconfig -a// 查看关键网卡信息ifconfig eth0// 临时改变IP地址为192.168.9.6,若不写入配置文件,则仅在此会话中生效ifconfig eth0 192.168.9.6 系统关机命令:shutdown功能:关机路径:/usr/sbin/shutdown语法:shutdown例子:12// 马上关机shutdown -h now reboot功能:重启系统路径:/usr/sbin/reboot语法:reboot例子:reboot shell应用技巧:小技巧:1.命令补全输入文件名起始的若干个字母后,按补齐文件名2.清空屏幕clear指令,或 ctrl + L3.快速删除ctrl + U 可以快速删除当前命令行光标前的所有内容4.命令历史history 指令:列出敲过的所有指令上下箭头键可以查找以前执行过的命令 大技巧:1.命令别名定义别名例子:123alias copy=cp// 带选项时要加双引号alisa xrm=&quot;rm -f&quot; 删除别名:unalias copy 2.输入/输出重定向shell对于每一个进程预先定义三个文件描述字(0,1,2)0(STDIN) 标准输入1(STDOUT) 标准输出2(STDERR) 标准错误输出 &gt;或&gt;&gt; 输出重定向例子:1234// tmp的内容输出到文件中,并替代原内容ls -l /tmp &gt; /tmp.msg// data输出到文件内,并追加在原内容后面date &gt;&gt; /tmp.msg &lt; 输入重定向例子:12// 把文件作为输入端wall &lt; /etc/motd 2&gt; 错误输出重定向例子:12// 错误信息输出到文件/bak.error内cp -R /usr /backup/usr.bak 2&gt; /bak.error 3.管道管道:将一个命令的输出传送给另一个命令,作为另一个命令的输入。使用方法:命令1|命令2|命令3……|命令n例子:12ls -l /etc | morels -l /etc | grep init 4.命令连接符 符号 功能 ; 用;间隔的各命令按顺序依次执行 &amp;&amp; 前后命令的执行存在逻辑与关系,前面的命令执行成功后,它后面的命令才被执行。 &#124;&#124; 前后命令的执行存在逻辑或关系,若前面的命令执行成功,则后面不执行；若前面的命令执行失败,则后面才会执行。 5.命令替换符命令替换:将一个命令的输出作为另一个命令的参数格式:命令1 `命令2`(注意不是单引号,而是键盘按键1旁边的替换符)例子:ls -l `which touch`]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux学习笔记之vim]]></title>
    <url>%2F2018%2F11%2F06%2Flinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8Bvim%2F</url>
    <content type="text"><![CDATA[Vim/Vi:文本编辑器(没有菜单，只有命令) 三种工作模式:命令模式、插入模式、编辑模式 常用命令:1.插入命令 命令 功能 a 在光标后附加文本 A 在本行行末附加文本 i 在光标前插入文本 I 在本行开始插入文本 o 在光标下插入新行 O 在光标上插入新行 2.定位命令 命令 功能 h or 方向左键 左移一个字符 j or 方向下键 下移一行 k or 方向上键 上移一行 l or 方向右键 右移一个字符 $ 移至行尾 0 移至行首 H 移至屏幕上端 M 移至屏幕中央 L 移至屏幕下端 :set nu 设置行号 :set nonu 取消行号 gg 到第一行 G 到最后一行 nG 到第n行 :n 到第n行 3.删除命令 命令 功能 x 删除光标所在处字符 nx 删除光标所在处后n个字符 dd 删除光标所在行，ndd删除n行 dG 删除光标所在行到末尾的内容 D 删除从光标所在处到行尾 :n1,n2d 删除指定范围的行 4.复制和剪切命令 命令 功能 yy、Y 复制当前行 nyy、nY 复制当前行以下n行 dd 剪切当前行 ndd 剪切当前行以下n行 p、P 粘贴在当前光标所在行下或行上 5.替换和取消命令 命令 功能 r 取代光标所在处字符 R 从光标所在处开始替换字符，按Esc结束 u 取消上一步操作 6.搜索和替换命令 命令 功能 /string 向前搜索指定字符串(搜索时忽略大小写:set ic) n 搜索指定字符串的下一个出现位置 N 搜索指定字符串的上一个出现位置 :%s/old/new/g 全文替换指定字符串 :n1,n2/old/new/g 在一定范围内替换指定字符串(注:选项g是强制替换，若需经过询问再替换则用选项c) 7.保存退出命令 命令 功能 :wq、ZZ 保存并退出 :wq! 强行保存退出 :w /dir 保存到新位置 :q! 退出 使用技巧:1.:r [文件名]功能:导入文件2.:![命令]功能:在Vi中执行命令(注:妙用 :r 和 :! ，如 :r :!date 快速导入时间)3.:map 快捷键 出发命令功能:定义快捷键例子: 12345678// 在行首加#:map ^P I# + &lt;ESC键&gt;// 删除行首字母:map ^B 0x// 取消快捷键ctrl+P:unmap ^P// 注意:[^字母]的输入方法是 ctrl + v + 字母 4.连续行注释(^代表行首) 123456// 注释掉n1到n2行:n1,n2s/^/#/g// 去除n1到n2行的注释:n1,n2s/^#//g// 在n1到n2行前添加//，注:转义符\可保留符号本意:n1,n2s/^/\/\//g 5.:ab功能:替换例子: 12:ab blogs pomo16.coding.me(把blogs自动替换为pomo16(我)的博客地址):unab blogs(取消上述替换) /.vimrc若需修改个人的vim使用习惯，可修改此配置文件，如添加一些快捷键通过vi /.vimrc来编辑打开后可添加常用的指令，如: 1234set numap ^P I# + &lt;ESC键&gt;ab blogs pomo16.coding.me等// 注:无需加:]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>vim</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP中的GET和POST请求]]></title>
    <url>%2F2018%2F11%2F05%2FHTTP%E4%B8%AD%E7%9A%84GET%E5%92%8CPOST%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[背景什么是HTTP？&emsp;&emsp;HTTP 即超文本传输协议，其设计目的是提供一种发布和接收 HTML 页面的方法，保证客户机与服务器之间的通信。HTTP 是一个客户端和服务器端请求和应答的标准（TCP）。客户端是终端用户，服务器端是网站。通过使用 Web 浏览器、网络爬虫或者其它的工具，客户端发起一个到服务器上指定端口（默认端口为80）的 HTTP 请求。 HTTP请求只有POST和GET两种方法吗？&emsp;&emsp;No。HTTP1.1 共定义了八种请求方法，包括 GET, POST, HEAD, OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。虽然 HTTP 的请求方式有 8 种，但是我们在实际应用中常用的也就是 GET 和 POST，其他请求方式也都可以通过这两种方式间接的来实现。 GET和POST在客户机和服务器之间进行请求-响应时，两种最常被用到的方法是：GET 和 POST。 GET：从指定的资源请求数据，请求指定的页面信息，并返回实体主体。 POST：向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 GET 方法查询字符串（名称/值对）是在 GET 请求的 URL 中发送的，请求的数据会附加在 URL 之后，以?分割 URL 和传输数据，多个参数用&amp;。因此，GET请求的数据会暴露在地址栏中。1/test/demo_form.asp?name1=value1&amp;name2=value2 POST 方法 查询字符串（名称/值对）是在 POST 请求的 HTTP 消息主体中发送的，所以 POST 请求的数据不会暴露在地址栏中。123POST /test/demo_form.asp HTTP/1.1Host: pomo16.clubname1=value1&amp;name2=value2 二者区别 特性 GET POST 后退按钮/刷新 无害 数据会被重新提交（浏览器应该告知用户数据会被重新提交）。 书签 可收藏为书签 不可收藏为书签 缓存 能被缓存 不能缓存 编码类型 application/x-www-form-urlencoded application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 历史 参数保留在浏览器历史中。 参数不会保存在浏览器历史中。 对数据长度的限制 是的。当发送数据时，GET 方法向 URL 添加数据；URL 的长度是受限制的（URL 的最大长度是 2048 个字符）。 无限制。 对数据类型的限制 只允许 ASCII 字符。 没有限制。也允许二进制数据。 安全性 与 POST 相比，GET 的安全性较差，因为所发送的数据是 URL 的一部分。在发送密码或其他敏感信息时绝不要使用 GET ！ POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 可见性 数据在 URL 中对所有人都是可见的。 数据不会显示在 URL 中。 深层解析(这部分内容总结自 WebTechGarden 的文章)前文已经提到 HTTP 是基于 TCP/IP 的协议，GET 和 POST 本质上就是 TCP 链接，并无差别。但是由于 HTTP 的规定和浏览器/服务器的限制，导致他们在应用过程中体现出一些不同。GET 和 POST 还有一个重大区别，简单的说： GET 产生一个 TCP 数据包；POST产生两个 TCP 数据包。 对于 GET 方式的请求，浏览器会把 http header 和 data 一并发送出去，服务器响应200（返回数据）；而对于 POST，浏览器先发送 header，服务器响应 100 continue，浏览器再发送 data，服务器响应 200 ok（返回数据）。 因为 POST 需要两步，时间上消耗的要多一点，看起来 GET 比 POST 更有效。因此 Yahoo 团队有推荐用 GET 替换 POST 来优化网站性能。但这是一个坑！跳入需谨慎。为什么？ GET 与 POST 都有自己的语义，不能随便混用。 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。 并不是所有浏览器都会在 POST 中发送两次包，Firefox 就只发送一次。]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单理解Java的jre和jdk]]></title>
    <url>%2F2018%2F11%2F05%2F%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3Java%E7%9A%84jre%E5%92%8Cjdk%2F</url>
    <content type="text"><![CDATA[jre 和 jdk 是 Java 开发中必不可少的两样东西，本文分别介绍这两个东东。 JREJava Runtime Environment的缩写，即Java的运行环境，是 Sun 公司的产品。 Java Runtime Environment(JRE)是可以在其上运行、测试和传输应用程序的 Java 平台，包括 Java 虚拟机(jvm)、Java 核心类库和支持文件。它不包含开发工具(JDK)–编译器、调试器和其它工具。 JDKJDK(Java Development Kit) 是 Java 语言的软件开发工具包(SDK)。 版本 简述 SE(JavaSE) standard edition，标准版，是我们通常用的一个版本，从JDK 5.0开始，改名为Java SE。 EE(JavaEE) enterprise edition，企业版，使用这种 JDK 开发 J2EE 应用程序，从 JDK 5.0 开始，改名为 Java EE。从2018年2月26日开始，J2EE 改名为 Jakarta EE。 ME(J2ME) micro edition，主要用于移动设备、嵌入式设备上的 java 应用程序，从 JDK 5.0 开始，改名为 Java ME。 没有 JDK 的话，无法编译 Java 程序（指 java 源码 .java 文件），如果想只运行 Java 程序（指 class 或 jar 或其它归档文件），要确保已安装相应的 JRE。 SummaryJDK：java development kit （java 开发工具） JRE：java runtime environment （java 运行时环境） 简单来说，编译( java )用的是 jdk，运行( javac )用的是 jre。一般 jdk 包含了 jre，毕竟没有运行环境还编译啥呢。另外从 Java 的技术体系来说，Java 的技术体系至少包括四个部分： Java 程序设计语言 各种平台上的 Java 虚拟机 （ JVM ） Java API 类库 一系列辅助工具，如 javac 1 + 2 + 3 + 4 = JDK 2 + 3 = JRE JDK &gt; JRE &gt; JVM]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jre</tag>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse常用快捷键]]></title>
    <url>%2F2018%2F11%2F05%2FEclipse%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[对于Eclipse快捷键，就个人使用爱好总结一下，毕竟每个人的习惯不一样，这里以我自己觉得好用的一些官方快捷键为主(没有改过键！)。 按键 功能 Alt + / 代码自动补全 Ctrl + Alt + Up(↑) 在本行上方插入一行，内容与本行相同 Ctrl + Alt + Down(↓) 在本行下方插入一行，内容与本行相同 Ctrl + Shift + o 快速导包 Alt + Shift + j 自动生成方法的注释格式 Ctrl + / 添加(或消除)//注释 Ctrl + Shift + F 代码格式化(会和搜狗输入法的快捷键冲突) Ctrl + Z 撤回 Ctrl + Y 下一步(与撤回相反) Ctrl + Page up 上一页文件 Ctrl + Page Down 下一页文件 Ctrl + Shift + t 打开open type]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客见闻录之图床搭建]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%8D%9A%E5%AE%A2%E8%A7%81%E9%97%BB%E5%BD%95%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[在做博客的 工具箱 和 常用社区 时使用了大量的图片，如果把图片放在静态文件中一起 deploy ，那么每次加载网页图片的时候就会很卡，所以无论是建博客需要还是后期文章图片引用，图床这个东西都必不可少。 图床是啥？图床(Picture bed)，指的是专门用于储存文件的服务器。储存的图片可以生成外链被调用，并且在 CDN 加速的情况下，图片可以被快速加载。 七牛云七牛云 简单易用，而且重要的是它是免费的。在注册账号以后，在其对象存储服务下新建存储空间，配置如下，名字自取： 公开空间比私有空间更容易获取外链。创建之后上传图片，然后在图片右侧点击下拉菜单即可获取外链：]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>七牛云</tag>
        <tag>blog</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客见闻录之主题个性化]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%8D%9A%E5%AE%A2%E8%A7%81%E9%97%BB%E5%BD%95%E4%B9%8B%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%2F</url>
    <content type="text"><![CDATA[我的博客使用的是简约至上的 Maupassant 主题，主题虽好但也想弄点个性化的东西把玩把玩。 修改page页通过修改 themes\maupassant\_config.yml 可以配置基本的页面布局。 menu菜单12345678910111213141516171819menu: - page: 首页 directory: . icon: fa-home - page: 归档 directory: archives/ icon: fa-archive - page: 关于 directory: about/ icon: fa-user - page: 工具箱 directory: toolbox/ icon: fa-book - page: 常用社区 directory: community/ icon: fa-comments - page: rss directory: atom.xml icon: fa-rss 这里一共配置了六个菜单项，要使自定义的菜单项生效，需要在 hexo 根目录的 source 文件夹中创建与菜单项同名的文件夹并在文件夹内创建 index.md 文件(实测 index.html 也可以，但是不方便下一步的 layout )。为了使菜单页保留 page 的布局，需要在 index.md 的 front-matter 中添加 layout: page 。 菜单项页面布局在添加了菜单项页面之后，我们需要把他们布局。虽然这是 markdown 文件，但是可以内嵌 html5 的语法。 坑1：在 md 中写 html 时，不要乱换行，不然 markdown 可能会当真，最后可能会影响布局。所以我的页面代码是这种 style 的：（仿佛被压缩了的代码。。。） 坑2：如果页面不生效，先检查页面是不是放错路径了。记得是根目录下的 source 不是主题目录下的 source。 坑3：因为父级元素的布局会干扰你的布局，而你在不知道的情况下布局就会跟玄学一样。布局的时候如果发现位置不对劲，记得使用下面的代码： 1style = &quot;padding: 0;margin: 0;&quot; 坑4：如果你也想用 font awesome，直接用就好了，主题中已经导入了 font awesome，如果再导入的话会引起侧边栏图标显示错误。 坑5：如果侧边栏语言错乱，记得在 hexo 根目录下的 _config.yml 中指定语言。 坑6：记得在 index.md 的 front-matter 中添加 title ，效果显示如下： 网页ico在网上找一个 16 × 16 或者 32 × 32 的 ico 文件命名为 favicon.ico 放置在 hexo 的 source 目录下即可。如果没有，可以用 png 在线制作一个(点这里)。如果网页显示不出来的话，大概率是 ico 或者缓存的问题，清理浏览器的缓存试一下。另外 edge 浏览器不用挣扎，我的就没有成功显示过。 点击效果添加在我的博客中点击的时候会冒出彩色的小爱心，制作方法如下: 添加 clicklove.js 在 themes\maupassant\source\js\ 中创建 clicklove.js 并在文件中添加如下代码： 1!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &apos;&apos;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 导入 clicklove.js 编辑 themes\maupassant\layout\_partial\after_footer.pug ，在末尾导入 clicklove.js： 12// 添加点击爱心功能script(type=&apos;text/javascript&apos;, src=url_for(theme.js) + &apos;/clicklove.js&apos; + &apos;?v=&apos; + theme.version) 开启文章字数统计在 hexo 根目录下的 _config.yml 中设置 wordcount: true 即可。效果如下：​ fancybox在 hexo 根目录下的 _config.yml 中设置 fancybox: true 即可。效果如下：​ 但是对于作为超链接的图片我们不需要fancybox，这时只需要在相应 &lt;img&gt; 标签中使用 class=&quot;nofancybox&quot; 就可以避免fancybox样式阻止超链接使用的情况。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客见闻录之基础搭建]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%8D%9A%E5%AE%A2%E8%A7%81%E9%97%BB%E5%BD%95%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[算是第二次用 hexo 建站了吧，第一次建的 blog 还保留着，不过荒废已久了。这次建站总体过程还算顺利，这里就记录一下搭建博客中踩过的坑。 搭建步骤 安装 nodejs 安装 hexo 12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时候在 localhost:4000 就能看到 hexo 的欢迎界面了。 美化 blog 在 hexo 主题 中寻找合适的主题，一般配置都是下载主题后解压放入 hexo 根目录下的 themes，然后在 _config.yml 中把 theme 配置为所选主题名称即可。 一些常用 hexo 指令 12345678//清除缓存和生成的静态文件hexo clean //生成静态文件hexo g//部署网站hexo d//启动hexo本地服务器hexo s 安装 git 安装依赖 1npm install hexo-deployer-git --save 设置全局配置 user.name 和 user.email 12git config –-global user.name &quot;xxx&quot; //(&quot;&quot;的账号是刚才Github里面自己注册的账号) git config –-global user.email &quot;xxx@xxx.com&quot; //(&quot;&quot;的邮箱是你自己注册的邮箱) 生成秘钥 123cd ~/.ssh ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot; //打自己的邮箱//如果cd失败说找不到.ssh的话，直接执行第二步生成就好了(我就是这样，23333) 设置 ssh key 到 GitHub 默认生成 ssh key 在 .ssh 文件夹中，复制 id_rsa.pub 文件内容到 github-&gt;settings-&gt;SSH and GPG key-&gt;new ssh key。 测试 ssh 如果是第一次测试的话终端会丢一个 warn 来问候你一下，敲 yes 然后回车就行了。如果收到 Hi xxx!(xxx是你的用户名)，证明 ssh 连接成功。 1ssh -T git@github.com 在 github 上新建仓库 yourname.github.io 在 hexo 根目录下的 _config.yml 中配置 Deployment 如下： 1234deploy: type: git repository: git@github.com:xxx/xxx.github.io.git branch: master &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;repository 是仓库地址，获取地址参考下图： 终极测试 输入hexo指令全家桶： 123hexo clean hexo ghexo d 或者偷一下懒： 12hexo cleanhexo d -g 或者懒到没有得救了： 1hexo d -g 部署完毕后登陆 https://xxx.github.io ，如果看到自己的博客那就大功告成了。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
